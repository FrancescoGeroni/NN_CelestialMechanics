{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1258,
   "id": "e2d78f64",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1259,
   "id": "7bea2ffe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from pyDOE import lhs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1260,
   "id": "336603ab",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1261,
   "id": "65012647",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Working on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1262,
   "id": "fbc56398",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def batch_generator(x, t, y, batch_size, dev = device):   \n",
    "    #idx = torch.randperm(len(x))\n",
    "    idx = torch.arange(len(x)) \n",
    "    \n",
    "    # Ensure the batch size is not larger than the available data points\n",
    "    num_batches = (len(x) + batch_size - 1) // batch_size  # calculate number of batches\n",
    "    \n",
    "    for i in range(num_batches): \n",
    "        \n",
    "        batch_idx = idx[i * batch_size : min((i + 1) * batch_size, len(x))]  # Get batch indices\n",
    "        \n",
    "        batch_x = x[batch_idx].to(dev)\n",
    "        batch_t = t[batch_idx].to(dev)\n",
    "        batch_y = y[batch_idx].to(dev)\n",
    "        \n",
    "        # Yield the batch\n",
    "        yield batch_x, batch_t, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1263,
   "id": "b2df51bf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#functions for data normalization\n",
    "def normalize(inputs): # to [-1, 1]\n",
    "    inputs_min, inputs_max = inputs.min(), inputs.max()\n",
    "    \n",
    "    return 2*(inputs - inputs_min) / (inputs_max - inputs_min) - 1\n",
    "\n",
    "def denormalize(inputs): # from [-1, 1]\n",
    "    inputs_min, inputs_max = inputs.min(), inputs.max()\n",
    "    \n",
    "    return ((inputs + 1)/2)*(inputs_max - inputs_min) + inputs_min\n",
    "\n",
    "\n",
    "def standardize(inputs):\n",
    "    \n",
    "    return  (inputs - inputs.mean())/(inputs.std())\n",
    "\n",
    "def destandardize(inputs):\n",
    "    \n",
    "    return  inputs*inputs.std() + inputs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1264,
   "id": "e3a434de-b6cd-4066-b5a4-c1f264b47d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion for data a-dimensionalization\n",
    "\n",
    "L = 3.85e5 # mean orbital radius of the moon\n",
    "T = (2.361e6)/(2*np.pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1265,
   "id": "abc8ed50-29d0-4108-b9c1-141ab35b6584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling Function\n",
    "def data_handler(x, t, y, n_ic, n_domain):\n",
    "    \n",
    "    idx = torch.randperm(len(x))\n",
    "    x_shuff, t_shuff = x[idx], t[idx]\n",
    "    \n",
    "    # Bounds of space-time domain\n",
    "    lb = np.array([[t_shuff.min(), x_shuff.min()]])\n",
    "    ub = np.array([[t_shuff.max(), x_shuff.max()]])\n",
    "\n",
    "    # Initial Conditions\n",
    "    idx_x0 = torch.randperm(n_ic) \n",
    "    X0 = np.column_stack((x[idx_x0, 0], np.zeros(len(idx_x0))))\n",
    "    Y0 = exactSolution(X0[:,1], X0[:,0])[:, None]\n",
    "    \n",
    "    # Collocation Points with LHS\n",
    "    X_T_domain = lb + (ub - lb) * lhs(2, n_domain)\n",
    "    Y_domain = exactSolution(X_T_domain[:,1], X_T_domain[:,0])[:, None]\n",
    "    \n",
    "    '''\n",
    "    # Normalization of domain and initial conditions\n",
    "    X0[:,0] = standardize(X0[:,0])\n",
    "    Y0 = standardize(Y0)\n",
    "\n",
    "    X_T_domain[:,1] = standardize(X_T_domain[:,1])\n",
    "    X_T_domain[:,0] = standardize(X_T_domain[:,0])\n",
    "    Y_domain = standardize(Y_domain)\n",
    "    '''\n",
    "    \n",
    "    x0 = torch.tensor(X0[:, 0], requires_grad=True).view(-1,1).float().to(device)\n",
    "    t0 = torch.tensor(X0[:, 1], requires_grad=True).view(-1,1).float().to(device)\n",
    "    y0 = torch.tensor(Y0).float().to(device)\n",
    "\n",
    "    x_domain = torch.tensor(X_T_domain[:, 0], requires_grad=True).view(-1,1).float().to(device)\n",
    "    t_domain = torch.tensor(X_T_domain[:, 1], requires_grad=True).view(-1,1).float().to(device)\n",
    "    y_domain = torch.tensor(Y_domain).float().to(device)\n",
    "\n",
    "    x = torch.tensor(x, requires_grad=True).view(-1,1).float().to(device)\n",
    "    t = torch.tensor(t, requires_grad=True).view(-1,1).float().to(device)\n",
    "    y = torch.tensor(y).float().to(device)\n",
    "\n",
    "    return x0, t0, y0, x_domain, t_domain, y_domain, x, t, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1266,
   "id": "68404050",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class PINN_Advection(nn.Module):\n",
    "    \n",
    "    def __init__(self, layers, loss_type, n_batch, *data):\n",
    "        super(PINN_Advection, self).__init__() \n",
    "\n",
    "        (self.t0, self.x0, self.y0, self.t_domain, self.x_domain, self.y_domain, self.t, self.x, self.y) = data\n",
    "        self.n_batch = n_batch\n",
    "        self.losstype = loss_type\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activation = nn.Tanh() # Activation function\n",
    "\n",
    "        for i in range(len(layers) - 1): \n",
    "            self.layers.append(nn.Linear(layers[i], layers[i+1]))  # Fully connected linear layer: the operation [out = w*input + b] will be made by hand with factorized weights\n",
    "\n",
    "        # Random weight factorization memorization\n",
    "        self.s_list = {}\n",
    "        self.v_list = {}\n",
    "        \n",
    "        for i in range(len(self.layers)): \n",
    "            mean = 1.0\n",
    "            std = 0.1\n",
    "        \n",
    "            w = self.layers[i].weight # weights are trained but used only now (and for backward procedure)\n",
    "            \n",
    "            # Generate scaling vector s and pivot matrix v for weight factorization\n",
    "            s = mean + torch.normal(mean, std, size=(w.shape[-1],))\n",
    "            s = torch.exp(s)\n",
    "            self.s_list[f\"s_{i}\"] = nn.Parameter(s, requires_grad=True)\n",
    "            self.register_parameter(f\"s_{i}\", self.s_list[f\"s_{i}\"])  # Register the parameter\n",
    "            \n",
    "            v = w / s\n",
    "            self.v_list[f\"v_{i}\"] = nn.Parameter(v, requires_grad=True)\n",
    "            self.register_parameter(f\"v_{i}\", self.v_list[f\"v_{i}\"]) # Register the parameter\n",
    "   \n",
    "        self.optimizer = None\n",
    "        self.train_loss_history = []\n",
    "        \n",
    "        #For weighted loss procedures\n",
    "        self.n_losses = 3\n",
    "        self.loss_weights = torch.ones(self.n_losses) # Initialize losses weights\n",
    "        self.f = 50\n",
    "        #self.previous_residual = None\n",
    "        #self.n_temporal_weights = int(np.sqrt(len(self.t_domain)))\n",
    "        #self.temporal_weights = torch.ones(self.n_temporal_weights)  # Initialize temporal weights\n",
    "        self.alpha = 0.9\n",
    "        self.epsilon = 0.01\n",
    "        \n",
    "        self.c = 10 # flow velocity\n",
    "\n",
    "\n",
    "    def get_factorized_weight(self, i):        \n",
    "        b = self.layers[i].bias\n",
    "\n",
    "        s = self.s_list[f\"s_{i}\"]\n",
    "        v = self.v_list[f\"v_{i}\"]\n",
    "        \n",
    "        return s * v, b\n",
    "\n",
    "        \n",
    "    def forward(self, X): # Forward pass using random decomposed weights\n",
    "        a = X.float()\n",
    "        \n",
    "        for i in range(len(self.layers)):\n",
    "            \n",
    "            kernel, b = self.get_factorized_weight(i)\n",
    "            \n",
    "            a = torch.matmul(a, kernel.T) + b \n",
    "            \n",
    "            if i < len(self.layers) - 1:\n",
    "                a = self.activation(a)\n",
    "                    \n",
    "        return a    \n",
    "\n",
    "    \n",
    "    def network_prediction(self, t, x):\n",
    "        \n",
    "        return self.forward(torch.cat((t, x), 1))\n",
    "        \n",
    "        \n",
    "    def advection_prediction(self, t, x): # Compute the differential equation for Advection\n",
    "        u = self.network_prediction(t, x)\n",
    "        du_dt = self.get_derivative(u, t, 1)\n",
    "        du_dx = self.get_derivative(u, x, 1)\n",
    "        \n",
    "        f =  du_dt + self.c*du_dx\n",
    "        \n",
    "        return f\n",
    "\n",
    "    \n",
    "    def get_derivative(self, y, x, n): # General formula to compute the n-th order derivative of y = f(x) with respect to x\n",
    "        if n == 0:  # (n is the order if the derivative)\n",
    "            return y\n",
    "        else:\n",
    "            dy_dx = torch.autograd.grad(y, x, torch.ones_like(y).to(device), create_graph=True, retain_graph=True, allow_unused=True)[0]\n",
    "        \n",
    "        return self.get_derivative(dy_dx, x, n - 1)\n",
    "        \n",
    "\n",
    "    def loss_IC(self, x, t, y):\n",
    "        y_pred_IC = self.network_prediction(t, x)\n",
    "        y_pred_IC.requires_grad_(True)\n",
    "        \n",
    "        if self.losstype == 'mse':\n",
    "            loss_IC = torch.mean((y - y_pred_IC) ** 2).to(device)\n",
    "            \n",
    "        elif self.losstype == 'logcosh':\n",
    "            loss_IC = torch.mean(torch.log(torch.cosh(y - y_pred_IC))).to(device)\n",
    "    \n",
    "        return loss_IC\n",
    "\n",
    "\n",
    "    def loss_BC_symmetric(self):\n",
    "        loss_BC_symmetric = 0.0\n",
    "        y_pred = self.network_prediction(self.t, self.x)\n",
    "        y_pred.requires_grad_(True) \n",
    "        \n",
    "        if self.losstype == 'mse':\n",
    "            loss_BC_symmetric += torch.mean((y_pred[:,0] - y_pred[:,-1])**2).to(device)\n",
    "            loss_BC_symmetric += torch.mean((self.y[:,0] - y_pred[:,0])**2).to(device)\n",
    "            loss_BC_symmetric += torch.mean((self.y[:,-1] - y_pred[:,-1])**2).to(device)\n",
    "        \n",
    "        elif self.losstype == 'logcosh':\n",
    "            loss_BC_symmetric += torch.mean(torch.log(torch.cosh(y_pred[:,0] - y_pred[:,-1]))).to(device)\n",
    "            loss_BC_symmetric += torch.mean(torch.log(torch.cosh(self.y[:,0] - y_pred[:,0]))).to(device)\n",
    "            loss_BC_symmetric += torch.mean(torch.log(torch.cosh(self.y[:,-1] - y_pred[:,-1]))).to(device)\n",
    "            \n",
    "        return loss_BC_symmetric\n",
    "\n",
    "    \n",
    "    def loss_interior(self, x, t):\n",
    "        f_pred = self.advection_prediction(x, t)\n",
    "        f_pred.requires_grad_(True)\n",
    "        \n",
    "        if self.losstype == 'mse':\n",
    "            loss_interior = torch.mean(torch.pow(f_pred,2)).to(device)\n",
    "            \n",
    "        elif self.losstype == 'logcosh':\n",
    "            loss_interior = torch.mean(torch.log(torch.cosh(f_pred))).to(device)\n",
    "                 \n",
    "        return loss_interior\n",
    "\n",
    "\n",
    "    def forward_temp_weights(self, loss_domain):\n",
    "        loss_domain = torch.tensor(loss_domain)\n",
    "        \n",
    "        for i in range(1, self.n_temporal_weights):\n",
    "            self.temporal_weights[i] = torch.exp(-self.epsilon * torch.sum(loss_domain[0:i-1]))\n",
    "\n",
    "        return (self.temporal_weights*loss_domain).mean()\n",
    "\n",
    "    \n",
    "    def forward_loss_weights(self, losses):       \n",
    "        losses_tensor = torch.stack(losses)\n",
    "\n",
    "        parameters = list(self.parameters())\n",
    "            \n",
    "        # Create the gradient of each component of the loss respect to the parameters of the model\n",
    "        grad_norms = []\n",
    "        for l in losses_tensor: \n",
    "                \n",
    "            if l.requires_grad != True:\n",
    "                l = l.clone().detach().requires_grad_(True)\n",
    " \n",
    "            grads = torch.autograd.grad(l, parameters, retain_graph=True, allow_unused=True)\n",
    "            valid_grads = [g.view(-1) for g in grads if g != None]\n",
    "\n",
    "            if valid_grads:  \n",
    "                norm = torch.norm(torch.cat(valid_grads))\n",
    "                    \n",
    "            else:\n",
    "                norm = torch.tensor(0.0, device=parameters[0].device)\n",
    "                \n",
    "            grad_norms.append(norm)\n",
    "\n",
    "        grad_norms = torch.stack(grad_norms)\n",
    "            \n",
    "        for i in range(self.n_losses):\n",
    "            lambda_hat = grad_norms.sum() / grad_norms[i]\n",
    "            self.loss_weights[i] = self.alpha*self.loss_weights[i] + (1 - self.alpha)*lambda_hat\n",
    "            \n",
    "    \n",
    "    def get_training_history(self):\n",
    "        loss_his = np.array(self.train_loss_history)\n",
    "        total_loss, loss_IC, loss_domain, loss_BC_symmetric = np.split(loss_his, 4, axis=1) \n",
    "        \n",
    "        return total_loss, loss_IC, loss_domain, loss_BC_symmetric\n",
    "\n",
    "\n",
    "    def train_network(self, epochs, optim, learning_rate, regularization):\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr = learning_rate, weight_decay = regularization, amsgrad=True) \n",
    "       \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            loss_IC, loss_domain, loss_BC_symmetric = 0.0, 0.0, 0.0\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Mini-batch loss for Initial Conditions (IC)\n",
    "            for batch_x0, batch_t0, batch_y0 in batch_generator(self.x0, self.t0, self.y0, self.n_batch):\n",
    "                loss_IC_batch = self.loss_IC(batch_x0, batch_t0, batch_y0)\n",
    "                loss_IC += loss_IC_batch\n",
    "            \n",
    "            # Mini-batch loss for Domain Loss (interior) \n",
    "            #loss_domain_list = []\n",
    "            for batch_x_domain, batch_t_domain, batch_y_domain in batch_generator(self.x_domain, self.t_domain, self.y_domain, self.n_batch): \n",
    "                loss_domain_batch = self.loss_interior(batch_x_domain, batch_t_domain)\n",
    "                loss_domain += loss_domain_batch\n",
    "                #loss_domain_list.append(loss_domain_batch)\n",
    "            \n",
    "            #loss_domain = self.adaptive_weights.forward_temp_weights(loss_domain_list)\n",
    "            \n",
    "            # training for Simmetric boundary (BC_symmetric)\n",
    "            loss_BC_symmetric += self.loss_BC_symmetric()\n",
    "            \n",
    "            # Give a weigth to every singular loss term\n",
    "            weighted_losses_tensor = self.loss_weights*torch.stack([loss_IC, loss_domain, loss_BC_symmetric])\n",
    "            \n",
    "            # Total loss for this epoch after having given the weigths\n",
    "            total_loss = weighted_losses_tensor.sum()\n",
    "\n",
    "            # Calculate gradients and retain graph in order to derive also all of the singular losses terms for the correspondant weigth update\n",
    "            total_loss.backward(retain_graph = True)\n",
    "            \n",
    "            if epoch % self.f == 0:\n",
    "                self.forward_loss_weights([loss_IC, loss_domain, loss_BC_symmetric]) # global weigths update routine using the non-weigthed losses\n",
    "      \n",
    "            self.train_loss_history.append([total_loss.cpu().detach(), weighted_losses_tensor[0].cpu().detach(), weighted_losses_tensor[1].cpu().detach(), weighted_losses_tensor[2].cpu().detach()]) #includes backward\n",
    "                \n",
    "            # Optimize the network parameters\n",
    "            self.optimizer.step() \n",
    "\n",
    "            # Print out the loss every 100 epochs\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch ({optim}): {epoch}, Total Loss: {total_loss.detach().cpu().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1267,
   "id": "99f6cfbd-89b5-491e-b718-ad397a0b48e9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def exactSolution(t, x):\n",
    "    c = 10\n",
    "    \n",
    "    return np.sin(x - c*t)\n",
    "    \n",
    "x_len = 60\n",
    "t_len = 60\n",
    "\n",
    "x = np.linspace(0, 2*np.pi, x_len).reshape(-1,1) # Space domain (must be same size as time (for shuffle purposes))\n",
    "t = np.linspace(0, 1, t_len).reshape(-1,1) # Time domain\n",
    "\n",
    "X, T = np.meshgrid(x[:, 0], t[:, 0]) \n",
    "\n",
    "y_true = exactSolution(T, X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1268,
   "id": "1424f514",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "layers = [2, 256, 256, 256, 256, 256, 1]\n",
    "losstype = 'logcosh'\n",
    "n_batch = 16\n",
    "epochs = 2000\n",
    "L_rate = 0.0005\n",
    "lambda_reg = 0\n",
    "\n",
    "x0, t0, y0, x_domain, t_domain, y_domain, x, t, y = data_handler(x, t, y_true, x_len, x_len*t_len)\n",
    "\n",
    "model = PINN_Advection(layers, losstype, n_batch, t0, x0, y0, t_domain, x_domain, y_domain, t, x, y).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2310d65",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (Adam): 0, Total Loss: 2.393630266189575\n",
      "Epoch (Adam): 100, Total Loss: 19.63943862915039\n",
      "Epoch (Adam): 200, Total Loss: 16.468795776367188\n",
      "Epoch (Adam): 300, Total Loss: 13.89466667175293\n",
      "Epoch (Adam): 400, Total Loss: 11.81021499633789\n",
      "Epoch (Adam): 500, Total Loss: 10.137894630432129\n",
      "Epoch (Adam): 600, Total Loss: 8.766680717468262\n",
      "Epoch (Adam): 700, Total Loss: 8.107088088989258\n",
      "Epoch (Adam): 800, Total Loss: 7.096914291381836\n",
      "Epoch (Adam): 900, Total Loss: 6.264080047607422\n",
      "Epoch (Adam): 1000, Total Loss: 5.594166278839111\n",
      "Epoch (Adam): 1100, Total Loss: 5.051643371582031\n",
      "Epoch (Adam): 1200, Total Loss: 4.6120452880859375\n"
     ]
    }
   ],
   "source": [
    "model.train_network(epochs, 'Adam', L_rate, lambda_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35676103",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "total_loss, loss_IC, loss_domain, loss_BC_symmetric = model.get_training_history() \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(total_loss, label='Total Loss')\n",
    "plt.plot(loss_IC, label='Initial Condition Loss')\n",
    "plt.plot(loss_domain, label='Domain Loss')\n",
    "plt.plot(loss_BC_symmetric, label='Symmetric-boundary Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11efd86",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Get model predictions after training\n",
    "n_valid = int(20)\n",
    "\n",
    "x_valid = np.linspace(0, 2*np.pi, n_valid).reshape(-1,1) \n",
    "t_valid = np.linspace(0, 1, n_valid).reshape(-1,1) \n",
    "\n",
    "X_valid, T_valid = np.meshgrid(x_valid[:, 0], t_valid[:, 0]) \n",
    "\n",
    "y_true_valid = exactSolution(T_valid, X_valid) \n",
    "\n",
    "'''\n",
    "x_valid = standardize(x_valid)\n",
    "t_valid = standardize(t_valid)\n",
    "y_true_valid = standardize(y_true_valid)\n",
    "\n",
    "X_valid, T_valid = np.meshgrid(x_valid[:, 0], t_valid[:, 0])                                                                                                  \n",
    "'''\n",
    "\n",
    "idx = torch.randperm(n_valid)\n",
    "\n",
    "x_val = x_valid[idx]\n",
    "t_val = t_valid[idx]\n",
    "\n",
    "X_val, T_val = np.meshgrid(x_val[:, 0], t_val[:, 0]) \n",
    "\n",
    "#y_true_val = exactSolution(T_val, X_val) \n",
    "\n",
    "t_pred_shuffle = torch.tensor(T_val.flatten(), requires_grad=True).view(-1, 1).float().to(device)\n",
    "x_pred_shuffle = torch.tensor(X_val.flatten(), requires_grad=True).view(-1, 1).float().to(device)\n",
    "\n",
    "t_pred = torch.tensor(T_valid.flatten(), requires_grad=True).view(-1, 1).float().to(device)\n",
    "x_pred = torch.tensor(X_valid.flatten(), requires_grad=True).view(-1, 1).float().to(device)\n",
    "\n",
    "# Predict with the trained model\n",
    "y_pred_shuffle = model.network_prediction(t_pred_shuffle, x_pred_shuffle).float().detach().to(device).numpy()\n",
    "y_pred = model.network_prediction(t_pred, x_pred).float().detach().to(device).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e02c60-6222-4676-a504-7e01d54cd5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "# Subplot 1: Plot true values (exact solution)\n",
    "c1 = axes[0].contourf(X_valid, T_valid, y_true_valid, levels=50, cmap='magma', alpha=1)\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('T')\n",
    "fig.colorbar(c1, ax=axes[0], label='Exact solution')\n",
    "\n",
    "# Subplot 2: Plot only predictions (estimated solution)\n",
    "c2 = axes[1].scatter(X_val, T_val, c=y_pred_shuffle, cmap='magma', s=50, alpha=1)\n",
    "axes[1].set_xlabel('X')\n",
    "axes[1].set_ylabel('T')\n",
    "fig.colorbar(c2, ax=axes[1], label='Estimated solution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e788c3a6-60de-425e-94b2-b472266440ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.reshape(n_valid, n_valid)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(X_valid, T_valid, np.abs(y_true_valid - y_pred), levels=100, cmap='viridis')\n",
    "plt.xlabel(r'$\\delta X$')\n",
    "plt.ylabel(r'$\\delta T$')\n",
    "plt.colorbar(label='Prediction error')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a0d3a-90ba-406f-9b05-3cad3622f41b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Plot Initial conditions and Boundary conditions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x0.detach().numpy(), t0.detach().numpy(), c=y0.flatten(), cmap='jet')\n",
    "plt.colorbar(label='Conditions value')\n",
    "plt.xlabel('Position (x)')\n",
    "plt.ylabel('Time (t)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": -2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
