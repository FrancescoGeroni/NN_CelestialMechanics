{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "e2d78f64",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "7bea2ffe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from pyDOE import lhs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "336603ab",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "65012647",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Working on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "fbc56398",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def batch_generator(x, t, y, batch_size, dev = device):   \n",
    "    idx = torch.arange(len(x)) \n",
    "    \n",
    "    # Ensure the batch size is not larger than the available data points\n",
    "    num_batches = (len(x) + batch_size - 1) // batch_size  # calculate number of batches\n",
    "    \n",
    "    for i in range(num_batches): \n",
    "        \n",
    "        batch_idx = idx[i * batch_size : min((i + 1) * batch_size, len(x))]  # Get batch indices\n",
    "        \n",
    "        batch_x = x[batch_idx].to(dev)\n",
    "        batch_t = t[batch_idx].to(dev)\n",
    "        batch_y = y[batch_idx].to(dev)\n",
    "        \n",
    "        # Yield the batch\n",
    "        yield batch_x, batch_t, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "b2df51bf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#functions for data handling\n",
    "def normalize(inputs): # to [-1, 1]\n",
    "    inputs_min, inputs_max = inputs.min(), inputs.max()\n",
    "    \n",
    "    return 2*(inputs - inputs_min) / (inputs_max - inputs_min) - 1\n",
    "\n",
    "\n",
    "def denormalize(inputs): # from [-1, 1]\n",
    "    inputs_min, inputs_max = inputs.min(), inputs.max()\n",
    "    \n",
    "    return ((inputs + 1)/2)*(inputs_max - inputs_min) + inputs_min\n",
    "\n",
    "\n",
    "def standardize(inputs):\n",
    "    \n",
    "    return  (inputs - inputs.mean())/(inputs.std())\n",
    "\n",
    "\n",
    "def destandardize(inputs):\n",
    "    \n",
    "    return  inputs*inputs.std() + inputs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "d279f11a-5a97-449c-9bd5-b1c85a60f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the untouched datas and conditions points numbers and returns the handled data ready for training\n",
    "def data_handler(x, t, y, n_ic, n_bc, n_domain):\n",
    "\n",
    "    #Prepare shuffled data and respective domain (initial and boundary conditions are shuffled when they're created)\n",
    "    idx = torch.randperm(len(x))\n",
    "\n",
    "    x_shuff = x[idx]\n",
    "    t_shuff = t[idx]\n",
    "    X_shuff, T_shuff = np.meshgrid(x_shuff[:, 0], t_shuff[:, 0]) \n",
    "    \n",
    "    #Lower and upper bound of the space-time domain\n",
    "    lb = np.zeros((1,2))\n",
    "    ub = np.zeros((1,2))\n",
    "\n",
    "    lb[0, 0] = t_shuff[0,0]\n",
    "    lb[0, 1] = x_shuff[0,0]\n",
    "    ub[0, 0] = t_shuff[-1,0]\n",
    "    ub[0, 1] = x_shuff[-1,0]\n",
    "\n",
    "    x_norm = standardize(x)\n",
    "    t_norm = standardize(t)\n",
    "    y_true_norm = standardize(y_true)\n",
    "    \n",
    "    '''\n",
    "    x_norm = normalize(x)\n",
    "    t_norm = normalize(t)\n",
    "    y_true_norm = normalize(y_true)\n",
    "    '''\n",
    "    \n",
    "    X_norm, T_norm = np.meshgrid(x_norm[:, 0], t_norm[:, 0]) \n",
    "\n",
    "    # Initial Conditions \n",
    "    idx_x0 = np.random.choice(x.shape[0], n_ic, replace=False)\n",
    "\n",
    "    x0 = x[idx_x0, :]\n",
    "    X0 = np.column_stack((x0[:,0], np.zeros(len(x0))))\n",
    "\n",
    "    Y0 = exactSolution(X0[:,1], X0[:,0])\n",
    "    Y0 = Y0[:, None]\n",
    "\n",
    "    # Boundary Conditions (normalization builtin)\n",
    "    idx_t_lb = np.random.choice(t.shape[0], int(n_bc/2), replace=False)\n",
    "    BC_1_x_t = np.column_stack((x_norm, np.ones(t.shape[0]) * t_norm[0]))[idx_t_lb]\n",
    "    BC_1_y = y_true_norm[0, :]\n",
    "    BC_1_y = BC_1_y[idx_t_lb, None] \n",
    "\n",
    "    idx_t_ub = np.random.choice(t.shape[0], int(n_bc/2), replace=False)\n",
    "    BC_2_x_t = np.column_stack((x_norm, np.ones(t.shape[0]) * t_norm[-1]))[idx_t_ub]\n",
    "    BC_2_y = y_true_norm[-1, :] \n",
    "    BC_2_y = BC_2_y[idx_t_ub, None] \n",
    "\n",
    "    # Create collocation points with latin hypercube sampling\n",
    "    X_T_domain = lb + (ub - lb) * lhs(2, n_domain)\n",
    "\n",
    "    Y_domain = exactSolution(X_T_domain[:,1], X_T_domain[:,0])\n",
    "    Y_domain = Y_domain[:, None]\n",
    "\n",
    "    #Normalization of domain and initial conditions\n",
    "    '''\n",
    "    X0[:,0] = normalize(X0[:,0])\n",
    "    Y0 = normalize(Y0)\n",
    "\n",
    "    X_T_domain[:,1] = normalize(X_T_domain[:,1])\n",
    "    X_T_domain[:,0] = normalize(X_T_domain[:,0])\n",
    "    Y_domain = normalize(Y_domain)\n",
    "    '''\n",
    "    \n",
    "    X0[:,0] = standardize(X0[:,0])\n",
    "    Y0 = standardize(Y0)\n",
    "\n",
    "    X_T_domain[:,1] = standardize(X_T_domain[:,1])\n",
    "    X_T_domain[:,0] = standardize(X_T_domain[:,0])\n",
    "    Y_domain = standardize(Y_domain)\n",
    "    \n",
    "    x0 = torch.tensor(X0[:, 0], requires_grad=True).view(-1,1).float().to(device)\n",
    "    t0 = torch.tensor(X0[:, 1], requires_grad=True).view(-1,1).float().to(device)\n",
    "    y0 = torch.tensor(Y0).float().to(device)\n",
    "\n",
    "    x_lb = torch.tensor(BC_1_x_t[:, 0], requires_grad=True).view(-1,1).float().to(device)\n",
    "    t_lb = torch.tensor(BC_1_x_t[:, 1], requires_grad=True).view(-1,1).float().to(device)\n",
    "    y_lb = torch.tensor(BC_1_y).float().to(device)\n",
    "\n",
    "    x_ub = torch.tensor(BC_2_x_t[:, 0], requires_grad=True).view(-1,1).float().to(device)\n",
    "    t_ub = torch.tensor(BC_2_x_t[:, 1], requires_grad=True).view(-1,1).float().to(device)\n",
    "    y_ub = torch.tensor(BC_2_y).float().to(device)\n",
    "\n",
    "    x_domain = torch.tensor(X_T_domain[:, 0], requires_grad=True).view(-1,1).float().to(device)\n",
    "    t_domain = torch.tensor(X_T_domain[:, 1], requires_grad=True).view(-1,1).float().to(device)\n",
    "    y_domain = torch.tensor(Y_domain).float().to(device)\n",
    "\n",
    "    x_norm = torch.tensor(x_norm, requires_grad=True).view(-1,1).float().to(device)\n",
    "    t_norm = torch.tensor(t_norm, requires_grad=True).view(-1,1).float().to(device)\n",
    "    y_true_norm = torch.tensor(y_true_norm).float().to(device)\n",
    "\n",
    "    return x0, t0, y0, x_lb, t_lb, y_lb, x_ub, t_ub, y_ub, x_domain, t_domain, y_domain, x_norm, t_norm, y_true_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "68404050",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 layers, losstype, n_batch,\n",
    "                 t0, x0, y0, \n",
    "                 t_lb, x_lb, y_lb, \n",
    "                 t_ub, x_ub, y_ub, \n",
    "                 t_domain, x_domain, y_domain, x_norm, t_norm, y_norm):\n",
    "        \n",
    "        super(PINN, self).__init__() \n",
    "\n",
    "        self.activation = nn.Tanh() # ACTIVATION function\n",
    "        self.layers = layers\n",
    "        self.losstype = losstype\n",
    "        self.n_batch = n_batch\n",
    "        \n",
    "        #self.T_weight = nn.Tensor.ones(self.n_batch)\n",
    "        \n",
    "        #self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)]) # layer structure (INCLUDES Glorot-Xavier inizialization)\n",
    "        \n",
    "        self.linears = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(layers) - 1): #Create layers\n",
    "            self.linears.append(nn.Linear(layers[i], layers[i+1]))  # Fully connected layer\n",
    "            \n",
    "            # Add dropout layers\n",
    "            if i < (len(layers) - 2):  # Avoid dropout on the output layer\n",
    "                self.linears.append(nn.Dropout(0.00000001)) # p = probability of nullify each element of the tensor\n",
    "\n",
    "        \n",
    "        self.s_list = {}\n",
    "        self.v_list = {}\n",
    "   \n",
    "        for i in range(0, len(self.linears), 2): #(modified Glorot-Xavier inizialization + random weight factorization memorization)\n",
    "            #n = self.linears[i].in_features\n",
    "            #gx = 1 / np.sqrt(n)\n",
    "            #self.linears[i].weight.data.normal_(0, gx) \n",
    "            #self.linears[i].bias.data.fill_(0)\n",
    "            \n",
    "            mean = 1.0\n",
    "            std = 0.1\n",
    "        \n",
    "            w = self.linears[i].weight\n",
    "            \n",
    "            # Generate scaling vector s and pivot matrix v for weight factorization\n",
    "            s = mean + torch.normal(mean, std, size=(w.shape[-1],))\n",
    "            s = torch.exp(s)\n",
    "            self.s_list[f\"s_{i}\"] = nn.Parameter(s, requires_grad=True)\n",
    "            self.register_parameter(f\"s_{i}\", self.s_list[f\"s_{i}\"])  # Register the parameter\n",
    "            \n",
    "            v = w / s\n",
    "            self.v_list[f\"v_{i}\"] = nn.Parameter(v, requires_grad=True)\n",
    "            self.register_parameter(f\"v_{i}\", self.v_list[f\"v_{i}\"]) # Register the parameter\n",
    "        \n",
    "        \n",
    "        self.t0 = t0\n",
    "        self.x0 = x0\n",
    "        self.y0 = y0\n",
    "        \n",
    "        self.t_lb = t_lb\n",
    "        self.x_lb = x_lb\n",
    "        self.y_lb = y_lb\n",
    "        \n",
    "        self.t_ub = t_ub\n",
    "        self.x_ub = x_ub\n",
    "        self.y_ub = y_ub\n",
    "        \n",
    "        self.t_domain = t_domain\n",
    "        self.x_domain = x_domain\n",
    "        self.y_domain = y_domain\n",
    "\n",
    "        self.x_norm = x_norm\n",
    "        self.t_norm = t_norm\n",
    "        self.y_norm = y_norm\n",
    "        \n",
    "        # Batches portions too\n",
    "        self.batch_x0 = 0\n",
    "        self.batch_t0 = 0\n",
    "        self.batch_y0 = 0 \n",
    "\n",
    "        self.batch_x_ub = 0\n",
    "        self.batch_t_ub = 0\n",
    "        self.batch_y_ub = 0\n",
    "        \n",
    "        self.batch_x_lb = 0 \n",
    "        self.batch_t_lb = 0\n",
    "        self.batch_y_lb = 0\n",
    "        \n",
    "        self.batch_x_domain = 0\n",
    "        self.batch_t_domain = 0\n",
    "        self.batch_y_domain = 0\n",
    "        \n",
    "        self.optimizer = []\n",
    "        self.train_loss_history = []\n",
    "\n",
    "    \n",
    "    def get_factorized_weight(self, i):        \n",
    "        b = self.linears[i].bias\n",
    "\n",
    "        s = self.s_list[f\"s_{i}\"]\n",
    "        v = self.v_list[f\"v_{i}\"]\n",
    "        \n",
    "        return s * v, b\n",
    "    \n",
    "    \n",
    "    def forward(self, X): # Forward pass using decomposed weights with dropout and skip connections\n",
    "        a = X.float()\n",
    "        \n",
    "        for i in range(0, len(self.linears), 2):  # Skip the dropout layers\n",
    "            \n",
    "            a_prev = a\n",
    "            \n",
    "            kernel, b = self.get_factorized_weight(i)\n",
    "            a = torch.matmul(a_prev, kernel.T) + b  \n",
    "\n",
    "            #a = self.linears[i](a_prev)\n",
    "            \n",
    "            #Apply activation + dropout only for hidden layers\n",
    "            if i < (len(self.linears) - 1):  \n",
    "                a = self.activation(a)\n",
    "                a = self.linears[i+1](a)\n",
    "                \n",
    "                if 0 < i : #Skip connections are activated only after the input layer, included the output\n",
    "                    if a.shape != a_prev.shape: #In case of layers of different size\n",
    "                    # Apply a 1x1 linear transformation to match dimensions, but after activation\n",
    "                        projection = nn.Linear(a_prev.shape[1], a.shape[1], bias=False).to(a.device)\n",
    "                        a_prev = projection(a_prev) \n",
    "                    \n",
    "                    a += a_prev\n",
    "\n",
    "        return a\n",
    "        \n",
    "    \n",
    "    def network_prediction(self, t, x):\n",
    "        \n",
    "        return self.forward(torch.cat((t, x), 1))\n",
    "\n",
    "    \n",
    "    def PDE_prediction(self, t, x): # Compute the differential equation\n",
    "        N = self.network_prediction(t, x)\n",
    "        dN_dt = self.get_derivative(N, t, 1)\n",
    "        #dN_dxx = self.get_derivative(N, x, 2)\n",
    "        #f = dN_dt - dN_dxx + torch.exp(-t)*(torch.sin(np.pi*x) - np.pi*np.pi*torch.sin(np.pi*x))\n",
    "        f =  dN_dt - torch.pow(N, 2) - 1 \n",
    "        \n",
    "        return f\n",
    "\n",
    "    \n",
    "    def get_derivative(self, y, x, n): # General formula to compute the n-th order derivative of y = f(x) with respect to x\n",
    "        if n == 0:  # (n is the order if the derivative)\n",
    "            return y\n",
    "        else:\n",
    "            dy_dx = torch.autograd.grad(y, x, torch.ones_like(y).to(device), create_graph=True, retain_graph=True, allow_unused=True)[0]\n",
    "        \n",
    "        return self.get_derivative(dy_dx, x, n - 1)\n",
    "\n",
    "\n",
    "    def loss_IC(self):\n",
    "        y_pred_IC = self.network_prediction(self.batch_t0, self.batch_x0)\n",
    "        \n",
    "        if self.losstype == 'mse':\n",
    "            loss_IC = torch.mean((self.batch_y0 - y_pred_IC) ** 2).to(device)\n",
    "            \n",
    "        elif self.losstype == 'logcosh':\n",
    "            loss_IC = torch.mean(torch.log(torch.cosh(self.batch_y0 - y_pred_IC))).to(device)\n",
    "    \n",
    "        return loss_IC     \n",
    "\n",
    "    \n",
    "    def loss_BC(self, part):\n",
    "        if self.losstype == 'mse':\n",
    "            if part == 0:\n",
    "                y_pred_BC = self.network_prediction(self.batch_t_lb, self.batch_x_lb)\n",
    "                loss_BC = torch.mean((self.batch_y_lb - y_pred_BC)**2).to(device)\n",
    "            else:\n",
    "                y_pred_BC = self.network_prediction(self.batch_t_ub, self.batch_x_ub)\n",
    "                loss_BC = torch.mean((self.batch_y_ub - y_pred_BC)**2).to(device)\n",
    "        \n",
    "        elif self.losstype == 'logcosh':\n",
    "           if part == 0:\n",
    "               y_pred_BC = self.network_prediction(self.batch_t_lb, self.batch_x_lb)\n",
    "               loss_BC = torch.mean(torch.log(torch.cosh(self.batch_y_lb - y_pred_BC))).to(device)\n",
    "\n",
    "           else:\n",
    "               y_pred_BC = self.network_prediction(self.batch_t_ub, self.batch_x_ub)\n",
    "               loss_BC = torch.mean(torch.log(torch.cosh(self.batch_y_ub - y_pred_BC))).to(device)\n",
    "        \n",
    "        return loss_BC\n",
    "    \n",
    "\n",
    "    def loss_BC_symmetric(self):\n",
    "        loss_BC_symmetric = 0.0\n",
    "        \n",
    "        if self.losstype == 'mse':\n",
    "            y_pred = self.network_prediction(self.t_norm, self.x_norm)\n",
    "            loss_BC_symmetric += torch.mean((y_pred[:,0] - y_pred[:,-1])**2).to(device)\n",
    "            loss_BC_symmetric += torch.mean((self.y_norm[:,0] - y_pred[:,0])**2).to(device)\n",
    "            loss_BC_symmetric += torch.mean((self.y_norm[:,-1] - y_pred[:,-1])**2).to(device)\n",
    "        \n",
    "        elif self.losstype == 'logcosh':\n",
    "            y_pred = self.network_prediction(self.t_norm, self.x_norm)\n",
    "            loss_BC_symmetric += torch.mean(torch.log(torch.cosh(y_pred[:,0] - y_pred[:,-1]))).to(device)\n",
    "            loss_BC_symmetric += torch.mean(torch.log(torch.cosh(self.y_norm[:,0] - y_pred[:,0]))).to(device)\n",
    "            loss_BC_symmetric += torch.mean(torch.log(torch.cosh(self.y_norm[:,-1] - y_pred[:,-1]))).to(device)\n",
    "            \n",
    "        return loss_BC_symmetric\n",
    "\n",
    "    \n",
    "    def loss_interior(self):\n",
    "        f_pred = self.PDE_prediction(self.batch_t_domain, self.batch_x_domain)\n",
    "        \n",
    "        if self.losstype == 'mse':\n",
    "            loss_interior = torch.mean(torch.pow(f_pred,2)).to(device)\n",
    "            \n",
    "        elif self.losstype == 'logcosh':\n",
    "            loss_interior = torch.mean(torch.log(torch.cosh(f_pred))).to(device)\n",
    "            \n",
    "        '''\n",
    "        eps = 0.7 # T_weight slope: recommended eps = 1\n",
    "\n",
    "        loss_interior = torch.mean(self.T_weight*torch.log(torch.cosh(f_pred))).to(device)\n",
    "\n",
    "        for i in range(1, self.n_batch):\n",
    "            self.T_weight[i] = torch.exp(-eps*torch.sum(loss_interior[1:i]))\n",
    "        '''        \n",
    "        return loss_interior\n",
    "\n",
    "    \n",
    "    def get_training_history(self):\n",
    "        loss_his = np.array(self.train_loss_history)\n",
    "        total_loss, loss_IC, loss_BC, loss_domain, loss_BC_symmetric = np.split(loss_his, 5, axis=1) \n",
    "        \n",
    "        return total_loss, loss_IC, loss_BC, loss_domain, loss_BC_symmetric \n",
    "\n",
    "\n",
    "    def loss_func(self):\n",
    "        loss_IC = self.loss_IC()\n",
    "        loss_BC = self.loss_BC(0) + self.loss_BC(1) \n",
    "        loss_domain = self.loss_interior()\n",
    "        loss_BC_symmetric = self.loss_BC_symmetric()\n",
    "        \n",
    "        return loss_IC, loss_BC, loss_domain, loss_BC_symmetric \n",
    "\n",
    "    \n",
    "    def closuring(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss_0, loss_b, loss_f, loss_b_sym = self.loss_func() \n",
    "        total_loss = loss_0 + loss_b + loss_f + loss_b_sym \n",
    "        total_loss.backward(retain_graph=True)\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    \n",
    "    def train_network(self, epochs, optim, batch_size, learning_rate, regularization):\n",
    "        if optim == 'Adam':\n",
    "            self.optimizer = torch.optim.Adam(self.parameters(), lr = learning_rate, weight_decay = regularization, amsgrad=True)\n",
    "            \n",
    "        elif optim == 'AdamW':\n",
    "            self.optimizer = torch.optim.AdamW(self.parameters(), lr = learning_rate, weight_decay = regularization, amsgrad=True)\n",
    "\n",
    "        elif optim == 'L-BFGS':\n",
    "            self.optimizer = torch.optim.LBFGS(self.parameters(), lr = learning_rate, max_iter= 15, history_size=120)\n",
    "\n",
    "        elif optim == 'SGD':  \n",
    "            self.optimizer = torch.optim.SGD(self.parameters(), lr = learning_rate, momentum=0.9, weight_decay = regularization, nesterov=True)\n",
    "\n",
    "        elif optim == 'Adagrad':  \n",
    "            self.optimizer = torch.optim.Adagrad(self.parameters(), lr = learning_rate, lr_decay = learning_rate*1e-2, weight_decay = regularization)\n",
    "        \n",
    "        elif optim == 'ASGD': \n",
    "            self.optimizer = torch.optim.ASGD(self.parameters(), lr = learning_rate, alpha = 0.8, weight_decay = regularization, t0 = 1e3)\n",
    "            \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            loss_IC, loss_BC, loss_domain, loss_BC_symmetric = 0.0, 0.0, 0.0, 0.0 \n",
    "\n",
    "            # Mini-batch training for Initial Conditions (IC)\n",
    "            for self.batch_x0, self.batch_t0, self.batch_y0 in batch_generator(self.x0, self.t0, self.y0, batch_size):\n",
    "                loss_IC_batch = self.loss_IC()\n",
    "                loss_IC += loss_IC_batch\n",
    "            \n",
    "            # Mini-batch training for Boundary Conditions (BC)\n",
    "            for self.batch_x_lb, self.batch_t_lb, self.batch_y_lb in batch_generator(self.x_lb,  self.t_lb, self.y_lb, batch_size):\n",
    "                loss_BC_batch_lb = self.loss_BC(0) # 0 for lower bound\n",
    "                loss_BC += loss_BC_batch_lb\n",
    "\n",
    "            for self.batch_x_ub, self.batch_t_ub, self.batch_y_ub in batch_generator(self.x_ub, self.t_ub, self.y_ub, batch_size):\n",
    "                loss_BC_batch_ub = self.loss_BC(1) # 1 for upper bound\n",
    "                loss_BC += loss_BC_batch_ub\n",
    "            \n",
    "            # Mini-batch training for Domain Loss (interior) and update temporal weights\n",
    "            for self.batch_x_domain, self.batch_t_domain, self.batch_y_domain in batch_generator(self.x_domain, self.t_domain, self.y_domain, batch_size): \n",
    "                loss_domain_batch = self.loss_interior()\n",
    "                loss_domain += loss_domain_batch\n",
    "\n",
    "            # Mini-batch training for Simmetric boundary (BC_symmetric)\n",
    "            loss_BC_symmetric += self.loss_BC_symmetric()\n",
    "\n",
    "            \n",
    "            if optim == 'AdamW' or optim == 'Adam':\n",
    "                # Total loss for this epoch\n",
    "                total_loss = loss_IC + loss_BC + loss_domain + loss_BC_symmetric  \n",
    "                self.train_loss_history.append([total_loss.cpu().detach(), loss_IC.cpu().detach(), loss_BC.cpu().detach(), loss_domain.cpu().detach(), loss_BC_symmetric.cpu().detach()]) #, \n",
    "                             \n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Calculate gradients\n",
    "                total_loss.backward()\n",
    "                \n",
    "                if epoch % 100 == 0:\n",
    "                    print(self.s_list[f\"s_{0}\"].grad)\n",
    "                    #print(self.linears[0].weight.grad)\n",
    "                \n",
    "                # Optimize the network parameters\n",
    "                self.optimizer.step() \n",
    "                \n",
    "            else:\n",
    "                # Total loss for this epoch\n",
    "                total_loss = loss_IC  + loss_BC + loss_domain + loss_BC_symmetric  \n",
    "                self.train_loss_history.append([total_loss.cpu().detach(), loss_IC.cpu().detach(), loss_BC.cpu().detach(), loss_domain.cpu().detach(), loss_BC_symmetric.cpu().detach()]) #, loss_BC.cpu().detach()\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss.backward(retain_graph=True)\n",
    "                # Optimize the network parameters (with closure)\n",
    "                self.optimizer.step(self.closuring)\n",
    "                \n",
    "            # Print out the loss every 100 epochs\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch ({optim}): {epoch}, Total Loss: {total_loss.detach().cpu().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "99f6cfbd-89b5-491e-b718-ad397a0b48e9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def exactSolution(t, x):\n",
    "    \n",
    "    return np.power(x, 2) + 1\n",
    "    \n",
    "x_len = 100\n",
    "t_len = 100\n",
    "\n",
    "'''\n",
    "x = np.random.uniform(-5, 5, x_len)\n",
    "x = np.sort(x)\n",
    "x = x.reshape(-1,1)\n",
    "\n",
    "t = np.random.uniform(0, 5, t_len)\n",
    "t = np.sort(t)\n",
    "t = t.reshape(-1,1)\n",
    "'''\n",
    "\n",
    "x = np.linspace(-5, 5, x_len).reshape(-1,1) # Space domain (must be same space as time one (for shuffle purposes))\n",
    "t = np.linspace(0, 5, t_len).reshape(-1,1) # Time domain\n",
    "\n",
    "X, T = np.meshgrid(x[:, 0], t[:, 0]) \n",
    "\n",
    "y_true = exactSolution(T, X) # NOT normalized and NOT reshuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "1424f514",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "layers = [2, 256, 128, 256, 128, 256, 1] #[2, 8, 16, 8, 32, 8, 16, 8, 1]\n",
    "losstype = 'mse'\n",
    "n_batch = 64\n",
    "epochs = 1000\n",
    "L_rate = 0.0001\n",
    "lambda_reg = 0 #0.00002\n",
    "#frac = 4/5\n",
    "\n",
    "x0, t0, y0, x_lb, t_lb, y_lb, x_ub, t_ub, y_ub, x_domain, t_domain, y_domain, x_norm, t_norm, y_norm = data_handler(x, t, y_true, 100, 200, x_len*t_len)\n",
    "\n",
    "model = PINN(layers, losstype, n_batch, t0, x0, y0, t_lb, x_lb, y_lb, t_ub, x_ub, y_ub, t_domain, x_domain, y_domain, x_norm, t_norm, y_norm).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2310d65",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1020, 0.1296])\n",
      "Epoch (Adam): 0, Total Loss: 175.61492919921875\n",
      "tensor([-4.4922,  0.9188])\n",
      "Epoch (Adam): 100, Total Loss: 97.13905334472656\n",
      "tensor([0.0125, 0.0860])\n",
      "Epoch (Adam): 200, Total Loss: 81.4858169555664\n",
      "tensor([-2.3695,  0.2231])\n",
      "Epoch (Adam): 300, Total Loss: 77.45077514648438\n",
      "tensor([-2.7493,  0.0279])\n",
      "Epoch (Adam): 400, Total Loss: 76.01690673828125\n",
      "tensor([0.4121, 0.0855])\n",
      "Epoch (Adam): 500, Total Loss: 75.29888916015625\n",
      "tensor([10.7188,  0.5732])\n",
      "Epoch (Adam): 600, Total Loss: 75.25588989257812\n",
      "tensor([-4.0501,  0.1117])\n",
      "Epoch (Adam): 700, Total Loss: 71.16608428955078\n",
      "tensor([2.8571, 0.3601])\n",
      "Epoch (Adam): 800, Total Loss: 73.84877014160156\n"
     ]
    }
   ],
   "source": [
    "model.train_network(epochs, 'Adam', n_batch, L_rate, lambda_reg) # L-BFGS, AdamW, SGD, ASGD, Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35676103",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "total_loss, loss_IC, loss_BC, loss_domain, loss_BC_symmetric = model.get_training_history() \n",
    "#total_loss, loss_IC, loss_BC, loss_domain, loss_prediction = model.get_training_history()\n",
    "\n",
    "# training/validation losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(total_loss, label='Total Loss')\n",
    "plt.plot(loss_IC, label='Initial Condition Loss')\n",
    "plt.plot(loss_BC, label='Boundary Condition Loss')\n",
    "plt.plot(loss_domain, label='Domain Loss')\n",
    "plt.plot(loss_BC_symmetric, label='Symmetric-boundary Loss')\n",
    "#plt.plot(loss_prediction, label='Prediction Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a585f6db-5f53-4389-be59-3aab69d52691",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Get model predictions after training\n",
    "x_valid = np.linspace(-5, 5, n_valid).reshape(-1,1) # Space domain (must be same space as time one (for shuffle purposes))\n",
    "t_valid = np.linspace(0, 5, n_valid).reshape(-1,1) # Time domain\n",
    "\n",
    "X_valid, T_valid = np.meshgrid(x_valid[:, 0], t_valid[:, 0]) \n",
    "\n",
    "y_true_valid = exactSolution(T_valid, X_valid) \n",
    "\n",
    "\n",
    "x0_val, t0_val, y0_val, x_lb_val, x_lb_val, t_lb_val, y_lb_val, x_ub_val, t_ub_val, y_ub_val, x_domain_val, t_domain_val, y_domain_val = data_handler(x_valid, t_valid, y_true_valid, 50, 80, 1000)\n",
    "\n",
    "model_valid = PINN(model.layers, model.losstype, t0_val, x0_val, y0_val, t_lb_val, x_lb_val, y_lb_val, t_ub_val, x_ub_val, y_ub_val, t_domain_val, x_domain_val, y_domain_val).to(device)\n",
    "\n",
    "model_valid.train_network(epochs, 'Adam', n_batch, L_rate, lambda_reg)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d613ee-5314-4670-8757-d8033b7dd484",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "total_loss_val, loss_IC_val, loss_BC_val, loss_domain_val, loss_BC_symmetric_val = model.get_training_history()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11efd86",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Get model predictions after training\n",
    "n_valid = 40\n",
    "\n",
    "'''\n",
    "x_valid = np.random.uniform(-5, 5, n_valid)\n",
    "x_valid = np.sort(x_valid)\n",
    "x_valid = x_valid.reshape(-1,1)\n",
    "\n",
    "t_valid = np.random.uniform(0, 5, n_valid)\n",
    "t_valid = np.sort(t_valid)\n",
    "t_valid = t_valid.reshape(-1,1)\n",
    "'''\n",
    "\n",
    "x_valid = np.linspace(-5, 5, n_valid).reshape(-1,1) # Space domain (must be same space as time one (for shuffle purposes))\n",
    "t_valid = np.linspace(0, 5, n_valid).reshape(-1,1) # Time domain\n",
    "\n",
    "X_valid, T_valid = np.meshgrid(x_valid[:, 0], t_valid[:, 0]) \n",
    "\n",
    "y_true_valid = exactSolution(T_valid, X_valid) \n",
    "\n",
    "'''\n",
    "x_valid = normalize(x_valid)\n",
    "t_valid = normalize(t_valid)\n",
    "y_true_valid = normalize(y_true_valid)\n",
    "'''\n",
    "\n",
    "x_valid = standardize(x_valid)\n",
    "t_valid = standardize(t_valid)\n",
    "y_true_valid = standardize(y_true_valid)\n",
    "\n",
    "X_valid, T_valid = np.meshgrid(x_valid[:, 0], t_valid[:, 0])                                                                                                  \n",
    "\n",
    "idx = torch.randperm(n_valid)\n",
    "#idx_0 = torch.torch.arange(0, 100, 1).int()\n",
    "\n",
    "x_val = x_valid[idx]\n",
    "t_val = t_valid[idx]\n",
    "\n",
    "X_val, T_val = np.meshgrid(x_val[:, 0], t_val[:, 0])\n",
    "\n",
    "t_pred = torch.tensor(T_val.flatten(), requires_grad=True).view(-1, 1).float().to(device)\n",
    "x_pred = torch.tensor(X_val.flatten(), requires_grad=True).view(-1, 1).float().to(device)\n",
    "\n",
    "# Predict with the trained model\n",
    "y_pred = model.network_prediction(t_pred, x_pred).cpu().detach().numpy()\n",
    "\n",
    "# Reshape to match the shape of the true values\n",
    "#y_pred = y_pred.reshape(n_valid, n_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e02c60-6222-4676-a504-7e01d54cd5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(X_valid, T_valid, y_true_valid, levels=int(n_valid/2), cmap='jet', alpha=0.7)  # Use alpha for transparency of background\n",
    "plt.colorbar(label='Exact solution')\n",
    "plt.contour(X_val, T_val, y_pred, levels=int(n_valid/2), cmap='jet', alpha=0.8)\n",
    "plt.colorbar(label='Estimated solution')\n",
    "plt.xlabel('X (Space Domain)')\n",
    "plt.ylabel('T (Time Domain)')\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "# Subplot 1: Plot true values (exact solution)\n",
    "c1 = axes[0].contourf(X_valid, T_valid, y_true_valid, levels=int(n_valid/2), cmap='jet', alpha=1)\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('T')\n",
    "fig.colorbar(c1, ax=axes[0], label='Exact solution')\n",
    "\n",
    "# Subplot 2: Plot only predictions (estimated solution)\n",
    "c2 = axes[1].scatter(X_val, T_val, c=y_pred, cmap='jet', s=n_valid/2, alpha=1)\n",
    "axes[1].set_xlabel('X')\n",
    "axes[1].set_ylabel('T')\n",
    "fig.colorbar(c2, ax=axes[1], label='Estimated solution')\n",
    "#plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a0d3a-90ba-406f-9b05-3cad3622f41b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Plot Initial conditions and Boundary conditions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x0.detach().numpy(), t0.detach().numpy(), c=y0.flatten(), cmap='jet')\n",
    "plt.scatter(x_lb.detach().numpy(), t_lb.detach().numpy(), c=y_lb.flatten(), cmap='jet')\n",
    "plt.scatter(x_ub.detach().numpy(), t_ub.detach().numpy(), c=y_ub.flatten(), cmap='jet')\n",
    "plt.colorbar(label='Conditions value')\n",
    "plt.xlabel('Position (x)')\n",
    "plt.ylabel('Time (t)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "toc": {
   "base_numbering": -2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
