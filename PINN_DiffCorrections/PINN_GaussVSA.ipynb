{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "92267208-b9b1-4f7e-ab34-f65a8b5ba557",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7bea2ffe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import glob\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "336603ab",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "65012647",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Working on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "09937716-7564-4ebe-af28-533974ce7ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  \n",
    "                                                            !!!\n",
    "    \n",
    "    The data can grow in number CONSIDERABLY if one considers all the observations of one night and creates triplets by randomly selecting them in this way:\n",
    "    following the chronological time of observation, after 3 of them have been achieved, one can combine randomly the observations happened\n",
    "    after the 3rd one with the first ones (NOT EXCLUSIVE), thus creating triplets of observations relatively close in time.\n",
    "    Repeating this procedure for all the nights is possible to end up with multiples entries from each night.\n",
    "    \n",
    "                                                            !!!\n",
    "'''  \n",
    "\n",
    "columns_input  = ['alpha','delta','alpha_dot','delta_dot']\n",
    "columns_output = ['rho','rho_dot']\n",
    "\n",
    "def load_observations(prefix, input_names = columns_input, output_names = columns_output):\n",
    "    \n",
    "    ar_files = sorted(glob.glob(f\"{'datas_orbfit'}/{prefix}_N*.ar_sample\"))\n",
    "    \n",
    "    t_list, X_list, Y_list = [], [], []\n",
    "\n",
    "    for ar in ar_files:\n",
    "        # read attributable line\n",
    "        df_in = pd.read_csv(ar, sep='=', header=None, skiprows=1, nrows=1, names=['tag','values'], engine='python')\n",
    "        attrib = [float(x) for x in df_in.loc[0,'values'].split()]\n",
    "        s_attrib = pd.Series(attrib, index=columns_input)\n",
    "\n",
    "        # read first data row (rho, rho_dot)\n",
    "        df_out = pd.read_csv(ar, comment='%', sep='\\s+', skiprows= 8, nrows = 1, names=['i','j','rho','rho_dot','cc','succ','chi','Jac','E_Sun','E_Earth','H'], engine='python')\n",
    "        s_out = df_out.loc[0, columns_output]\n",
    "\n",
    "        # read epoch from mov_sample\n",
    "        mov = ar.replace('.ar_sample', '.mov_sample')\n",
    "        df_t = pd.read_csv(mov, sep='=', header=None, skiprows=1, nrows=1, names=['tag','epoch_str'], engine='python')\n",
    "        epoch = float(df_t.loc[0,'epoch_str'].split()[0])\n",
    "        \n",
    "        t_list.append(epoch)\n",
    "        X_list.append(s_attrib.values)\n",
    "        Y_list.append(s_out.values)\n",
    "\n",
    "    return torch.tensor(t_list), torch.tensor(X_list), torch.tensor(Y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "678e797c-26df-4101-80a5-6b62543c55d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gravitational constant for the Sun \n",
    "#mu = np.power(0.01720209895, 2) #([AU]^3/[day]^2)\n",
    "mu = 398600.4418 #([km]^3/[s]^2)\n",
    "\n",
    "#Characteristics quantities of the system\n",
    "AU =  149597870.7 # km in Au\n",
    "day = 60*60*24 # s in day\n",
    "L_c = 3.844e5 # (km) mean orbital radius of the moon\n",
    "T_c = np.sqrt(np.power(L_c, 3) / mu) # (seconds) orbital period of the moon (2.361e6 [s]) \n",
    "V_c = L_c / T_c # (km/s) characteristic velocity\n",
    "\n",
    "t_0 = torch.tensor([57291.95817]).float().to(device) # t0 for 2015RN35\n",
    "\n",
    "# Solve Kepler's equation for elliptic orbits using Newton-Raphson iterations\n",
    "def solve_kepler(M, e, tol=1e-10, maxiter=10):\n",
    "    E = M.copy()\n",
    "    for _ in range(maxiter):\n",
    "        f  = E - e * np.sin(E) - M\n",
    "        fp = 1 - e * np.cos(E)\n",
    "        dE = -f / fp\n",
    "        E += dE\n",
    "        if np.all(np.abs(dE) < tol):\n",
    "            break\n",
    "    return E\n",
    "\n",
    "def equinoctial_to_cartesian(equin_coord):  #(a, h, k, p, q, L) --> (r; v)\n",
    "    \n",
    "    e = np.sqrt(equin_coord[1]**2 + equin_coord[2]**2)\n",
    "    omega_plus_Omega = np.arctan2(equin_coord[1], equin_coord[2])\n",
    "\n",
    "    i = 2 * np.arctan(np.sqrt(equin_coord[3]**2 + equin_coord[4]**2))\n",
    "    Omega = np.arctan2(equin_coord[3], equin_coord[4])\n",
    "\n",
    "    M = equin_coord[5] - omega_plus_Omega\n",
    "    M = np.mod(M, 2*np.pi)\n",
    "    E = solve_kepler(M,e)\n",
    "   \n",
    "    nu = 2 * np.arctan2(np.sqrt(1+e)*np.sin(E/2), np.sqrt(1-e)*np.cos(E/2))\n",
    "\n",
    "    # Perifocal position and velocity\n",
    "    r_pqw = equin_coord[0] * (1 - e*np.cos(E))\n",
    "    r_vec_pqw = r_pqw * np.array([np.cos(nu), np.sin(nu), 0])\n",
    "    v_vec_pqw = np.sqrt(mu * equin_coord[0]) / r_pqw * np.array([-np.sin(E), np.sqrt(1 - e**2)*np.cos(E), 0])\n",
    "\n",
    "    # Rotation matrices\n",
    "    cos_Om = np.cos(Omega)\n",
    "    sin_Om = np.sin(Omega)\n",
    "    cos_w = np.cos(omega_plus_Omega - Omega)\n",
    "    sin_w = np.sin(omega_plus_Omega - Omega)\n",
    "    cos_i = np.cos(i)\n",
    "    sin_i = np.sin(i)\n",
    "\n",
    "    # Perifocal to ECI transformation matrix\n",
    "    R = np.array([\n",
    "        [cos_Om*cos_w - sin_Om*sin_w*cos_i, -cos_Om*sin_w - sin_Om*cos_w*cos_i, sin_Om*sin_i],\n",
    "        [sin_Om*cos_w + cos_Om*sin_w*cos_i, -sin_Om*sin_w + cos_Om*cos_w*cos_i, -cos_Om*sin_i],\n",
    "        [sin_w*sin_i,                        cos_w*sin_i,                       cos_i]\n",
    "    ])\n",
    "\n",
    "    # Convert to ECI\n",
    "    r = R @ r_vec_pqw\n",
    "    v = R @ v_vec_pqw\n",
    "\n",
    "    return r, v\n",
    "    \n",
    "def equinoctial_to_obs(equin_coord, r_obs=np.zeros(3), v_obs=np.zeros(3)): # here r_obs and v_obs are the coordinates of the observatory\n",
    "\n",
    "    # Equinoctial → Cartesian\n",
    "    r_ast, v_ast = equinoctial_to_cartesian(equin_coord)\n",
    "    \n",
    "    # Topocentric vectors\n",
    "    rho_vec = r_ast - r_obs\n",
    "    v_rel   = v_ast - v_obs\n",
    "\n",
    "    rho     = np.linalg.norm(rho_vec)\n",
    "    hat_rho = rho_vec / rho\n",
    "    rho_dot = hat_rho.dot(v_rel)\n",
    "\n",
    "    # Compute hat_rho_dot\n",
    "    rho_hat_dot = (v_rel - rho_dot*hat_rho) / rho\n",
    "\n",
    "    # Angles\n",
    "    alpha = np.arctan2(hat_rho[1], hat_rho[0])\n",
    "    delta = np.arcsin(hat_rho[2])\n",
    "\n",
    "    # Angular rates\n",
    "    denom = np.power(hat_rho[0], 2) + np.power(hat_rho[1], 2)\n",
    "    alpha_dot = (hat_rho[0]*rho_hat_dot[1] - hat_rho[1]*rho_hat_dot[0]) / denom\n",
    "    delta_dot = rho_hat_dot[2] / np.cos(delta)\n",
    "\n",
    "    return alpha, delta, alpha_dot, delta_dot, rho, rho_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fba67f89-afb4-4f0a-8eda-dabddc5410c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for data normalization\n",
    "def normalize(inputs, export_minmax): # from standard to [-1, 1]\n",
    "    inputs_min = inputs.min(dim=1, keepdim=True).values\n",
    "    inputs_max = inputs.max(dim=1, keepdim=True).values\n",
    "    \n",
    "    if export_minmax == True:\n",
    "        return 2*(inputs - inputs_min) / (inputs_max - inputs_min + 1e-8) - 1, inputs_min, inputs_max\n",
    "    else:\n",
    "        return 2*(inputs - inputs_min) / (inputs_max - inputs_min + 1e-8) - 1\n",
    "\n",
    "def denormalize(inputs, inputs_min, inputs_max): # from [-1, 1] to standard\n",
    "    \n",
    "    return ((inputs + 1)/2)*(inputs_max - inputs_min + 1e-8) + inputs_min\n",
    "\n",
    "#funtion for data a-dimensionalization\n",
    "def non_dimensionalise(x, t, y, t0 = t_0): # x = (alpha, delta, alpha_dot, delta_dot), t = t_obs, y = (rho, rho_dot)\n",
    "    x_nodim = torch.empty_like(x)\n",
    "    x_nodim[:,0] = x[:,0] # dimensionless      \n",
    "    x_nodim[:,1] = x[:,1] # dimensionless               \n",
    "    x_nodim[:,2] = (x[:,2] / day) * T_c # ([rad/AU] → [rad/s]) → dimensionless                   \n",
    "    x_nodim[:,3] = (x[:,3] / day) * T_c # ([rad/AU] → [rad/s]) → dimensionless                              \n",
    "    \n",
    "    t_mjd = (t - t0) # [s] \n",
    "    \n",
    "    t_nodim = t_mjd / T_c # [s] → dimensionless \n",
    "\n",
    "    t_nodim = t_nodim.view(-1, 1)\n",
    "    \n",
    "    y_nodim = torch.empty_like(y)\n",
    "    \n",
    "    y_nodim[:,0] = (y[:,0] * AU) / L_c # ([AU] → [km]) → dimensionless              \n",
    "    y_nodim[:,1] = (y[:,1] * (AU/day)) / V_c  # ([AU/day] → [km/s]) → dimensionless  \n",
    "\n",
    "    x_nodim.requires_grad_(True).float().to(device)\n",
    "    t_nodim.requires_grad_(True).float().to(device)\n",
    "    y_nodim.float().to(device)\n",
    "    \n",
    "    return x_nodim, t_nodim, y_nodim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "29fb837d-dc3a-4c5a-bd6a-e946c8c891af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to implement a fourier spectral expansion of the inputs (fourier_M = # of frequencies in the spectrum\n",
    "class FourierEmbedding(nn.Module):\n",
    "    def __init__(self, in_dim, mapping_size, scale):\n",
    "        super().__init__()\n",
    "        self.register_buffer('B', torch.randn(in_dim, mapping_size) * scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = self.B.to(x.device).to(x.dtype)\n",
    "        x_proj = 2 * np.pi * x @ B\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "68404050",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Main model\n",
    "class PINN_VSA(nn.Module):\n",
    "    \n",
    "    def __init__(self, layers, loss_type, n_batch, mu, *data, fourier_m = 64, fourier_scale = 10.0): \n",
    "        super(PINN_VSA, self).__init__() \n",
    "        \n",
    "        self.fourier = FourierEmbedding(in_dim=5, mapping_size=fourier_m, scale=fourier_scale)\n",
    "\n",
    "        L0 = 2 * fourier_m # size on input layer is bigger using fourier embedding\n",
    "        layers = [L0] + layers[1:]\n",
    "        \n",
    "        (self.t0, self.x0, self.y0, self.t_domain, self.x_domain, self.y_domain) = data\n",
    "        self.n_batch = n_batch\n",
    "        self.losstype = loss_type\n",
    "\n",
    "        self.mu = mu\n",
    "        \n",
    "        #---------------------------------------- Define layers ------------------------------------------\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activation = nn.Tanh() # Activation function\n",
    "        \n",
    "        self.s_list = {}\n",
    "        self.v_list = {}\n",
    "        biases = []\n",
    "        mean = 1.0\n",
    "        std = 0.1\n",
    "        \n",
    "        for i in range(len(layers) - 1): \n",
    "            \n",
    "            self.layers.append(nn.Linear(layers[i], layers[i+1], bias = True)) # Fully connected linear layer: the operation [out = w*input + b] will be made by hand with factorized weights\n",
    "            # Random weight factorization memorization\n",
    "            \n",
    "            W = self.layers[-1].weight.detach().clone()  # weights are bot trained but used only now to calculate their factorization\n",
    "            \n",
    "            s = torch.exp(torch.randn(layers[i+1])*std + mean)\n",
    "            v = W / s.unsqueeze(1)\n",
    "\n",
    "            self.s_list[f\"s_{i}\"] = nn.Parameter(s, requires_grad = True)\n",
    "            self.register_parameter(f\"s_{i}\", self.s_list[f\"s_{i}\"])  # Register the parameter\n",
    "            \n",
    "            self.v_list[f\"v_{i}\"] = nn.Parameter(v, requires_grad = True)\n",
    "            self.register_parameter(f\"v_{i}\", self.v_list[f\"v_{i}\"]) # Register the parameter\n",
    "            \n",
    "            biases.append(self.layers[-1].bias.requires_grad_(True))\n",
    "\n",
    "            self.layers[i].weight.requires_grad_(False) # we don't need to train these\n",
    "            self.layers[i].bias.requires_grad_(False) # we have copied the trainable ones in the list \"biases\" \n",
    "            \n",
    "        self.new_param = list(self.s_list.values()) + list(self.v_list.values()) + biases # new parameters to be trained\n",
    "        \n",
    "        #----------------------------------------------------------------------------------------------------------------------------\n",
    "        self.optimizer = None\n",
    "        self.train_loss_history = []\n",
    "        \n",
    "        #------------------------ For weighted loss procedures and temporal weights -------------------------------------------------\n",
    "        self.n_losses = 3\n",
    "        self.loss_weights = torch.ones(self.n_losses, device = device).requires_grad_(False) # Initialize losses weights\n",
    "        self.f = 50\n",
    "        self.alpha = 0.9\n",
    "        self.epsilon = 1.0\n",
    "\n",
    "    #------------------ Loop operations ---------------------------------------- \n",
    "    def get_factorized_weight(self, i):        \n",
    "        b = self.layers[i].bias\n",
    "\n",
    "        s = self.s_list[f\"s_{i}\"]\n",
    "        v = self.v_list[f\"v_{i}\"]\n",
    "        \n",
    "        return (s.view(-1,1) * v), b\n",
    "    \n",
    "        \n",
    "    def forward(self, X): # Forward pass using random decomposed weights\n",
    "\n",
    "        X_embed = self.fourier(X)\n",
    "        a = X_embed.float()\n",
    "        \n",
    "        for i in range(len(self.layers) - 1):\n",
    "            input_copy = a # for skip connections\n",
    "            \n",
    "            kernel, b = self.get_factorized_weight(i)\n",
    "            a = a @ kernel.T + b \n",
    "            \n",
    "            if i < (len(self.layers) - 1):\n",
    "                a = self.activation(a)\n",
    "\n",
    "                #Skip connections are activated only after the input layer\n",
    "                if i >= 1: \n",
    "                    if a.shape != input_copy.shape: #In case of layers of different size\n",
    "                        # Apply a 1x1 linear transformation to match dimensions, but after activation\n",
    "                        projection = nn.Linear(input_copy.shape[1], a.shape[1], bias=False).to(a.device)\n",
    "                        input_copy = projection(input_copy) \n",
    "                        projection.weight.requires_grad_(False)\n",
    "                        \n",
    "                    a = a + input_copy\n",
    "                     \n",
    "        kernel, b = self.get_factorized_weight(len(self.layers)-1)\n",
    "        a = a @ kernel.T + b \n",
    "                    \n",
    "        return a \n",
    "\n",
    "    \n",
    "    def network_prediction(self, t, x):\n",
    "\n",
    "        return self.forward(torch.cat([t, x], 1))\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "        \n",
    "    #--------------------------- PDEs ---------------------------------------------\n",
    "    def get_derivative(self, y, x, n): # General formula to compute the n-th order derivative of y = f(x) with respect to x\n",
    "        if n == 0:  # (n is the order if the derivative)\n",
    "            return y\n",
    "        else:\n",
    "            dy_dx = torch.autograd.grad(y, x, torch.ones_like(y).to(device), retain_graph=True, allow_unused=True, create_graph=True)[0]\n",
    "        \n",
    "        return self.get_derivative(dy_dx, x, n - 1)\n",
    "\n",
    "\n",
    "    def energy_conservation(self, t, x): # Compute the energy derivative in time\n",
    "        out = self.network_prediction(t, x) # (rho, rho_dot)\n",
    "        \n",
    "        att = torch.cat([x, out], dim=1)\n",
    "        r, v = self.attributable_to_cartesian(att)\n",
    "        \n",
    "        dv_dt = self.get_derivative(v, t, 1)\n",
    "        \n",
    "        if dv_dt == None:\n",
    "            dv_dt = torch.zeros_like(v)\n",
    "            \n",
    "        dv_dt.float().to(device)\n",
    "        \n",
    "        vr = (v * dv_dt).sum(dim=1)\n",
    "        rv = (r * v).sum(dim=1)\n",
    "        rnorm3 = torch.pow(r.norm(dim=1), 3)\n",
    "            \n",
    "        f =  vr - self.mu * ( rv / (rnorm3 + 1e-8) ) # v·dv_dt =  mu(r·v/(|r|^(3)))\n",
    "        \n",
    "        return f\n",
    "\n",
    "    '''\n",
    "    def curvature(self, t, x): #pg. 173 Milani-Gronchi\n",
    "        out = self.network_prediction(t, x) #(rho, rho_dot)\n",
    "\n",
    "        att = torch.stack((x[0], x[1], x[2], x[3], out[0], out[1])).detach().numpy()\n",
    "\n",
    "        r, v = self.attributable_to_cartesian(att.flatten())\n",
    "\n",
    "        eta = torch.norm(v).requires_grad_(True)\n",
    "        eta_dot = self.get_derivative(eta, t, 1)\n",
    "\n",
    "        f = eta_dot - \n",
    "    '''\n",
    "    #-------------------------------------------------------------------------------\n",
    "\n",
    "    def attributable_to_cartesian(self, att): # (alpha, delta, alpha_dot, delta_dot, rho, rho_dot) --> (r, v)\n",
    "        # unit line-of-sight\n",
    "        cd, sd = torch.cos(att[:,1]), torch.sin(att[:,1])\n",
    "        ca, sa = torch.cos(att[:,0]), torch.sin(att[:,0])\n",
    "        rho_hat =  torch.stack([cd*ca, cd*sa, sd], dim=1)\n",
    "\n",
    "        # partials\n",
    "        d_rho_hat_dalpha = torch.stack([-cd*sa, cd*ca, torch.zeros_like(sd)], dim=1)\n",
    "        d_rho_hat_ddelta = torch.stack([-sd*ca, -sd*sa, cd], dim=1)\n",
    "\n",
    "        # time-derivative of rho_hat\n",
    "        rho_hat_dot = d_rho_hat_dalpha * att[:,2].unsqueeze(1) + d_rho_hat_ddelta * att[:,3].unsqueeze(1)\n",
    "\n",
    "        # position and velocity\n",
    "        r = att[:,4].unsqueeze(1) * rho_hat\n",
    "        v = att[:,5].unsqueeze(1) * rho_hat + att[:,4].unsqueeze(1) * rho_hat_dot\n",
    "\n",
    "        r.requires_grad_(True).float().to(device)\n",
    "        v.requires_grad_(True).float().to(device)\n",
    "        \n",
    "        return r, v\n",
    "\n",
    "    #------------------------ Loss and related ------------------------------------\n",
    "    def loss_IC(self, x, t, y):\n",
    "        y_pred_IC = self.network_prediction(t, x)\n",
    "        \n",
    "        if self.losstype == 'mse':\n",
    "            loss_IC = torch.mean(torch.pow((y - y_pred_IC),2)).to(device)\n",
    "            \n",
    "        elif self.losstype == 'logcosh':\n",
    "            loss_IC = torch.mean(torch.log(torch.cosh(y - y_pred_IC)))\n",
    "    \n",
    "        return loss_IC\n",
    "        \n",
    "\n",
    "    def loss_residual(self, x, t, y):\n",
    "        y_pred = self.network_prediction(t, x)\n",
    "                \n",
    "        if self.losstype == 'mse':\n",
    "            loss_residual = torch.mean(torch.pow((y - y_pred),2)).to(device)\n",
    "        \n",
    "        elif self.losstype == 'logcosh':\n",
    "            loss_residual = torch.mean(torch.log(torch.cosh(y - y_pred))).to(device)\n",
    "\n",
    "        return loss_residual\n",
    "\n",
    "    \n",
    "    def loss_PDE(self, x, t):\n",
    "        f_pred = self.energy_conservation(t, x)\n",
    "\n",
    "        f_pred = torch.nan_to_num(f_pred, nan=0.0, posinf=1e3, neginf=-1e3) # clamp out any huge values or NaNs \n",
    "        \n",
    "        if self.losstype == 'mse':\n",
    "            loss_PDE = torch.mean(torch.pow(f_pred,2)).to(device)\n",
    "            \n",
    "        elif self.losstype == 'logcosh':\n",
    "            loss_PDE = torch.mean(torch.log(torch.cosh(f_pred))).to(device)\n",
    "                 \n",
    "        return loss_PDE\n",
    "\n",
    "    '''\n",
    "    def forward_temp_weights(self, loss_domain):\n",
    "        loss_domain = torch.tensor(loss_domain, device=self.temporal_weights.device)\n",
    "        \n",
    "        self.temporal_weights = torch.exp(-self.epsilon * torch.cumsum(loss_domain, dim=0))\n",
    "    '''\n",
    "    \n",
    "    def forward_loss_weights(self, losses):       \n",
    "        losses_tensor = torch.stack(losses)\n",
    "\n",
    "        parameters = [p for p in self.new_param if p.requires_grad and p.is_leaf] # pick only the parameters that we actually want to differentiate\n",
    "        \n",
    "        # Create the gradient of each component of the loss respect to the parameters of the model\n",
    "        grad_norms = []\n",
    "        \n",
    "        for l in losses_tensor: \n",
    "            l = l.clone().detach().requires_grad_(True) # make sure this loss is tracked in the current graph\n",
    "            \n",
    "            grads = torch.autograd.grad(l, parameters, retain_graph = True, create_graph=True, allow_unused=True)\n",
    "\n",
    "            flat = []\n",
    "            for g in grads:\n",
    "                if g is not None:\n",
    "                    flat.append(g.view(-1))\n",
    "            if flat:\n",
    "                grad_norms.append(torch.norm(torch.cat(flat)))\n",
    "            else:\n",
    "                grad_norms.append(torch.tensor(0.0, device=l.device))\n",
    "            \n",
    "        grad_norms = torch.stack(grad_norms)\n",
    "\n",
    "        # Update loss weights\n",
    "        lambda_hat = grad_norms.sum() / (grad_norms + 1e-8) # ensure lambda is not infinite\n",
    "        \n",
    "        self.loss_weights = self.alpha*self.loss_weights.clone() + (1 - self.alpha)*lambda_hat\n",
    "        \n",
    "    \n",
    "    def get_training_history(self):\n",
    "        loss_his = np.array(self.train_loss_history)\n",
    "        total_loss, loss_IC, loss_PDE, loss_residual = np.split(loss_his, 4, axis=1) \n",
    "        \n",
    "        return total_loss, loss_IC, loss_PDE, loss_residual\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def batch_generator(self, x, t, y):   \n",
    "        #idx = torch.randperm(len(x))\n",
    "        idx = torch.arange(len(x)) \n",
    "    \n",
    "        # Ensure the batch size is not larger than the available data points\n",
    "        num_batches = (len(x) + self.n_batch - 1) // self.n_batch  # calculate number of batches\n",
    "    \n",
    "        for i in range(num_batches): \n",
    "        \n",
    "            batch_idx = idx[i * self.n_batch : min((i + 1) * self.n_batch, len(x))]  # Get batch indices\n",
    "        \n",
    "            batch_x = x[batch_idx]\n",
    "            batch_t = t[batch_idx]\n",
    "            batch_y = y[batch_idx]\n",
    "            \n",
    "            yield batch_x, batch_t, batch_y\n",
    "            \n",
    "    #--------------------------- Training -------------------------------------------------------------\n",
    "    def train_network(self, epochs, learning_rate, regularization):\n",
    "        optim = 'Adam'\n",
    "        #self.optimizer = torch.optim.AdamW(self.new_param, lr = learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.0001, amsgrad=True)\n",
    "        self.optimizer = torch.optim.Adam(self.new_param, lr = learning_rate, weight_decay = regularization, amsgrad=True) \n",
    "\n",
    "        # Learning-rate warmp-up + exponential decay scheduler\n",
    "        warmup_steps = 600\n",
    "\n",
    "        warmup_lr = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda = lambda step: min(1.0, (step+1) / warmup_steps)) # lr_Lambda for warmup: lr(t) = [(t / warmup_steps) for t ≤ warmup_steps; 1 afterwards]\n",
    "        decay_lr = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size = 2000, gamma = 0.90) # exponential decay every step_size steps by factor gamma\n",
    "        self.scheduler = torch.optim.lr_scheduler.ChainedScheduler([warmup_lr, decay_lr])\n",
    "\n",
    "        # Training checkpoint \n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model_state = None\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            loss_IC, loss_PDE, loss_residual = 0.0, 0.0, 0.0\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Mini-batch loss for Initial Conditions (IC)\n",
    "            for batch_x0, batch_t0, batch_y0 in self.batch_generator(self.x0, self.t0, self.y0):  # (arange activated !!)\n",
    "                loss_IC += self.loss_IC(batch_x0, batch_t0, batch_y0)  \n",
    "            \n",
    "            # Mini-batch loss for Domain Loss (PDE) \n",
    "            #loss_residual_list = []\n",
    "            for batch_x_domain, batch_t_domain, batch_y_domain in self.batch_generator(self.x_domain, self.t_domain, self.y_domain): # (arange activated !!)\n",
    "                loss_PDE += self.loss_PDE(batch_x_domain, batch_t_domain)   \n",
    "                \n",
    "                loss_residual += self.loss_residual(batch_x_domain, batch_t_domain, batch_y_domain)\n",
    "                #loss_residual_list.append(self.loss_residual(batch_x_domain, batch_t_domain, batch_y_domain)) \n",
    "\n",
    "            #loss_residual = loss_residual*self.temporal_weights.clone()\n",
    "            #self.forward_temp_weights(loss_residual_list)\n",
    "            \n",
    "            # Give a weights to every singular loss term\n",
    "            weighted_losses = self.loss_weights.clone()*torch.stack([loss_IC, loss_PDE, loss_residual])\n",
    "            \n",
    "            # Total loss for this epoch after having given the weights\n",
    "            total_loss = weighted_losses.sum()\n",
    "\n",
    "            '''\n",
    "            torch.autograd.set_detect_anomaly(True) # DEBUG\n",
    "            '''\n",
    "            \n",
    "            total_loss.backward(retain_graph = True) # always calculate the derivatives first  \n",
    "            \n",
    "            if epoch % self.f == 0:\n",
    "                # Calculate gradients and retain graph in order to derive also all of the singular losses terms for the correspondant global weigth\n",
    "                self.forward_loss_weights([loss_IC, loss_PDE, loss_residual]) # global weights update routine using the non-weigthed losses\n",
    "\n",
    "            # Memorize the state of best training (checkpoint!)\n",
    "            if total_loss.item() < self.best_loss:\n",
    "                self.best_loss = total_loss.item()\n",
    "                self.best_model_state = {'model': self.state_dict(),'epoch': epoch}\n",
    "                \n",
    "            self.train_loss_history.append([total_loss.cpu().detach(), weighted_losses[0].cpu().detach(), weighted_losses[1].cpu().detach(), weighted_losses[2].cpu().detach()]) \n",
    "                \n",
    "            torch.nn.utils.clip_grad_norm_(self.new_param, max_norm=1.0) # clipping\n",
    "            \n",
    "            # Optimize the network parameters            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            self.scheduler.step()\n",
    "                \n",
    "            # Print out the loss every 100 epochs\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch ({optim}): {epoch}, Total Loss: {total_loss.detach().cpu().numpy()}')\n",
    "        \n",
    "        # Reload the checkpoint\n",
    "        if self.best_model_state is not None:\n",
    "            print(f\"Restoring best model from epoch {self.best_model_state['epoch']} with loss {self.best_loss:.4e}\")\n",
    "            self.load_state_dict(self.best_model_state['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "282d8b38-32d4-4e8c-a05a-105ad6352b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data importing and handling\n",
    "t_allN, x_allN, y_true_allN = load_observations('2015RN35')\n",
    "\n",
    "# Shuffle !\n",
    "idx = torch.randperm(len(t_allN))\n",
    "\n",
    "x_allN = x_allN[idx]\n",
    "t_allN = t_allN[idx]\n",
    "y_true_allN = y_true_allN[idx]\n",
    "\n",
    "# Dividing between testing and validation data sets\n",
    "t_valid, x_valid, y_true_valid = t_allN[-5:-1], x_allN[-5:-1,:], y_true_allN[-5:-1,:]\n",
    "t, x, y_true = t_allN[1:-5], x_allN[1:-5,:], y_true_allN[1:-5,:]\n",
    "\n",
    "# Import initial conditions from fitob\n",
    "Input_0_equin = np.array([1.4597513976025835, 0.285859684022060, 0.187526292392717, -0.001113623441851, -0.001969823345975, 34.9412629996062])  # from .eq0 orbfit file\n",
    "Input_0 = torch.tensor([equinoctial_to_obs(Input_0_equin)]).float().to(device)\n",
    "\n",
    "x_0 = Input_0[:,0:4] # (alpha_0, delta_0, alpha_0_dot, delta_0_dot)\n",
    "y_0_true = Input_0[:,4:6] # (rho_0, rho_0_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "adbea553-f250-47cc-911b-6195c69ba908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A-dimensionalization\n",
    "x0, t0, y0_true = non_dimensionalise(x_0, t_0, y_0_true)\n",
    "x, t, y_true = non_dimensionalise(x, t, y_true)\n",
    "x_valid, t_valid, y_true_valid = non_dimensionalise(x_valid, t_valid, y_true_valid)\n",
    "\n",
    "# Normalization (optional)\n",
    "t = normalize(t, None)\n",
    "x = normalize(x, None)\n",
    "y_true = normalize(y_true, None)\n",
    "\n",
    "t_valid, t_valid_mins, t_valid_maxs = normalize(t_valid, True)\n",
    "x_valid, x_valid_mins, x_valid_maxs = normalize(x_valid, True)\n",
    "y_true_valid, y_true_valid_mins, y_true_valid_maxs = normalize(y_true_valid, True)\n",
    "\n",
    "t0 = normalize(t0, None)\n",
    "x0 = normalize(x0, None)\n",
    "y0_true = normalize(y0_true, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1424f514",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (Adam): 0, Total Loss: 6064725771043849.0\n",
      "Epoch (Adam): 100, Total Loss: 4912427527519126.0\n",
      "Epoch (Adam): 200, Total Loss: 3979066065939564.0\n",
      "Epoch (Adam): 300, Total Loss: 3223043404965299.0\n",
      "Epoch (Adam): 400, Total Loss: 2610665089339585.5\n",
      "Epoch (Adam): 500, Total Loss: 2114638577770734.2\n",
      "Epoch (Adam): 600, Total Loss: 1712857023873082.8\n",
      "Epoch (Adam): 700, Total Loss: 74379.09347159971\n",
      "Epoch (Adam): 800, Total Loss: 42268.018516217075\n",
      "Epoch (Adam): 900, Total Loss: 21939.938393673496\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m lambda_reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m PINN_VSA(layers, losstype, n_batch, mu, t0, x0, y0_true, t, x, y_true)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_reg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[117], line 324\u001b[0m, in \u001b[0;36mPINN_VSA.train_network\u001b[0;34m(self, epochs, learning_rate, regularization)\u001b[0m\n\u001b[1;32m    318\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m weighted_losses\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    320\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03mtorch.autograd.set_detect_anomaly(True) # DEBUG\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m--> 324\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# always calculate the derivatives first  \u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# Calculate gradients and retain graph in order to derive also all of the singular losses terms for the correspondant global weigth\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_loss_weights([loss_IC, loss_PDE, loss_residual]) \u001b[38;5;66;03m# global weights update routine using the non-weigthed losses\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/PINNs_lab/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/PINNs_lab/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/PINNs_lab/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layers = [5, 2048, 2048, 2] # input features [(x, t) = 5, con Fourier diventano 2*fourier_m] → hidden layers → outputs (y)\n",
    "losstype = 'mse'\n",
    "n_batch = 2\n",
    "epochs = 10000\n",
    "L_rate = 0.0001\n",
    "lambda_reg = 0\n",
    "\n",
    "model = PINN_VSA(layers, losstype, n_batch, mu, t0, x0, y0_true, t, x, y_true).to(device)\n",
    "\n",
    "model.train_network(epochs, L_rate, lambda_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35676103",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "total_loss, loss_IC, loss_PDE, loss_residual = model.get_training_history() \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(total_loss, label='Total Loss')\n",
    "plt.plot(loss_residual, label='Residual Loss')\n",
    "plt.plot(loss_IC, label='Initial Condition Loss')\n",
    "plt.plot(loss_PDE, label='PDE Domain Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed4eb6-4b53-4806-a4c8-a55d2aebc29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model predictions after training\n",
    "y_pred = model.network_prediction(t_valid, x_valid).float().detach().to(device).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc2574-e9c7-4f80-8fd3-466cc3960f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_valid = denormalize(t_valid, t_valid_mins, t_valid_maxs)\n",
    "x_valid = denormalize(x_valid, x_valid_mins, x_valid_maxs)\n",
    "y_true_valid = denormalize(y_true_valid, y_true_valid_mins, y_true_valid_maxs)\n",
    "y_pred = denormalize(torch.tensor(y_pred), y_true_valid_mins, y_true_valid_maxs)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "axes[0].scatter(t_valid.float().detach().to(device).numpy(), y_true_valid[:,0].float().detach().to(device).numpy(), label = 'True')\n",
    "axes[0].scatter(t_valid.float().detach().to(device).numpy(), y_pred[:,0], label = 'Estimated')\n",
    "axes[0].set_xlabel('t')\n",
    "axes[0].set_ylabel(r'$\\rho$')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].scatter(t_valid.float().detach().to(device).numpy(), y_true_valid[:,1].float().detach().to(device).numpy(), label = 'True')\n",
    "axes[1].scatter(t_valid.float().detach().to(device).numpy(), y_pred[:,1], label = 'Estimated')\n",
    "axes[1].set_xlabel('t')\n",
    "axes[1].set_ylabel(r'$\\dot{\\rho}$')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": -2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
