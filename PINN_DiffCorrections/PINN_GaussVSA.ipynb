{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "92267208-b9b1-4f7e-ab34-f65a8b5ba557",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7bea2ffe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import glob\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "336603ab",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "65012647",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Working on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "09937716-7564-4ebe-af28-533974ce7ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manegement functions\n",
    "\n",
    "def fit_second_derivatives(attrib, t_mean): # given (attrib, t_mean) this function makes a quadratic polynomian fit to estimates (alpha_ddot, delta_ddot)\n",
    "    #t_flat = t_mean.ravel()\n",
    "    attrib = np.asarray(attrib)\n",
    "    t_mean = np.asarray(t_mean)\n",
    "    N = len(t_mean)\n",
    "\n",
    "    # Unpack the variables\n",
    "    alpha     = attrib[:, 0] \n",
    "    alpha_dot = attrib[:, 1]  \n",
    "    delta     = attrib[:, 2] \n",
    "    delta_dot = attrib[:, 3]  \n",
    "\n",
    "    # Built the design matrix\n",
    "    A_values = np.stack([np.ones(N), t_mean, np.power(t_mean,2)], axis=1)      \n",
    "    A_derivs = np.stack([np.zeros(N), np.ones(N), 2*t_mean], axis=1)  \n",
    "    A = np.vstack([A_values, A_derivs])  \n",
    "\n",
    "    alpha_targets = np.concatenate([alpha, alpha_dot]) \n",
    "    delta_targets = np.concatenate([delta, delta_dot]) \n",
    "\n",
    "    # fit for the quadratic coefficients: f(x) = k0 + k1 * t + k2 * t^2\n",
    "    coef_alpha, *_ = np.linalg.lstsq(A, alpha_targets, rcond=None)\n",
    "    coef_delta, *_ = np.linalg.lstsq(A, delta_targets, rcond=None)\n",
    "\n",
    "    alpha_ddot = 2.0 * coef_alpha[2]\n",
    "    delta_ddot = 2.0 * coef_delta[2]\n",
    "\n",
    "    return float(alpha_ddot), float(delta_ddot), N\n",
    "\n",
    "\n",
    "'''  \n",
    "                                                            !!!\n",
    "    \n",
    "    The data can grow in number CONSIDERABLY if one considers all the observations of one night and creates triplets by randomly selecting them in this way:\n",
    "    following the chronological time of observation, after 3 of them have been achieved, one can combine randomly the observations happened\n",
    "    after the 3rd one with the first ones (NOT EXCLUSIVE), thus creating triplets of observations relatively close in time.\n",
    "    Repeating this procedure for all the nights is possible to end up with multiples entries from each night.\n",
    "    \n",
    "                                                            !!!\n",
    "'''  \n",
    "\n",
    "columns_input  = ['alpha','delta','alpha_dot','delta_dot']\n",
    "columns_output = ['rho','rho_dot']\n",
    "\n",
    "def load_observations(prefix, input_names = columns_input, output_names = columns_output):\n",
    "    \n",
    "    ar_files = sorted(glob.glob(f\"{'datas_orbfit'}/{prefix}_N*.ar_sample\"))\n",
    "    \n",
    "    t_list, X_list, Y_list = [], [], []\n",
    "\n",
    "    for ar in ar_files:\n",
    "        # read attributable line\n",
    "        df_in = pd.read_csv(ar, sep='=', header=None, skiprows=1, nrows=1, names=['tag','values'], engine='python')\n",
    "        attrib = [float(x) for x in df_in.loc[0,'values'].split()]\n",
    "        s_attrib = pd.Series(attrib, index=columns_input)\n",
    "\n",
    "        # read first data row (rho, rho_dot)\n",
    "        df_out = pd.read_csv(ar, comment='%', sep='\\s+', skiprows=8, nrows = 1, names=['i','j','rho','rho_dot','cc','succ','chi','Jac','E_Sun','E_Earth','H'], engine='python')\n",
    "        s_out = df_out.loc[0, columns_output]\n",
    "\n",
    "        # read epoch from mov_sample\n",
    "        mov = ar.replace('.ar_sample', '.mov_sample')\n",
    "        df_t = pd.read_csv(mov, sep='=', header=None, skiprows=1, nrows=1, names=['tag','epoch_str'], engine='python')\n",
    "        epoch = float(df_t.loc[0,'epoch_str'].split()[0])\n",
    "        \n",
    "        #alpha_ddot, delta_ddot = fit_second_derivatives(X_list, t_list)\n",
    "        \n",
    "        t_list.append(epoch)\n",
    "        X_list.append(s_attrib.values)\n",
    "        Y_list.append(s_out.values)\n",
    "    \n",
    "    # Add second derivatives from polynomial fit\n",
    "    alpha_ddot, delta_ddot, N = fit_second_derivatives(X_list, t_list)\n",
    "\n",
    "    alpha_ddot_col = np.full((N, 1), alpha_ddot)   \n",
    "    delta_ddot_col = np.full((N, 1), delta_ddot)   \n",
    "\n",
    "    X_list = np.hstack([X_list, alpha_ddot_col, delta_ddot_col])\n",
    "\n",
    "    return torch.tensor(t_list), torch.tensor(X_list), torch.tensor(Y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "678e797c-26df-4101-80a5-6b62543c55d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gravitational constant for the Sun \n",
    "#mu = np.power(0.01720209895, 2) #([AU]^3/[day]^2)\n",
    "G = 6.6743e-11  # [km]^3/([kg]*[s]^2)  \n",
    "M_sun = 1988400e24 # [kg]\n",
    "mu = G*M_sun # [km]^3/[s]^2\n",
    "k = 398600.4418  # [km]^3/[s]^2 <--- G*M_sun/4π²\n",
    "M_earth = 5.9722e24 # kg\n",
    "\n",
    "'''\n",
    "                ??\n",
    "            \n",
    "Investigate other normalization quantities?\n",
    "            \n",
    "                ??\n",
    "'''\n",
    "\n",
    "#Characteristics quantities of the system \n",
    "AU = 149597870.7 # km in Au\n",
    "day = 60*60*24 # s in day\n",
    "\n",
    "L_c = AU #3.844e5 # [km] mean orbital radius of the moon\n",
    "T_c = np.sqrt(np.power(L_c, 3) / k) # [s] orbital period of the moon (2.361e6 [s]) \n",
    "V_c = L_c / T_c # [km/s] characteristic velocity\n",
    "\n",
    "t_0 = torch.tensor([57291.95817]).float().to(device) # t0 for 2015RN35\n",
    "\n",
    "# Solve Kepler's equation for elliptic orbits using Newton-Raphson iterations\n",
    "def solve_kepler(M, e, tol=1e-10, maxiter=10):\n",
    "    E = M.copy()\n",
    "    for _ in range(maxiter):\n",
    "        f  = E - e * np.sin(E) - M\n",
    "        fp = 1 - e * np.cos(E)\n",
    "        dE = -f / fp\n",
    "        E += dE\n",
    "        if np.all(np.abs(dE) < tol):\n",
    "            break\n",
    "    return E\n",
    "\n",
    "def equinoctial_to_cartesian(equin_coord):  #(a, h, k, p, q, L) --> (r; v)\n",
    "    \n",
    "    e = np.sqrt(equin_coord[1]**2 + equin_coord[2]**2)\n",
    "    omega_plus_Omega = np.arctan2(equin_coord[1], equin_coord[2])\n",
    "\n",
    "    i = 2 * np.arctan(np.sqrt(equin_coord[3]**2 + equin_coord[4]**2))\n",
    "    Omega = np.arctan2(equin_coord[3], equin_coord[4])\n",
    "\n",
    "    M = equin_coord[5] - omega_plus_Omega\n",
    "    M = np.mod(M, 2*np.pi)\n",
    "    E = solve_kepler(M,e)\n",
    "   \n",
    "    nu = 2 * np.arctan2(np.sqrt(1+e)*np.sin(E/2), np.sqrt(1-e)*np.cos(E/2))\n",
    "\n",
    "    # Perifocal position and velocity\n",
    "    r_pqw = equin_coord[0] * (1 - e*np.cos(E))\n",
    "    r_vec_pqw = r_pqw * np.array([np.cos(nu), np.sin(nu), 0])\n",
    "    v_vec_pqw = np.sqrt(mu * equin_coord[0]) / r_pqw * np.array([-np.sin(E), np.sqrt(1 - e**2)*np.cos(E), 0])\n",
    "\n",
    "    # Rotation matrices\n",
    "    cos_Om = np.cos(Omega)\n",
    "    sin_Om = np.sin(Omega)\n",
    "    cos_w = np.cos(omega_plus_Omega - Omega)\n",
    "    sin_w = np.sin(omega_plus_Omega - Omega)\n",
    "    cos_i = np.cos(i)\n",
    "    sin_i = np.sin(i)\n",
    "\n",
    "    # Perifocal to ECI transformation matrix\n",
    "    R = np.array([\n",
    "        [cos_Om*cos_w - sin_Om*sin_w*cos_i, -cos_Om*sin_w - sin_Om*cos_w*cos_i, sin_Om*sin_i],\n",
    "        [sin_Om*cos_w + cos_Om*sin_w*cos_i, -sin_Om*sin_w + cos_Om*cos_w*cos_i, -cos_Om*sin_i],\n",
    "        [sin_w*sin_i,                        cos_w*sin_i,                       cos_i]\n",
    "    ])\n",
    "\n",
    "    # Convert to ECI\n",
    "    r = R @ r_vec_pqw\n",
    "    v = R @ v_vec_pqw\n",
    "\n",
    "    return r, v\n",
    "    \n",
    "def equinoctial_to_obs(equin_coord, r_obs=np.zeros(3), v_obs=np.zeros(3)): # here r_obs and v_obs are the coordinates of the observatory\n",
    "\n",
    "    # Equinoctial → Cartesian\n",
    "    r_ast, v_ast = equinoctial_to_cartesian(equin_coord)\n",
    "    \n",
    "    # Topocentric vectors\n",
    "    rho_vec = r_ast - r_obs\n",
    "    v_rel   = v_ast - v_obs\n",
    "\n",
    "    rho     = np.linalg.norm(rho_vec)\n",
    "    hat_rho = rho_vec / rho\n",
    "    rho_dot = hat_rho.dot(v_rel)\n",
    "\n",
    "    # Compute hat_rho_dot\n",
    "    rho_hat_dot = (v_rel - rho_dot*hat_rho) / rho\n",
    "\n",
    "    # Angles\n",
    "    alpha = np.arctan2(hat_rho[1], hat_rho[0])\n",
    "    delta = np.arcsin(hat_rho[2])\n",
    "\n",
    "    # Angular rates\n",
    "    denom = np.power(hat_rho[0], 2) + np.power(hat_rho[1], 2)\n",
    "    alpha_dot = (hat_rho[0]*rho_hat_dot[1] - hat_rho[1]*rho_hat_dot[0]) / denom\n",
    "    delta_dot = rho_hat_dot[2] / np.cos(delta)\n",
    "\n",
    "    return alpha, delta, alpha_dot, delta_dot, rho, rho_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fba67f89-afb4-4f0a-8eda-dabddc5410c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for data normalization\n",
    "def normalize(inputs, export_minmax): # from standard to [-1, 1]\n",
    "    inputs_min = inputs.min(dim=1, keepdim=True).values\n",
    "    inputs_max = inputs.max(dim=1, keepdim=True).values\n",
    "    \n",
    "    if export_minmax == True:\n",
    "        return 2*(inputs - inputs_min) / (inputs_max - inputs_min + 1e-8) - 1, inputs_min, inputs_max\n",
    "    else:\n",
    "        return 2*(inputs - inputs_min) / (inputs_max - inputs_min + 1e-8) - 1\n",
    "\n",
    "def denormalize(inputs, inputs_min, inputs_max): # from [-1, 1] to standard\n",
    "    \n",
    "    return ((inputs + 1)/2)*(inputs_max - inputs_min + 1e-8) + inputs_min\n",
    "\n",
    "# Funtion for data a-dimensionalization\n",
    "def non_dimensionalise(x, t, y, t0 = t_0): # x = (alpha, delta, alpha_dot, delta_dot), t = t_obs, y = (rho, rho_dot)\n",
    "    x_nodim = torch.empty_like(x)\n",
    "    x_nodim[:,0] = x[:,0] # dimensionless      \n",
    "    x_nodim[:,1] = x[:,1] # dimensionless               \n",
    "    x_nodim[:,2] = (x[:,2] / day) * T_c # ([rad/AU] → [rad/s]) → dimensionless                   \n",
    "    x_nodim[:,3] = (x[:,3] / day) * T_c # ([rad/AU] → [rad/s]) → dimensionless                              \n",
    "    \n",
    "    t_mjd = (t - t0) # [s] \n",
    "    \n",
    "    t_nodim = t_mjd / T_c # [s] → dimensionless \n",
    "\n",
    "    t_nodim = t_nodim.view(-1, 1)\n",
    "    \n",
    "    y_nodim = torch.empty_like(y)\n",
    "    \n",
    "    y_nodim[:,0] = (y[:,0] * AU) / L_c # ([AU] → [km]) → dimensionless              \n",
    "    y_nodim[:,1] = (y[:,1] * (AU/day)) / V_c  # ([AU/day] → [km/s]) → dimensionless  \n",
    "\n",
    "    x_nodim.requires_grad_(True).float().to(device)\n",
    "    t_nodim.requires_grad_(True).float().to(device)\n",
    "    y_nodim.float().to(device)\n",
    "    \n",
    "    return x_nodim, t_nodim, y_nodim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "29fb837d-dc3a-4c5a-bd6a-e946c8c891af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to implement a fourier spectral expansion of the inputs (fourier_M = # of frequencies in the spectrum)\n",
    "class FourierEmbedding(nn.Module):\n",
    "    def __init__(self, in_dim, mapping_size, scale):\n",
    "        super().__init__()\n",
    "        self.register_buffer('B', torch.randn(in_dim, mapping_size) * scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = self.B.to(x.device).to(x.dtype)\n",
    "        x_proj = 2 * np.pi * x @ B\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "68404050",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Main model\n",
    "class PINN_VSA(nn.Module):\n",
    "    \n",
    "    def __init__(self, layers, loss_type, n_batch, mu, r_earth, *data, fourier_m = 64, fourier_scale = 10.0): \n",
    "        super(PINN_VSA, self).__init__() \n",
    "        \n",
    "        self.fourier = FourierEmbedding(in_dim=7, mapping_size=fourier_m, scale=fourier_scale)\n",
    "\n",
    "        L0 = 2 * fourier_m # size on input layer is bigger using fourier embedding\n",
    "        layers = [L0] + layers[1:]\n",
    "        \n",
    "        (self.t0, self.x0, self.y0, self.t_domain, self.x_domain, self.y_domain) = data\n",
    "        self.n_batch = n_batch\n",
    "        self.losstype = loss_type\n",
    "\n",
    "        self.mu = mu\n",
    "        self.r_earth = r_earth\n",
    "        \n",
    "        #---------------------------------------- Define layers ------------------------------------------\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activation = nn.Tanh() # Activation function\n",
    "        \n",
    "        self.s_list = {}\n",
    "        self.v_list = {}\n",
    "        biases = []\n",
    "        mean = 1.0\n",
    "        std = 0.1\n",
    "        \n",
    "        for i in range(len(layers) - 1): \n",
    "            \n",
    "            self.layers.append(nn.Linear(layers[i], layers[i+1], bias = True)) # Fully connected linear layer: the operation [out = w*input + b] will be made by hand with factorized weights\n",
    "            # Random weight factorization memorization\n",
    "            \n",
    "            W = self.layers[-1].weight.detach().clone()  # weights are bot trained but used only now to calculate their factorization\n",
    "            \n",
    "            s = torch.exp(torch.randn(layers[i+1])*std + mean)\n",
    "            v = W / s.unsqueeze(1)\n",
    "\n",
    "            self.s_list[f\"s_{i}\"] = nn.Parameter(s, requires_grad = True)\n",
    "            self.register_parameter(f\"s_{i}\", self.s_list[f\"s_{i}\"])  # Register the parameter\n",
    "            \n",
    "            self.v_list[f\"v_{i}\"] = nn.Parameter(v, requires_grad = True)\n",
    "            self.register_parameter(f\"v_{i}\", self.v_list[f\"v_{i}\"]) # Register the parameter\n",
    "            \n",
    "            biases.append(self.layers[-1].bias.requires_grad_(True))\n",
    "\n",
    "            self.layers[i].weight.requires_grad_(False) # we don't need to train these\n",
    "            self.layers[i].bias.requires_grad_(False) # we have copied the trainable ones in the list \"biases\" \n",
    "            \n",
    "        self.new_param = list(self.s_list.values()) + list(self.v_list.values()) + biases # new parameters to be trained\n",
    "        \n",
    "        #----------------------------------------------------------------------------------------------------------------------------\n",
    "        self.optimizer = None\n",
    "        self.train_loss_history = []\n",
    "        \n",
    "        #------------------------ For weighted loss procedures and temporal weights -------------------------------------------------\n",
    "        self.n_losses = 3\n",
    "        self.loss_weights = torch.ones(self.n_losses, device = device).requires_grad_(False) # Initialize losses weights\n",
    "        self.f = 50\n",
    "        self.alpha = 0.9\n",
    "        self.epsilon = 1.0\n",
    "\n",
    "    \n",
    "    #------------------ Loop operations ---------------------------------------- \n",
    "    def get_factorized_weight(self, i):        \n",
    "        b = self.layers[i].bias\n",
    "\n",
    "        s = self.s_list[f\"s_{i}\"]\n",
    "        v = self.v_list[f\"v_{i}\"]\n",
    "        \n",
    "        return (s.view(-1,1) * v), b\n",
    "    \n",
    "        \n",
    "    def forward(self, X): # Forward pass using random decomposed weights\n",
    "\n",
    "        X_embed = self.fourier(X)\n",
    "        a = X_embed.float()\n",
    "        \n",
    "        for i in range(len(self.layers) - 1):\n",
    "            input_copy = a # for skip connections\n",
    "            \n",
    "            kernel, b = self.get_factorized_weight(i)\n",
    "            a = a @ kernel.T + b \n",
    "            \n",
    "            if i < (len(self.layers) - 1):\n",
    "                a = self.activation(a)\n",
    "\n",
    "                #Skip connections are activated only after the input layer\n",
    "                if i >= 1: \n",
    "                    if a.shape != input_copy.shape: #In case of layers of different size\n",
    "                        # Apply a 1x1 linear transformation to match dimensions, but after activation\n",
    "                        projection = nn.Linear(input_copy.shape[1], a.shape[1], bias=False).to(a.device)\n",
    "                        input_copy = projection(input_copy) \n",
    "                        projection.weight.requires_grad_(False)\n",
    "                        \n",
    "                    a = a + input_copy\n",
    "                     \n",
    "        kernel, b = self.get_factorized_weight(len(self.layers)-1)\n",
    "        a = a @ kernel.T + b \n",
    "                    \n",
    "        return a \n",
    "\n",
    "    \n",
    "    def network_prediction(self, t, x):\n",
    "\n",
    "        return self.forward(torch.cat([t, x], 1))\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "\n",
    "    #--------------------------- PDEs ---------------------------------------------\n",
    "    \n",
    "    '''\n",
    "                              ??\n",
    "                              \n",
    "    Integral or first derivative of energy (and curvature)? Or hybrid?\n",
    "    \n",
    "                              ??\n",
    "    '''\n",
    "    \n",
    "    def get_derivative(self, y, x, n): # General formula to compute the n-th order derivative of y = f(x) with respect to x\n",
    "        if n == 0:  # (n is the order if the derivative)\n",
    "            return y\n",
    "        else:\n",
    "            dy_dx = torch.autograd.grad(y, x, torch.ones_like(y).to(device), retain_graph=True, allow_unused=True, create_graph=True)[0]\n",
    "        \n",
    "        return self.get_derivative(dy_dx, x, n - 1)\n",
    "\n",
    "    '''\n",
    "    def energy_conservation(self, t, x): # Compute the specific orbital energy derivative (in time) of the asteroid\n",
    "        out = self.network_prediction(t, x) # (rho, rho_dot)\n",
    "        \n",
    "        v_a = torch.cat([x, out], dim=1)\n",
    "        r, v = self.attributable_to_cartesian(v_a)\n",
    "        \n",
    "        dv_dt = self.get_derivative(v, t, 1)\n",
    "        \n",
    "        if dv_dt == None:\n",
    "            dv_dt = torch.zeros_like(v)\n",
    "            \n",
    "        dv_dt.float().to(device)\n",
    "            \n",
    "        f =  - (v * dv_dt).sum(dim=1) + self.mu * ( (r * v).sum(dim=1) / (torch.pow(r.norm(dim=1), 3) + 1e-8) ) #  f = - v·dv_dt + mu(r·v/(|r|^(3))) \n",
    "        \n",
    "        return f\n",
    "    '''  \n",
    "    \n",
    "    def curvature(self, t, x): # Danby, eq. 7.1.2\n",
    "        out = self.network_prediction(t, x) #(rho, rho_dot)\n",
    "        print(x, out)\n",
    "        v_a = torch.hstack([x[0], x[1], x[2], x[3], out[0], out[1]]).detach().numpy()\n",
    "        \n",
    "        r, v = self.attributable_to_cartesian(v_a)\n",
    "\n",
    "        r_ast = r - self.r_earth\n",
    "        \n",
    "        rho_versor = r / v_a[4]\n",
    "        rho_versor_dot = (v - v_a[5]*rho_versor) / v_a[4]\n",
    "        n_versor = torch.cross(rho_versor, rho_versor_dot)\n",
    "        \n",
    "        d_asts = torch.norm(r_ast).requires_grad_(True)\n",
    "        d_sun = torch.norm(self.r_earth).requires_grad_(True)\n",
    "        eta = torch.norm(v).requires_grad_(True)\n",
    "        K_urvature = ((1/torch.pow(eta,3))*((x[5]*x[2] - x[4]*x[3])*torch.cos(x[1]) + x[2]*(torch.pow(eta,2) + torch.pow(x[3],2))*torch.sin(x[1]))).requires_grad_(True)\n",
    "\n",
    "        f = - self.mu*(1/torch.pow(d_ast,3) - 1/torch.pow(d_sun,3))*(torch.dot(self.r_earth, n_versor)) + v_[4]*torch.pow(eta,2)*K_urvature\n",
    "        \n",
    "    #-------------------------------------------------------------------------------\n",
    "\n",
    "    def attributable_to_cartesian(self, v_a): # (alpha, delta, alpha_dot, delta_dot, rho, rho_dot) --> (r, v)\n",
    "        # unit line-of-sight\n",
    "        cd, sd = torch.cos(v_a[:,1]), torch.sin(v_a[:,1])\n",
    "        ca, sa = torch.cos(v_a[:,0]), torch.sin(v_a[:,0])\n",
    "        rho_hat =  torch.stack([cd*ca, cd*sa, sd], dim=1)\n",
    "\n",
    "        # partials\n",
    "        d_rho_hat_dalpha = torch.stack([-cd*sa, cd*ca, torch.zeros_like(sd)], dim=1)\n",
    "        d_rho_hat_ddelta = torch.stack([-sd*ca, -sd*sa, cd], dim=1)\n",
    "\n",
    "        # time-derivative of rho_hat\n",
    "        rho_hat_dot = d_rho_hat_dalpha * v_a[:,2].unsqueeze(1) + d_rho_hat_ddelta * v_a[:,3].unsqueeze(1)\n",
    "\n",
    "        # position and velocity\n",
    "        r = v_a[:,4].unsqueeze(1) * rho_hat\n",
    "        v = v_a[:,5].unsqueeze(1) * rho_hat + v_a[:,4].unsqueeze(1) * rho_hat_dot\n",
    "\n",
    "        r.requires_grad_(True).float().to(device)\n",
    "        v.requires_grad_(True).float().to(device)\n",
    "        \n",
    "        return r, v\n",
    "\n",
    "    \n",
    "    #------------------------ Loss and related ------------------------------------\n",
    "    '''\n",
    "    def loss_far_orbit(self, x, t, y): # Excluding interstellar orbits\n",
    "        out = self.network_prediction(t, x) # (rho, rho_dot)\n",
    "\n",
    "        v_a = torch.cat([x, out], dim=1)\n",
    "        r, v = self.attributable_to_cartesian(v_a)\n",
    "\n",
    "        if dv_dt == None:\n",
    "            dv_dt = torch.zeros_like(v)\n",
    "            \n",
    "        dv_dt.float().to(device)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def loss_no_satellite(self, x, t, y): # Positive geocentric energy\n",
    "    '''\n",
    "    \n",
    "    def loss_IC(self, x, t, y):\n",
    "        y_pred_IC = self.network_prediction(t, x)\n",
    "        \n",
    "        if self.losstype == 'mse':\n",
    "            loss_IC = torch.mean(torch.pow((y - y_pred_IC),2)).to(device)\n",
    "            \n",
    "        elif self.losstype == 'logcosh':\n",
    "            loss_IC = torch.mean(torch.log(torch.cosh(y - y_pred_IC)))\n",
    "    \n",
    "        return loss_IC\n",
    "        \n",
    "\n",
    "    def loss_residual(self, x, t, y):\n",
    "        y_pred = self.network_prediction(t, x)\n",
    "                \n",
    "        if self.losstype == 'mse':\n",
    "            loss_residual = torch.mean(torch.pow((y - y_pred),2)).to(device)\n",
    "        \n",
    "        elif self.losstype == 'logcosh':\n",
    "            loss_residual = torch.mean(torch.log(torch.cosh(y - y_pred))).to(device)\n",
    "\n",
    "        return loss_residual\n",
    "\n",
    "    \n",
    "    def loss_PDE(self, x, t):\n",
    "        f_pred = self.curvature(t, x)\n",
    "\n",
    "        f_pred = torch.nan_to_num(f_pred, nan=0.0, posinf=1e6, neginf=-1e6) # clamp out any huge values or NaNs \n",
    "        \n",
    "        if self.losstype == 'mse':\n",
    "            loss_PDE = torch.mean(torch.pow(f_pred,2)).to(device)\n",
    "            \n",
    "        elif self.losstype == 'logcosh':\n",
    "            loss_PDE = torch.mean(torch.log(torch.cosh(f_pred))).to(device)\n",
    "                 \n",
    "        return loss_PDE\n",
    "\n",
    "    '''\n",
    "    def forward_temp_weights(self, loss_domain):\n",
    "        loss_domain = torch.tensor(loss_domain, device=self.temporal_weights.device)\n",
    "        \n",
    "        self.temporal_weights = torch.exp(-self.epsilon * torch.cumsum(loss_domain, dim=0))\n",
    "    '''\n",
    "    \n",
    "    def forward_loss_weights(self, losses):       \n",
    "        losses_tensor = torch.stack(losses)\n",
    "\n",
    "        parameters = [p for p in self.new_param if p.requires_grad and p.is_leaf] # pick only the parameters that we actually want to differentiate\n",
    "        \n",
    "        # Create the gradient of each component of the loss respect to the parameters of the model\n",
    "        grad_norms = []\n",
    "        \n",
    "        for l in losses_tensor: \n",
    "            l = l.clone().detach().requires_grad_(True) # make sure this loss is tracked in the current graph\n",
    "            \n",
    "            grads = torch.autograd.grad(l, parameters, retain_graph = True, create_graph=True, allow_unused=True)\n",
    "\n",
    "            flat = []\n",
    "            for g in grads:\n",
    "                if g is not None:\n",
    "                    flat.append(g.view(-1))\n",
    "            if flat:\n",
    "                grad_norms.append(torch.norm(torch.cat(flat)))\n",
    "            else:\n",
    "                grad_norms.append(torch.tensor(0.0, device=l.device))\n",
    "            \n",
    "        grad_norms = torch.stack(grad_norms)\n",
    "\n",
    "        # Update loss weights\n",
    "        lambda_hat = grad_norms.sum() / (grad_norms + 1e-8) # ensure lambda is not infinite\n",
    "        \n",
    "        self.loss_weights = self.alpha*self.loss_weights.clone() + (1 - self.alpha)*lambda_hat\n",
    "        \n",
    "    \n",
    "    def get_training_history(self):\n",
    "        loss_his = np.array(self.train_loss_history)\n",
    "        total_loss, loss_IC, loss_PDE, loss_residual = np.split(loss_his, 4, axis=1) \n",
    "        \n",
    "        return total_loss, loss_IC, loss_PDE, loss_residual\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def batch_generator(self, x, t, y):   \n",
    "        idx = torch.randperm(len(x))\n",
    "        #idx = torch.arange(len(x)) \n",
    "    \n",
    "        # Ensure the batch size is not larger than the available data points\n",
    "        num_batches = (len(x) + self.n_batch - 1) // self.n_batch  # calculate number of batches\n",
    "    \n",
    "        for i in range(num_batches): \n",
    "        \n",
    "            batch_idx = idx[i * self.n_batch : min((i + 1) * self.n_batch, len(x))]  # Get batch indices\n",
    "        \n",
    "            batch_x = x[batch_idx]\n",
    "            batch_t = t[batch_idx]\n",
    "            batch_y = y[batch_idx]\n",
    "            \n",
    "            yield batch_x, batch_t, batch_y\n",
    "\n",
    "    \n",
    "    #--------------------------- Training -------------------------------------------------------------\n",
    "    def train_network(self, epochs, learning_rate, regularization):\n",
    "        optim = 'Adam'\n",
    "        #self.optimizer = torch.optim.AdamW(self.new_param, lr = learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.0001, amsgrad=True)\n",
    "        self.optimizer = torch.optim.Adam(self.new_param, lr = learning_rate, weight_decay = regularization, amsgrad=True) \n",
    "\n",
    "        # Learning-rate warmp-up + exponential decay scheduler\n",
    "        warmup_steps = 600\n",
    "\n",
    "        warmup_lr = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda = lambda step: min(1.0, (step+1) / warmup_steps)) # lr_Lambda for warmup: lr(t) = [(t / warmup_steps) for t ≤ warmup_steps; 1 afterwards]\n",
    "        decay_lr = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size = 2000, gamma = 0.95) # exponential decay every step_size steps by factor gamma\n",
    "        self.scheduler = torch.optim.lr_scheduler.ChainedScheduler([warmup_lr, decay_lr])\n",
    "\n",
    "        # Training checkpoint \n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model_state = None\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            loss_IC, loss_PDE, loss_residual = 0.0, 0.0, 0.0\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Mini-batch loss for Initial Conditions (IC)\n",
    "            for batch_x0, batch_t0, batch_y0 in self.batch_generator(self.x0, self.t0, self.y0):  # (arange activated !!)\n",
    "                loss_IC += self.loss_IC(batch_x0, batch_t0, batch_y0)  \n",
    "            \n",
    "            # Mini-batch loss for Domain Loss (PDE) \n",
    "            #loss_residual_list = []\n",
    "            for batch_x_domain, batch_t_domain, batch_y_domain in self.batch_generator(self.x_domain, self.t_domain, self.y_domain): # (arange activated !!)\n",
    "                loss_PDE += self.loss_PDE(batch_x_domain, batch_t_domain)   \n",
    "                \n",
    "                loss_residual += self.loss_residual(batch_x_domain, batch_t_domain, batch_y_domain)\n",
    "                #loss_residual_list.append(self.loss_residual(batch_x_domain, batch_t_domain, batch_y_domain)) \n",
    "\n",
    "            #loss_residual = loss_residual*self.temporal_weights.clone()\n",
    "            #self.forward_temp_weights(loss_residual_list)\n",
    "            \n",
    "            # Give a weights to every singular loss term\n",
    "            weighted_losses = self.loss_weights.clone()*torch.stack([loss_IC, loss_PDE, loss_residual])\n",
    "            \n",
    "            # Total loss for this epoch after having given the weights\n",
    "            total_loss = weighted_losses.sum()\n",
    "\n",
    "            '''\n",
    "            torch.autograd.set_detect_anomaly(True) # DEBUG\n",
    "            '''\n",
    "            \n",
    "            total_loss.backward(retain_graph = True) # always calculate the derivatives first  \n",
    "            \n",
    "            if epoch % self.f == 0:\n",
    "                # Calculate gradients and retain graph in order to derive also all of the singular losses terms for the correspondant global weigth\n",
    "                self.forward_loss_weights([loss_IC, loss_PDE, loss_residual]) # global weights update routine using the non-weigthed losses\n",
    "\n",
    "            # Memorize the state of best training (checkpoint!)\n",
    "            if total_loss.item() < self.best_loss:\n",
    "                self.best_loss = total_loss.item()\n",
    "                self.best_model_state = {'model': self.state_dict(),'epoch': epoch}\n",
    "                \n",
    "            self.train_loss_history.append([total_loss.cpu().detach(), weighted_losses[0].cpu().detach(), weighted_losses[1].cpu().detach(), weighted_losses[2].cpu().detach()]) \n",
    "                \n",
    "            torch.nn.utils.clip_grad_norm_(self.new_param, max_norm=1.0) # clipping\n",
    "            \n",
    "            # Optimize the network parameters            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            self.scheduler.step()\n",
    "                \n",
    "            # Print out the loss every 100 epochs\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch ({optim}): {epoch}, Total Loss: {total_loss.detach().cpu().numpy()}')\n",
    "        \n",
    "        # Reload the checkpoint\n",
    "        if self.best_model_state is not None:\n",
    "            print(f\"Restoring best model from epoch {self.best_model_state['epoch']} with loss {self.best_loss:.4e}\")\n",
    "            self.load_state_dict(self.best_model_state['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "282d8b38-32d4-4e8c-a05a-105ad6352b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data importing and handling\n",
    "t_allN, x_allN, y_true_allN = load_observations('2015RN35')\n",
    "\n",
    "# Shuffle !\n",
    "idx = torch.randperm(len(t_allN))\n",
    "\n",
    "x_allN = x_allN[idx]\n",
    "t_allN = t_allN[idx]\n",
    "y_true_allN = y_true_allN[idx]\n",
    "\n",
    "# Dividing between testing and validation data sets\n",
    "t_valid, x_valid, y_true_valid = t_allN[-5:-2], x_allN[-5:-2,:], y_true_allN[-5:-2,:]\n",
    "t, x, y_true = t_allN[1:-5], x_allN[1:-5,:], y_true_allN[1:-5,:]\n",
    "\n",
    "# Import initial conditions from fitob\n",
    "Input_0_equin = np.array([1.4597513976025835, 0.285859684022060, 0.187526292392717, -0.001113623441851, -0.001969823345975, 34.9412629996062])  # from .eq0 orbfit file\n",
    "Input_0_cart = np.array([equinoctial_to_obs(Input_0_equin)])\n",
    "Input_0 = torch.tensor(Input_0_cart).float().to(device)\n",
    "\n",
    "x_0 = Input_0[:,0:4] # (alpha_0, delta_0, alpha_0_dot, delta_0_dot)\n",
    "\n",
    "alpha_ddot0, delta_ddot0, N = fit_second_derivatives(x_0, t_0)\n",
    "alpha_ddot0_col = torch.full((N, 1), alpha_ddot0)  # shape (N,1)\n",
    "delta_ddot0_col = torch.full((N, 1), delta_ddot0)\n",
    "\n",
    "x_0 = torch.hstack([x_0, alpha_ddot0_col, delta_ddot0_col]) # (alpha, alpha_dot, delta, delta_dot, alpha_ddot, delta_ddot)\n",
    "\n",
    "y_0_true = Input_0[:,4:6] # (rho_0, rho_0_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "adbea553-f250-47cc-911b-6195c69ba908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A-dimensionalization\n",
    "x0, t0, y0_true = non_dimensionalise(x_0, t_0, y_0_true)\n",
    "x, t, y_true = non_dimensionalise(x, t, y_true)\n",
    "x_valid, t_valid, y_true_valid = non_dimensionalise(x_valid, t_valid, y_true_valid)\n",
    "\n",
    "# Normalization (optional)\n",
    "t = normalize(t, None)\n",
    "x = normalize(x, None)\n",
    "y_true = normalize(y_true, None)\n",
    "\n",
    "t_valid, t_valid_mins, t_valid_maxs = normalize(t_valid, True)\n",
    "x_valid, x_valid_mins, x_valid_maxs = normalize(x_valid, True)\n",
    "y_true_valid, y_true_valid_mins, y_true_valid_maxs = normalize(y_true_valid, True)\n",
    "\n",
    "t0 = normalize(t0, None)\n",
    "x0 = normalize(x0, None)\n",
    "y0_true = normalize(y0_true, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1424f514",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9985, -0.9994,  1.0000, -0.4934, -1.0000, -1.0000],\n",
      "        [ 0.5025,  0.4870, -1.0000,  1.0000,  0.4864,  0.4864],\n",
      "        [-0.9985, -0.9994,  1.0000, -0.4865, -1.0000, -1.0000],\n",
      "        [ 1.0000,  0.9671, -1.0000,  0.4578,  0.9687,  0.9687]],\n",
      "       dtype=torch.float64, grad_fn=<IndexBackward0>) tensor([[ 0.0683, -0.2260],\n",
      "        [ 0.0864, -0.3784],\n",
      "        [ 0.1267, -0.3218],\n",
      "        [-0.1598, -0.0574]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m lambda_reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m PINN_VSA(layers, losstype, n_batch, mu, AU, t0, x0, y0_true, t, x, y_true)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_reg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[137], line 340\u001b[0m, in \u001b[0;36mPINN_VSA.train_network\u001b[0;34m(self, epochs, learning_rate, regularization)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# Mini-batch loss for Domain Loss (PDE) \u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m#loss_residual_list = []\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x_domain, batch_t_domain, batch_y_domain \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_generator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_domain, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_domain, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_domain): \u001b[38;5;66;03m# (arange activated !!)\u001b[39;00m\n\u001b[0;32m--> 340\u001b[0m     loss_PDE \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_PDE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x_domain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_t_domain\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m    342\u001b[0m     loss_residual \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_residual(batch_x_domain, batch_t_domain, batch_y_domain)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;66;03m#loss_residual_list.append(self.loss_residual(batch_x_domain, batch_t_domain, batch_y_domain)) \u001b[39;00m\n\u001b[1;32m    344\u001b[0m \n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m#loss_residual = loss_residual*self.temporal_weights.clone()\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m#self.forward_temp_weights(loss_residual_list)\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# Give a weights to every singular loss term\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[137], line 235\u001b[0m, in \u001b[0;36mPINN_VSA.loss_PDE\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_PDE\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m--> 235\u001b[0m     f_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurvature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m     f_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnan_to_num(f_pred, nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, posinf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e6\u001b[39m, neginf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e6\u001b[39m) \u001b[38;5;66;03m# clamp out any huge values or NaNs \u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosstype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[0;32mIn[137], line 152\u001b[0m, in \u001b[0;36mPINN_VSA.curvature\u001b[0;34m(self, t, x)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(x, out)\n\u001b[1;32m    150\u001b[0m v_a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhstack([x[\u001b[38;5;241m0\u001b[39m], x[\u001b[38;5;241m1\u001b[39m], x[\u001b[38;5;241m2\u001b[39m], x[\u001b[38;5;241m3\u001b[39m], out[\u001b[38;5;241m0\u001b[39m], out[\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m--> 152\u001b[0m r, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattributable_to_cartesian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m r_ast \u001b[38;5;241m=\u001b[39m r \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr_earth\n\u001b[1;32m    156\u001b[0m rho_versor \u001b[38;5;241m=\u001b[39m r \u001b[38;5;241m/\u001b[39m v_a[\u001b[38;5;241m4\u001b[39m]\n",
      "Cell \u001b[0;32mIn[137], line 171\u001b[0m, in \u001b[0;36mPINN_VSA.attributable_to_cartesian\u001b[0;34m(self, v_a)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattributable_to_cartesian\u001b[39m(\u001b[38;5;28mself\u001b[39m, v_a): \u001b[38;5;66;03m# (alpha, delta, alpha_dot, delta_dot, rho, rho_dot) --> (r, v)\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# unit line-of-sight\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m     cd, sd \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(\u001b[43mv_a\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m), torch\u001b[38;5;241m.\u001b[39msin(v_a[:,\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    172\u001b[0m     ca, sa \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(v_a[:,\u001b[38;5;241m0\u001b[39m]), torch\u001b[38;5;241m.\u001b[39msin(v_a[:,\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    173\u001b[0m     rho_hat \u001b[38;5;241m=\u001b[39m  torch\u001b[38;5;241m.\u001b[39mstack([cd\u001b[38;5;241m*\u001b[39mca, cd\u001b[38;5;241m*\u001b[39msa, sd], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "layers = [7, 2048, 2048, 2] # input features [(x, t) = 5, con Fourier diventano 2*fourier_m] → hidden layers → outputs (y)\n",
    "losstype = 'mse'\n",
    "n_batch = 4\n",
    "epochs = 10000\n",
    "L_rate = 0.0001\n",
    "lambda_reg = 0\n",
    "\n",
    "model = PINN_VSA(layers, losstype, n_batch, mu, AU, t0, x0, y0_true, t, x, y_true).to(device)\n",
    "\n",
    "model.train_network(epochs, L_rate, lambda_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35676103",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "total_loss, loss_IC, loss_PDE, loss_residual = model.get_training_history() \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(total_loss, label='Total Loss')\n",
    "plt.plot(loss_residual, label='Residual Loss')\n",
    "plt.plot(loss_IC, label='Initial Condition Loss')\n",
    "plt.plot(loss_PDE, label='PDE Domain Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed4eb6-4b53-4806-a4c8-a55d2aebc29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model predictions after training\n",
    "y_pred = model.network_prediction(t_valid, x_valid).float().detach().to(device).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc2574-e9c7-4f80-8fd3-466cc3960f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_valid = denormalize(t_valid, t_valid_mins, t_valid_maxs)\n",
    "x_valid = denormalize(x_valid, x_valid_mins, x_valid_maxs)\n",
    "y_true_valid = denormalize(y_true_valid, y_true_valid_mins, y_true_valid_maxs)\n",
    "y_pred = denormalize(torch.tensor(y_pred), y_true_valid_mins, y_true_valid_maxs)\n",
    "\n",
    "t_valid = t_valid.float().clone().detach().to(device).numpy()\n",
    "y_true_valid = y_true_valid.float().clone().detach().to(device).numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "axes[0].scatter(t_valid, y_true_valid[:,0], label = 'True')\n",
    "axes[0].scatter(t_valid, y_pred[:,0], label = 'Estimated')\n",
    "#axes[0].vlines(t_valid, y_pred[:,0], y_pred[:,0], colors = 'red', linestyles='dashed', label = 'Observations')\n",
    "axes[0].set_xlabel('t')\n",
    "axes[0].set_ylabel(r'$\\rho$')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].scatter(t_valid, y_true_valid[:,1], label = 'True')\n",
    "axes[1].scatter(t_valid, y_pred[:,1], label = 'Estimated')\n",
    "#axes[1].vlines(t_valid, y_pred[:,1], y_pred[:,1], colors = 'red', linestyles='dashed', label = 'Observations')\n",
    "axes[1].set_xlabel('t')\n",
    "axes[1].set_ylabel(r'$\\dot{\\rho}$')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14594ac-f0e6-45f0-a60e-a1b73217102b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": -2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
