{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7bea2ffe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Working on cpu\n",
      "  Using CPU with optimized threading\n",
      "  CPU Threads: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f0b21b4ded0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "\n",
    "# ==============\n",
    "# DEVICE SETUP \n",
    "# ==============\n",
    "\n",
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Progress bar for notebooks\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Astronomy libraries\n",
    "from astropy.time import Time\n",
    "from astropy.coordinates import get_body_barycentric\n",
    "import astropy.units as u\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device-agnostic configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Working on {device}\")\n",
    "\n",
    "# Device-specific optimizations\n",
    "if device.type == 'cuda':\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"  CUDA Version: {torch.version.cuda if hasattr(torch.version, 'cuda') else 'Unknown'}\")\n",
    "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    # Enable CUDNN optimizations for consistent input sizes\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    \n",
    "    # Set memory allocation strategy\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"  Using CPU with optimized threading\")\n",
    "    # CPU optimizations\n",
    "    torch.set_num_threads(torch.get_num_threads())\n",
    "    print(f\"  CPU Threads: {torch.get_num_threads()}\")\n",
    "\n",
    "# General optimizations (CPU and GPU)\n",
    "torch.backends.cudnn.deterministic = False  # Allow non-deterministic for speed\n",
    "torch.autograd.set_detect_anomaly(False)    # Disable anomaly detection for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "678e797c-26df-4101-80a5-6b62543c55d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# ORBITAL MECHANICS FUNCTIONS\n",
    "# ===============================================================================\n",
    "\n",
    "def solve_kepler(M: np.ndarray, e: float, tol: float = 1e-10, maxiter: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Solve Kepler's equation for elliptic orbits using Newton-Raphson method.\n",
    "    \n",
    "    Args:\n",
    "        M: Mean anomaly [rad]\n",
    "        e: Eccentricity\n",
    "        tol: Convergence tolerance\n",
    "        maxiter: Maximum iterations\n",
    "        \n",
    "    Returns:\n",
    "        E: Eccentric anomaly [rad]\n",
    "    \"\"\"\n",
    "    M = np.asarray(M)\n",
    "    E = M.copy()\n",
    "    \n",
    "    for _ in range(maxiter):\n",
    "        f = E - e * np.sin(E) - M\n",
    "        fp = 1 - e * np.cos(E)\n",
    "        dE = -f / fp\n",
    "        E += dE\n",
    "        if np.all(np.abs(dE) < tol):\n",
    "            break\n",
    "    return E\n",
    "\n",
    "\n",
    "def keplerian_to_cartesian(kepler_coord: np.ndarray, mu_param: float = 1.) -> tuple:\n",
    "    \"\"\"\n",
    "    Convert Keplerian elements to Cartesian coordinates.\n",
    "    \n",
    "    Args:\n",
    "        kepler_coord: [a, e, i, Omega, omega, M] in [AU, -, rad, rad, rad, rad]\n",
    "        mu_param: Gravitational parameter [AU¬≥/day¬≤]\n",
    "        \n",
    "    Returns:\n",
    "        r, v: Position [AU] and velocity [AU/day] in heliocentric ECI frame\n",
    "    \"\"\"\n",
    "    a, e, i, Omega, omega, M = kepler_coord\n",
    "    mu_use = mu_param if mu_param is not None else mu\n",
    "    \n",
    "    # Solve Kepler's equation\n",
    "    E = solve_kepler(M, e)\n",
    "    \n",
    "    # True anomaly\n",
    "    nu = 2 * np.arctan2(np.sqrt(1 + e) * np.sin(E / 2), \n",
    "                        np.sqrt(1 - e) * np.cos(E / 2))\n",
    "    \n",
    "    # Distance from central body\n",
    "    r_norm = a * (1 - e * np.cos(E))\n",
    "    \n",
    "    # Position and velocity in perifocal frame\n",
    "    cos_nu, sin_nu = np.cos(nu), np.sin(nu)\n",
    "    sin_E, cos_E = np.sin(E), np.cos(E)\n",
    "    \n",
    "    r_pqw = r_norm * np.array([cos_nu, sin_nu, 0])\n",
    "    v_pqw = (np.sqrt(mu_use * a) / r_norm * \n",
    "             np.array([-sin_E, np.sqrt(1 - e**2) * cos_E, 0]))\n",
    "    \n",
    "    # Rotation matrix elements (optimized)\n",
    "    cos_Om, sin_Om = np.cos(Omega), np.sin(Omega)\n",
    "    cos_w, sin_w = np.cos(omega), np.sin(omega)\n",
    "    cos_i, sin_i = np.cos(i), np.sin(i)\n",
    "    \n",
    "    # Perifocal to ECI transformation\n",
    "    R = np.array([\n",
    "        [cos_Om*cos_w - sin_Om*sin_w*cos_i, -cos_Om*sin_w - sin_Om*cos_w*cos_i,  sin_Om*sin_i],\n",
    "        [sin_Om*cos_w + cos_Om*sin_w*cos_i, -sin_Om*sin_w + cos_Om*cos_w*cos_i, -cos_Om*sin_i],\n",
    "        [sin_w*sin_i,                        cos_w*sin_i,                        cos_i]\n",
    "    ])\n",
    "    \n",
    "    return R @ r_pqw, R @ v_pqw\n",
    "\n",
    "\n",
    "def keplerian_to_obs(kepler_coord: np.ndarray, r_obs: np.ndarray = np.zeros(3), \n",
    "                     v_obs: np.ndarray = np.zeros(3), mu_param: float = 1.) -> tuple:\n",
    "    \"\"\"\n",
    "    Convert Keplerian elements to observational quantities.\n",
    "    \n",
    "    Args:\n",
    "        kepler_coord: [a, e, i, Omega, omega, M]\n",
    "        r_obs: Observatory position [AU]\n",
    "        v_obs: Observatory velocity [AU/day]\n",
    "        mu_param: Gravitational parameter [AU¬≥/day¬≤]\n",
    "        \n",
    "    Returns:\n",
    "        alpha, delta, alpha_dot, delta_dot, rho, rho_dot: Observables\n",
    "    \"\"\"\n",
    "    # Get heliocentric Cartesian coordinates\n",
    "    r_ast, v_ast = keplerian_to_cartesian(kepler_coord, mu_param)\n",
    "    \n",
    "    # Topocentric vectors\n",
    "    rho_vec = r_ast - r_obs\n",
    "    v_rel = v_ast - v_obs\n",
    "    \n",
    "    rho = np.linalg.norm(rho_vec)\n",
    "    hat_rho = rho_vec / rho\n",
    "    rho_dot = np.dot(hat_rho, v_rel)\n",
    "    \n",
    "    # Angular quantities\n",
    "    rho_hat_dot = (v_rel - rho_dot * hat_rho) / rho\n",
    "    \n",
    "    alpha = np.arctan2(hat_rho[1], hat_rho[0])\n",
    "    delta = np.arcsin(hat_rho[2])\n",
    "    \n",
    "    # Angular rates\n",
    "    denom = hat_rho[0]**2 + hat_rho[1]**2\n",
    "    alpha_dot = (hat_rho[0]*rho_hat_dot[1] - hat_rho[1]*rho_hat_dot[0]) / denom\n",
    "    delta_dot = rho_hat_dot[2] / np.cos(delta)\n",
    "    \n",
    "    return alpha, delta, alpha_dot, delta_dot, rho, rho_dot\n",
    "\n",
    "\n",
    "def equinoctial_to_cartesian(equin_coord: np.ndarray, mu_param: float = 1.) -> tuple:\n",
    "    \"\"\"\n",
    "    Convert equinoctial elements to Cartesian coordinates.\n",
    "    \n",
    "    Args:\n",
    "        equin_coord: [a, h, k, p, q, lambda] with a in [AU]\n",
    "        mu_param: Gravitational parameter [AU¬≥/day¬≤]\n",
    "        \n",
    "    Returns:\n",
    "        r, v: Position [AU] and velocity [AU/day] in heliocentric ECI frame\n",
    "    \"\"\"\n",
    "    a, h, k, p, q, l = equin_coord\n",
    "    mu_use = mu_param if mu_param is not None else mu\n",
    "    \n",
    "    # Derived quantities\n",
    "    e = np.sqrt(h**2 + k**2)\n",
    "    omega_plus_Omega = np.arctan2(h, k)\n",
    "    i = 2 * np.arctan(np.sqrt(p**2 + q**2))\n",
    "    Omega = np.arctan2(p, q)\n",
    "    \n",
    "    # Mean anomaly and eccentric anomaly\n",
    "    M = np.mod(l - omega_plus_Omega, 2*np.pi)\n",
    "    E = solve_kepler(M, e)\n",
    "    \n",
    "    # True anomaly\n",
    "    nu = 2 * np.arctan2(np.sqrt(1+e)*np.sin(E/2), np.sqrt(1-e)*np.cos(E/2))\n",
    "    \n",
    "    # Perifocal coordinates\n",
    "    r_pqw_norm = a * (1 - e*np.cos(E))\n",
    "    cos_nu, sin_nu = np.cos(nu), np.sin(nu)\n",
    "    sin_E, cos_E = np.sin(E), np.cos(E)\n",
    "    \n",
    "    r_vec_pqw = r_pqw_norm * np.array([cos_nu, sin_nu, 0])\n",
    "    v_vec_pqw = (np.sqrt(mu_use * a) / r_pqw_norm * \n",
    "                 np.array([-sin_E, np.sqrt(1 - e**2)*cos_E, 0]))\n",
    "    \n",
    "    # Rotation matrix\n",
    "    omega = omega_plus_Omega - Omega\n",
    "    cos_Om, sin_Om = np.cos(Omega), np.sin(Omega)\n",
    "    cos_w, sin_w = np.cos(omega), np.sin(omega)\n",
    "    cos_i, sin_i = np.cos(i), np.sin(i)\n",
    "    \n",
    "    R = np.array([\n",
    "        [cos_Om*cos_w - sin_Om*sin_w*cos_i, -cos_Om*sin_w - sin_Om*cos_w*cos_i, sin_Om*sin_i],\n",
    "        [sin_Om*cos_w + cos_Om*sin_w*cos_i, -sin_Om*sin_w + cos_Om*cos_w*cos_i, -cos_Om*sin_i],\n",
    "        [sin_w*sin_i,                        cos_w*sin_i,                       cos_i]\n",
    "    ])\n",
    "    \n",
    "    return R @ r_vec_pqw, R @ v_vec_pqw\n",
    "\n",
    "\n",
    "def equinoctial_to_obs(equin_coord: np.ndarray, r_obs: np.ndarray = np.zeros(3), \n",
    "                       v_obs: np.ndarray = np.zeros(3), mu_param: float = 1.) -> tuple:\n",
    "    \"\"\"Convert equinoctial elements to observational quantities.\"\"\"\n",
    "    r_ast, v_ast = equinoctial_to_cartesian(equin_coord, mu_param)\n",
    "    \n",
    "    # Topocentric vectors\n",
    "    rho_vec = r_ast - r_obs\n",
    "    v_rel = v_ast - v_obs\n",
    "    \n",
    "    rho = np.linalg.norm(rho_vec)\n",
    "    hat_rho = rho_vec / rho\n",
    "    rho_dot = np.dot(hat_rho, v_rel)\n",
    "    \n",
    "    # Angular quantities\n",
    "    rho_hat_dot = (v_rel - rho_dot * hat_rho) / rho\n",
    "    \n",
    "    alpha = np.arctan2(hat_rho[1], hat_rho[0])\n",
    "    delta = np.arcsin(hat_rho[2])\n",
    "    \n",
    "    # Angular rates\n",
    "    denom = hat_rho[0]**2 + hat_rho[1]**2\n",
    "    alpha_dot = (hat_rho[0]*rho_hat_dot[1] - hat_rho[1]*rho_hat_dot[0]) / denom\n",
    "    delta_dot = rho_hat_dot[2] / np.cos(delta)\n",
    "    \n",
    "    return alpha, delta, alpha_dot, delta_dot, rho, rho_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0f8881b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Solar Œº = 2.959122e-04 AU¬≥/day¬≤ for orbital calculations\n"
     ]
    }
   ],
   "source": [
    "# Physical constants and parameters\n",
    "AU =  149597870.7 # km per Au\n",
    "DAY = 60*60*24 # seconds per day\n",
    "\n",
    "MU_SUN_KM = 1.32712440018e11  # km¬≥/s¬≤ (Sun's gravitational parameter)\n",
    "mu = MU_SUN_KM * (DAY**2) / (AU**3)  # Convert to AU¬≥/day¬≤ for consistency with the .csv dataset\n",
    "\n",
    "print(f\"Using Solar Œº = {mu:.6e} AU¬≥/day¬≤ for orbital calculations\")\n",
    "\n",
    "# ===============================================================================  \n",
    "# IMPROVED EARTH POSITION CALCULATION FOR ACCURATE NEO RANGES IPORTING\n",
    "# ===============================================================================\n",
    "\n",
    "def get_earth_position_simple(t_mjd: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Get approximate Earth position and velocity at given MJD time.\n",
    "    Uses simplified circular orbit approximation.\n",
    "    \n",
    "    Args:\n",
    "        t_mjd: Modified Julian Date\n",
    "        \n",
    "    Returns:\n",
    "        (r_earth, v_earth): Position [AU] and velocity [AU/day] vectors\n",
    "    \"\"\"\n",
    "    # Convert MJD to days since J2000.0\n",
    "    t_j2000 = t_mjd - 51544.5\n",
    "    \n",
    "    # Earth's mean motion (rad/day) - approximately 2œÄ/365.25\n",
    "    n_earth = 2 * np.pi / 365.25\n",
    "    \n",
    "    # Earth's mean anomaly at time t\n",
    "    M = n_earth * t_j2000\n",
    "    \n",
    "    # Simplified circular orbit (Earth eccentricity ‚âà 0.017, neglected)\n",
    "    r_earth = np.array([\n",
    "        np.cos(M),      # x component [AU]\n",
    "        np.sin(M),      # y component [AU] \n",
    "        0.0             # z component [AU]\n",
    "    ])\n",
    "    \n",
    "    v_earth = np.array([\n",
    "        -n_earth * np.sin(M),   # vx [AU/day]\n",
    "        n_earth * np.cos(M),    # vy [AU/day]\n",
    "        0.0                     # vz [AU/day]\n",
    "    ])\n",
    "    \n",
    "    return r_earth, v_earth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6cf06023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# ===============================================================================\n",
    "\n",
    "# Reference time for observations (MJD)\n",
    "T_0_MJD = 60800.0\n",
    "t_0 = torch.tensor([T_0_MJD], dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "def load_dataset_from_csv(filepath: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Load NEO dataset from CSV file and convert to PyTorch tensors.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the CSV file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of tensors: (t_domain, x_domain, y_domain, t_0, x_0, y_0)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"üìÑ Successfully loaded {filepath}. Shape: {df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: File {filepath} not found.\")\n",
    "        return tuple(torch.empty(0) for _ in range(6))\n",
    "    \n",
    "    # Column definitions\n",
    "    input_cols = ['alpha', 'delta', 'alpha_dot', 'delta_dot']\n",
    "    kepler_cols = ['a_kep', 'e', 'i', 'Omega', 'omega', 'M']\n",
    "    equin_cols = ['a_equ', 'h', 'k', 'p', 'q', 'lambda']\n",
    "    \n",
    "    # Storage lists\n",
    "    data_lists = {\n",
    "        't': [], 'x': [], 'y': [],\n",
    "        't_0': [], 'x_0': [], 'y_0': []\n",
    "    }\n",
    "    \n",
    "    # Process each row\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            # Extract data\n",
    "            x_features = row[input_cols].values\n",
    "            t_fit = row['t_fit']\n",
    "            \n",
    "            # Fix: Convert to numpy arrays properly\n",
    "            equin_params = row[equin_cols].values.astype(np.float64)\n",
    "            kepler_params = row[kepler_cols].values.astype(np.float64)\n",
    "            \n",
    "            # CRITICAL FIX: Get Earth's position at observation time\n",
    "            earth_r, earth_v = get_earth_position_simple(t_fit)\n",
    "            \n",
    "            # Convert to observables WITH EARTH AS OBSERVER\n",
    "            va_0 = equinoctial_to_obs(equin_params, r_obs=earth_r, v_obs=earth_v, mu_param=mu)\n",
    "            va_true = keplerian_to_obs(kepler_params, r_obs=earth_r, v_obs=earth_v, mu_param=mu)\n",
    "            \n",
    "            # Check for NaN or infinite values\n",
    "            if np.any(np.isnan(va_0)) or np.any(np.isnan(va_true)):\n",
    "                print(f\"‚ö†Ô∏è  Warning: NaN values found in row {idx}, skipping\")\n",
    "                continue\n",
    "            if np.any(np.isinf(va_0)) or np.any(np.isinf(va_true)):\n",
    "                print(f\"‚ö†Ô∏è  Warning: Infinite values found in row {idx}, skipping\")\n",
    "                continue\n",
    "            # Check for valid MJD range\n",
    "            if not (15020 <= t_fit <= 88069):\n",
    "                print(f\"‚ö†Ô∏è  Skipping row {idx} with invalid MJD: {t_fit}\")\n",
    "                continue\n",
    "            # Check for negative range (physically impossible)\n",
    "            if va_0[4] <= 0 or va_true[4] <= 0:\n",
    "                print(f\"‚ö†Ô∏è  Warning: Negative range found in row {idx}, skipping\")\n",
    "                continue\n",
    "                \n",
    "            # Store data\n",
    "            data_lists['x'].append(x_features.tolist())\n",
    "            data_lists['t'].append(float(t_fit))\n",
    "            data_lists['y'].append([float(va_true[4]), float(va_true[5])])  # rho, rho_dot\n",
    "            \n",
    "            data_lists['x_0'].append([float(va_0[0]), float(va_0[1]), float(va_0[2]), float(va_0[3])])  # angles\n",
    "            data_lists['t_0'].append(float(T_0_MJD))\n",
    "            data_lists['y_0'].append([float(va_0[4]), float(va_0[5])])  # rho, rho_dot\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing row {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Check if data was loaded\n",
    "    if len(data_lists['y']) == 0:\n",
    "        print(\"‚ùå No valid data loaded!\")\n",
    "        return tuple(torch.empty(0) for _ in range(6))\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(data_lists['y'])} valid data points\")\n",
    "    \n",
    "    # Convert to tensors with debugging\n",
    "    tensors = {}\n",
    "    for key, data in data_lists.items():\n",
    "        try:\n",
    "            array = np.array(data, dtype=np.float32)\n",
    "            tensors[key] = torch.tensor(array, device=device)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error creating tensor for {key}: {e}\")\n",
    "            tensors[key] = torch.empty(0, device=device)\n",
    "    \n",
    "    return (tensors['t'], tensors['x'], tensors['y'], \n",
    "            tensors['t_0'], tensors['x_0'], tensors['y_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fba67f89-afb4-4f0a-8eda-dabddc5410c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characteristic scales:\n",
      "  L_c = 1e+08 km\n",
      "  T_c = 5e+06 s = 58.13 days\n",
      "  V_c = 29.78 km/s\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# CHARACTERISTIC SCALES AND NORMALIZATION\n",
    "# ===============================================================================\n",
    "\n",
    "'''\n",
    "# Characteristic quantities (based NEO typical scales)\n",
    "L_c = 1.0e8  # km (typical NEO distance ~100 million km)\n",
    "T_c = np.sqrt(L_c**3 / MU_SUN_KM)  # seconds\n",
    "V_c = L_c / T_c  # km/s\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Characteristic quantities (based on lunar orbit)\n",
    "L_c = 3.844e5  # km (mean orbital radius of the moon)\n",
    "T_c = np.sqrt(L_c**3 / MU_SUN_KM)  # seconds (characteristic time)\n",
    "V_c = L_c / T_c  # km/s (characteristic velocity)\n",
    "'''\n",
    "\n",
    "# Characteristic quantities (based on earth orbit)\n",
    "L_c = 1.496e8  # km (mean orbital radius of the earth)\n",
    "T_c = np.sqrt(L_c**3 / MU_SUN_KM)  # seconds (characteristic time)\n",
    "V_c = L_c / T_c  # km/s (characteristic velocity)\n",
    "\n",
    "mu_dimensionless = (MU_SUN_KM * T_c**2) / (L_c**3)\n",
    "\n",
    "print(f\"Characteristic scales:\")\n",
    "print(f\"  L_c = {L_c:.0e} km\")\n",
    "print(f\"  T_c = {T_c:.0e} s = {T_c/DAY:.2f} days\")\n",
    "print(f\"  V_c = {V_c:.2f} km/s\")\n",
    "\n",
    "\n",
    "def normalize(inputs: torch.Tensor, export_minmax: bool = False) -> 'torch.Tensor | tuple[torch.Tensor, torch.Tensor, torch.Tensor]':\n",
    "    \"\"\"\n",
    "    Normalize inputs to [-1, 1] range.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Input tensor\n",
    "        export_minmax: Whether to return min/max values\n",
    "        \n",
    "    Returns:\n",
    "        Normalized tensor, optionally with min/max values\n",
    "    \"\"\"\n",
    "    inputs_min = inputs.min(dim=0, keepdim=True).values\n",
    "    inputs_max = inputs.max(dim=0, keepdim=True).values\n",
    "    \n",
    "    normalized = 2 * (inputs - inputs_min) / (inputs_max - inputs_min + 1e-8) - 1\n",
    "    \n",
    "    if export_minmax:\n",
    "        return normalized, inputs_min, inputs_max\n",
    "    else:\n",
    "        return normalized\n",
    "\n",
    "\n",
    "def denormalize(inputs: torch.Tensor, inputs_min: torch.Tensor, \n",
    "                inputs_max: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Denormalize from [-1, 1] to original scale.\"\"\"\n",
    "    return ((inputs + 1) / 2) * (inputs_max - inputs_min + 1e-8) + inputs_min\n",
    "\n",
    "\n",
    "def non_dimensionalise(x: torch.Tensor, t: torch.Tensor, y: torch.Tensor) -> tuple:\n",
    "    \"\"\"\n",
    "    Convert physical quantities to dimensionless form.\n",
    "    \n",
    "    Args:\n",
    "        x: Angular observables [alpha, delta, alpha_dot, delta_dot]\n",
    "        t: Time observations [MJD]\n",
    "        y: Range observables [rho, rho_dot] in [AU, AU/day]\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of dimensionless tensors (x_nodim, t_nodim, y_nodim)\n",
    "    \"\"\"\n",
    "    # Angular quantities (already dimensionless, convert rates)\n",
    "    x_nodim = torch.empty_like(x)\n",
    "    x_nodim[:, 0] = x[:, 0]  # alpha [rad]\n",
    "    x_nodim[:, 1] = x[:, 1]  # delta [rad]\n",
    "    x_nodim[:, 2] = (x[:, 2] / DAY) * T_c  # alpha_dot [rad/day] --> alpha_dot [rad/s] --> dimensionless\n",
    "    x_nodim[:, 3] = (x[:, 3] / DAY) * T_c  # delta_dot [rad/day] --> delta_dot [rad/s] --> dimensionless\n",
    "\n",
    "    # Time conversion: MJD --> dimensionless\n",
    "    t_mjd = t - T_0_MJD  # Offset from reference time\n",
    "    t_sec = t_mjd * DAY  # Convert to seconds\n",
    "    t_nodim = (t_sec / T_c).view(-1, 1)  # Make dimensionless\n",
    "    \n",
    "    # Range quantities: AU --> dimensionless\n",
    "    y_nodim = torch.empty_like(y)\n",
    "    y_nodim[:, 0] = (y[:, 0] * AU) / L_c      # rho [AU] --> rho [km] --> dimensionless\n",
    "    y_nodim[:, 1] = (y[:, 1] * AU / DAY) / V_c  # rho_dot [AU/day] --> rho_dot [km/s] --> dimensionless\n",
    "    \n",
    "    return x_nodim, t_nodim, y_nodim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "29fb837d-dc3a-4c5a-bd6a-e946c8c891af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# NEURAL NETWORK SUPPORT CLASSES\n",
    "# ===============================================================================\n",
    "\n",
    "class FourierEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Fourier feature embedding for improved neural network training.\n",
    "    Maps input features to higher-dimensional space using random Fourier features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim: int, mapping_size: int, scale: float):\n",
    "        \"\"\"\n",
    "        Initialize Fourier embedding.\n",
    "        \n",
    "        Args:\n",
    "            in_dim: Input dimension\n",
    "            mapping_size: Number of random features\n",
    "            scale: Scale of random Fourier features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.register_buffer('B', torch.randn(in_dim, mapping_size) * scale)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply Fourier embedding to input.\"\"\"\n",
    "        B = self.B.to(x.device).to(x.dtype)\n",
    "        x_proj = 2 * np.pi * x @ B\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "68404050",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# PHYSICS-INFORMED NEURAL NETWORK FOR VERY SHORT ARCS\n",
    "# ===============================================================================\n",
    "\n",
    "class PINN_VSA(nn.Module):\n",
    "    \"\"\"\n",
    "    Physics-Informed Neural Network for Very Short Arc (VSA) orbit determination.\n",
    "    \n",
    "    Coordinate System Handling:\n",
    "    - Input attributable elements produce GEOCENTRIC coordinates (Earth-centered)\n",
    "    - Physical constraints (energy, angular momentum) computed in HELIOCENTRIC frame\n",
    "    - All coordinate transformations handled consistently\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers: list, loss_type: str, n_batch: int, mu: float, \n",
    "                 t0_mjd: float, *data, fourier_m: int, fourier_scale: float, corners: list):\n",
    "        \"\"\"\n",
    "        Initialize PINN_VSA model.\n",
    "        \n",
    "        Args:\n",
    "            layers: Network architecture [input_size, hidden_sizes..., output_size]\n",
    "            loss_type: Loss function type ('mse' or 'logcosh')\n",
    "            n_batch: Batch size\n",
    "            mu: Gravitational parameter\n",
    "            t0_mjd: Reference time (MJD)\n",
    "            t_edges: Time domain corners for normalization\n",
    "            data: Training data (t0, x0, y0, t_domain, x_domain, y_domain)\n",
    "            fourier_m: Fourier embedding size\n",
    "            fourier_scale: Fourier embedding scale\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Network configuration\n",
    "        self.fourier = FourierEmbedding(in_dim=5, mapping_size=fourier_m, scale=fourier_scale)\n",
    "        L0 = 2 * fourier_m  # Input size after Fourier embedding\n",
    "        layers = [L0] + layers[1:]\n",
    "        \n",
    "        # Physical parameters\n",
    "        self.mu = mu\n",
    "        self.losstype = loss_type\n",
    "        self.n_batch = n_batch\n",
    "        \n",
    "        # Time parameters\n",
    "        self.t0_mjd = t0_mjd\n",
    "        #self.t_edges = t_edges\n",
    "        self._earth_position_cache = {}\n",
    "        \n",
    "        # Setup data loaders\n",
    "        self._setup_data_loaders(*data, corners=corners)\n",
    "        \n",
    "        # Setup network layers with factorized weights\n",
    "        self._setup_network_layers(layers)\n",
    "        \n",
    "        # Training configuration\n",
    "        self._setup_training_config()\n",
    "\n",
    "    def _setup_data_loaders(self, t_domain, x_domain, y_domain, t0, x0, y0, corners: list):\n",
    "        \"\"\"Setup data loaders for training, validation, and initial conditions.\"\"\"\n",
    "        self.n_data = len(t_domain)\n",
    "        n_train = max(1, int(0.6 * self.n_data))  # 60% for training\n",
    "        \n",
    "        # Training data\n",
    "        self.train_dataset = TensorDataset(\n",
    "            x_domain[:n_train], t_domain[:n_train], y_domain[:n_train]\n",
    "        )\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.n_batch, shuffle=True)\n",
    "        \n",
    "        # Initial conditions\n",
    "        self.ic_dataset = TensorDataset(x0, t0, y0)\n",
    "        self.ic_loader = DataLoader(self.ic_dataset, batch_size=self.n_batch, shuffle=True)\n",
    "        \n",
    "        # Validation data\n",
    "        self.x_valid = x_domain[n_train:]\n",
    "        self.t_valid = t_domain[n_train:]\n",
    "        self.y_valid = y_domain[n_train:]\n",
    "        self.valid_dataset = TensorDataset(self.x_valid, self.t_valid, self.y_valid)\n",
    "        self.valid_loader = DataLoader(self.valid_dataset, batch_size=self.n_batch, shuffle=True)\n",
    "        \n",
    "\t\t# Border corners for physical data\n",
    "        self.t_border = [corners[0], corners[1]]\n",
    "        self.x_border = [corners[2], corners[3]]\n",
    "        self.y_border = [corners[4], corners[5]]\n",
    "\n",
    "    def _setup_network_layers(self, layers):\n",
    "        \"\"\"Setup network layers with factorized weights for better optimization.\"\"\"\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "        # Weight factorization parameters\n",
    "        self.s_list = {}\n",
    "        self.v_list = {}\n",
    "        biases = []\n",
    "        \n",
    "        for i in range(len(layers) - 1):\n",
    "            # Create linear layer\n",
    "            self.layers.append(nn.Linear(layers[i], layers[i+1], bias=True))\n",
    "            \n",
    "            # Factorize weights: W = s * v\n",
    "            W = self.layers[-1].weight.detach().clone()\n",
    "            s = torch.exp(torch.randn(layers[i+1]) * 0.1 + 1.0)\n",
    "            v = W / s.unsqueeze(1)\n",
    "            \n",
    "            # Register factorized parameters\n",
    "            self.s_list[f\"s_{i}\"] = nn.Parameter(s, requires_grad=True)\n",
    "            self.v_list[f\"v_{i}\"] = nn.Parameter(v, requires_grad=True)\n",
    "            self.register_parameter(f\"s_{i}\", self.s_list[f\"s_{i}\"])\n",
    "            self.register_parameter(f\"v_{i}\", self.v_list[f\"v_{i}\"])\n",
    "            \n",
    "            biases.append(self.layers[-1].bias.requires_grad_(True))\n",
    "            \n",
    "            # Freeze original weights\n",
    "            self.layers[i].weight.requires_grad_(False)\n",
    "            self.layers[i].bias.requires_grad_(False)\n",
    "        \n",
    "        self.new_param = (list(self.s_list.values()) + \n",
    "                         list(self.v_list.values()) + biases)\n",
    "    \n",
    "\n",
    "    def _setup_training_config(self):\n",
    "        \"\"\"Setup training configuration and loss weighting.\"\"\"\n",
    "        self.optimizer = None\n",
    "        self.train_loss_history = []\n",
    "        \n",
    "        # Loss weighting parameters\n",
    "        self.n_losses = 5  # IC, PDE, residual, interstellar, surface_revolution\n",
    "        self.loss_weights = torch.ones(self.n_losses, device=device, requires_grad=False)\n",
    "        self.f = 20  # Frequency for weight updates\n",
    "        self.alpha = 0.91  # Exponential moving average factor\n",
    "    \n",
    "\n",
    "    # --------------------------------- Core Methods -------------------------------------------------\n",
    "    def get_factorized_weight(self, i: int) -> tuple:\n",
    "        \"\"\"Get factorized weight and bias for layer i.\"\"\"\n",
    "        s = self.s_list[f\"s_{i}\"]\n",
    "        v = self.v_list[f\"v_{i}\"]\n",
    "        b = self.layers[i].bias\n",
    "\n",
    "        return s.view(-1, 1) * v, b\n",
    "    \n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        # Apply Fourier embedding\n",
    "        X_embed = self.fourier(X).float()\n",
    "        \n",
    "        a = X_embed\n",
    "\n",
    "        # Forward through hidden layers\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            input_copy = a  # For skip connections\n",
    "            \n",
    "            kernel, b = self.get_factorized_weight(i)\n",
    "            a = a @ kernel.T + b\n",
    "            \n",
    "            if i < len(self.layers) - 1:\n",
    "                a = self.activation(a)\n",
    "                \n",
    "                # Skip connections (after first layer)\n",
    "                if i >= 1:\n",
    "                    if a.shape != input_copy.shape:\n",
    "                        # Adapt input for different layer sizes\n",
    "                        eye = torch.eye(input_copy.shape[1], a.shape[1], \n",
    "                                       device=a.device, dtype=a.dtype)\n",
    "                        input_copy = input_copy @ eye\n",
    "                    a = a + input_copy\n",
    "        \n",
    "        # Output layer\n",
    "        kernel, b = self.get_factorized_weight(len(self.layers) - 1)\n",
    "        \n",
    "        output = a @ kernel.T + b\n",
    "\n",
    "        #output_rho = torch.nn.functional.softplus(output[:, 0]) + 1e-6 # ENFORCE POSITIVE VALUES FOR RHO !!!\n",
    "        #output_rho_dot = output[:, 1] \n",
    "        \n",
    "        return output #torch.stack([output_rho, output_rho_dot], dim=1)\n",
    "\n",
    "\n",
    "    def network_prediction(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Make network prediction given time and input features.\"\"\"\n",
    "        return self.forward(torch.cat([t, x], dim=1))\n",
    "\n",
    "\n",
    "    # --------------------------------- Miscellaneous Methods ---------------------------------\n",
    "    @staticmethod\n",
    "    def log_cosh(x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Numerically stable log-cosh function.\"\"\"\n",
    "\n",
    "        return x + torch.nn.functional.softplus(-2.0 * x) - np.log(2.0)\n",
    "    \n",
    "\n",
    "    def get_derivative(self, y: torch.Tensor, x: torch.Tensor, n: int = 1) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute nth derivative of y with respect to x using automatic differentiation.\n",
    "        Handles both scalar and vector outputs efficiently.\n",
    "        \"\"\"\n",
    "        if n == 0:\n",
    "\n",
    "            return y\n",
    "        \n",
    "        if y.shape[1] > 1:\n",
    "            # Vector output: compute Jacobian\n",
    "            dy_dx = torch.zeros_like(y)\n",
    "            for i in range(y.shape[1]):\n",
    "                grad_outputs = torch.zeros_like(y)\n",
    "                grad_outputs[:, i] = 1\n",
    "                \n",
    "                dy_dx_i = torch.autograd.grad(\n",
    "                    outputs=y, inputs=x, grad_outputs=grad_outputs,\n",
    "                    create_graph=True, retain_graph=True, allow_unused=True\n",
    "                )[0]\n",
    "                \n",
    "                if dy_dx_i is not None:\n",
    "                    dy_dx[:, i] = dy_dx_i.view(-1)\n",
    "        else:\n",
    "            # Scalar output: direct gradient\n",
    "            dy_dx = torch.autograd.grad(\n",
    "                outputs=y, inputs=x, grad_outputs=torch.ones_like(y),\n",
    "                create_graph=True, retain_graph=True, allow_unused=True\n",
    "            )[0]\n",
    "        \n",
    "        if dy_dx is None:\n",
    "            return torch.zeros_like(y)\n",
    "        \n",
    "        # Recursive call for higher-order derivatives\n",
    "        return self.get_derivative(dy_dx, x, n - 1) if n > 1 else dy_dx\n",
    "    \n",
    "    \n",
    "    def attributable_to_cartesian(self, att): # (alpha, delta, alpha_dot, delta_dot, rho, rho_dot) --> (r, v)\n",
    "        \"\"\"\n",
    "        Convert attributable elements to Cartesian coordinates.\n",
    "        \n",
    "        Note: This produces GEOCENTRIC coordinates (Earth-centered).\n",
    "        For heliocentric calculations, you need to add Earth's position.\n",
    "        \"\"\"\n",
    "        # unit line-of-sight\n",
    "        cd, sd = torch.cos(att[:,1]), torch.sin(att[:,1])\n",
    "        ca, sa = torch.cos(att[:,0]), torch.sin(att[:,0])\n",
    "        rho_hat =  torch.stack([cd*ca, cd*sa, sd], dim=1)\n",
    "\n",
    "        # partials\n",
    "        d_rho_hat_dalpha = torch.stack([-cd*sa, cd*ca, torch.zeros_like(sd)], dim=1)\n",
    "        d_rho_hat_ddelta = torch.stack([-sd*ca, -sd*sa, cd], dim=1)\n",
    "\n",
    "        # time-derivative of rho_hat\n",
    "        rho_hat_dot = d_rho_hat_dalpha * att[:,2].unsqueeze(1) + d_rho_hat_ddelta * att[:,3].unsqueeze(1)\n",
    "\n",
    "        # position and velocity (GEOCENTRIC)\n",
    "        r = att[:,4].unsqueeze(1) * rho_hat\n",
    "        v = att[:,5].unsqueeze(1) * rho_hat + att[:,4].unsqueeze(1) * rho_hat_dot\n",
    "\n",
    "        r.requires_grad_(True).float().to(device)\n",
    "        v.requires_grad_(True).float().to(device)\n",
    "        \n",
    "        return r, v\n",
    "    \n",
    "\n",
    "    def get_earth_position(self, t): # Get the heliocentric position of the Earth at time t\n",
    "        # Create a cache key based on the time values\n",
    "        t_key = tuple(t.detach().cpu().numpy().flatten())\n",
    "        \n",
    "        if t_key in self._earth_position_cache:\n",
    "\n",
    "            return self._earth_position_cache[t_key]\n",
    "                    \n",
    "        t_sec = t * T_c  # Convert to seconds\n",
    "        t_mjd_offset = t_sec / DAY  # Convert to days\n",
    "        t_mjd = self.t0_mjd + t_mjd_offset  # Add to reference MJD\n",
    "        \n",
    "        # Ensure we have valid MJD values\n",
    "        #t_mjd = torch.clamp(t_mjd, min=50000.0, max=70000.0)  # Reasonable MJD range\n",
    "        \n",
    "        t_mjd_np = t_mjd.detach().cpu().numpy()\n",
    "        \n",
    "        times = Time(t_mjd_np, format='mjd')\n",
    "        earth_cart = get_body_barycentric('earth', times) \n",
    "        xyz_au = u.Quantity(\n",
    "            [earth_cart.x.to_value(u.au),\n",
    "            earth_cart.y.to_value(u.au),\n",
    "            earth_cart.z.to_value(u.au)],\n",
    "        unit=u.au\n",
    "        ).T \n",
    "        xyz = torch.from_numpy(xyz_au.value).to(torch.float32).to(device)\n",
    "          \n",
    "        if xyz.ndim == 3:\n",
    "            xyz = xyz.squeeze(0)\n",
    "        if xyz.shape[0] != t.shape[0]:\n",
    "            xyz = xyz.expand(t.shape[0], -1)\n",
    "          \n",
    "        # Cache the result for future use\n",
    "        self._earth_position_cache[t_key] = xyz\n",
    "\n",
    "        return xyz\n",
    "\n",
    "\n",
    "    def geocentric_to_heliocentric(self, r_geocentric, v_geocentric, t):\n",
    "        \"\"\"\n",
    "        Convert geocentric coordinates to heliocentric coordinates.\n",
    "        \n",
    "        Args:\n",
    "            r_geocentric: Position vectors in geocentric frame\n",
    "            v_geocentric: Velocity vectors in geocentric frame  \n",
    "            t: Time tensor\n",
    "            \n",
    "        Returns:\n",
    "            r_heliocentric, v_heliocentric: Position and velocity in heliocentric frame\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get Earth's heliocentric position\n",
    "        with torch.no_grad():\n",
    "            r_earth_helio = self.get_earth_position(t)  # Use time from first column\n",
    "            r_earth_helio = (r_earth_helio * AU) / L_c\n",
    "            r_earth_helio = r_earth_helio.detach().to(r_geocentric.device)\n",
    "        \n",
    "        # Transform to heliocentric frame\n",
    "        r_heliocentric = r_geocentric + r_earth_helio.clone()\n",
    "        v_heliocentric = v_geocentric  # Simplified velocity transformation\n",
    "        \n",
    "        return r_heliocentric, v_heliocentric\n",
    "\n",
    "\n",
    "    # ------------------------ Loss functions for the PINN_VSA class --------------------------------------------\n",
    "    def negative_rho_penalty(self, out, penalty_weight=1e6): # inputs denormalized\n",
    "        negative_rho = -torch.minimum(out[:, 0], torch.tensor(0.0, device=out.device))\n",
    "        \n",
    "        return penalty_weight * torch.mean(negative_rho)\n",
    "\n",
    "\n",
    "    def loss_IC(self, x, t, y): # inputs NOT denormalized\n",
    "        y_pred_IC = self.network_prediction(t, x)\n",
    "        y_pred_IC = denormalize(y_pred_IC, self.y_border[0], self.y_border[1])\n",
    "        y = denormalize(y, self.y_border[0], self.y_border[1])\n",
    "        \n",
    "        if self.losstype == 'mse':\n",
    "            loss_IC = torch.mean(torch.pow((y - y_pred_IC),2))\n",
    "        elif self.losstype == 'logcosh':\n",
    "            loss_IC = torch.mean(self.log_cosh(y - y_pred_IC))\n",
    "        \n",
    "        penalty = self.negative_rho_penalty(y_pred_IC)\n",
    "\n",
    "        return loss_IC + penalty\n",
    "\n",
    "\n",
    "    def loss_residuals(self, out, batch_y): # out denormalized, batch_y NOT denormalized\n",
    "        \"\"\"Compute residual loss using cached network output.\"\"\"\n",
    "        batch_y = denormalize(batch_y, self.y_border[0], self.y_border[1])\n",
    "        \n",
    "        if self.losstype == 'mse':\n",
    "            loss_residual = torch.mean(torch.pow((batch_y - out), 2))\n",
    "        elif self.losstype == 'logcosh':\n",
    "            loss_residual = torch.mean(self.log_cosh(batch_y - out))\n",
    "            \n",
    "        penalty = self.negative_rho_penalty(out)\n",
    "\n",
    "        return loss_residual + penalty\n",
    "\n",
    "\n",
    "    def loss_physics(self, r_helio, v_helio, batch_t): # inputs denormalized\n",
    "        # Ensure r_helio and v_helio are on the same device\n",
    "        r_helio = r_helio.to(device)\n",
    "        v_helio = v_helio.to(device)\n",
    "                \n",
    "        # Energy conservation\n",
    "        dv_dt = self.get_derivative(v_helio, batch_t, 1)\n",
    "        \n",
    "        if dv_dt is None:\n",
    "            dv_dt = torch.zeros_like(v_helio).requires_grad_(True)\n",
    "        \n",
    "        vr = (v_helio * dv_dt).sum(dim=1)\n",
    "        rv = (r_helio * v_helio).sum(dim=1)\n",
    "        rnorm3 = torch.pow(r_helio.norm(dim=1), 3)\n",
    "        \n",
    "        dE_dt = vr - mu_dimensionless * (rv / (rnorm3 + 1e-8))\n",
    "        \n",
    "        # Angular momentum conservation\n",
    "        L = torch.cross(r_helio, v_helio, dim=1)\n",
    "        dL_dt = self.get_derivative(L, batch_t, 1)\n",
    "        \n",
    "        if dL_dt is None:\n",
    "            dL_dt = torch.zeros_like(L).requires_grad_(True)\n",
    "        \n",
    "        # Combine losses\n",
    "        if self.losstype == 'mse':\n",
    "            loss_PDE = torch.mean(torch.pow(dE_dt, 2)) + torch.mean(torch.pow(dL_dt, 2))\n",
    "        elif self.losstype == 'logcosh':\n",
    "            loss_PDE = torch.mean(self.log_cosh(dE_dt)) + torch.mean(self.log_cosh(dL_dt))\n",
    "        \n",
    "        return loss_PDE\n",
    "\n",
    "\n",
    "    def loss_surface_revolution(self, att, r_helio, t): # inputs denormalized\n",
    "        \"\"\"Compute surface revolution loss using cached coordinates.\"\"\"\n",
    "        with torch.no_grad():            \n",
    "            r_earth_helio = self.get_earth_position(t)  # Use time from first column\n",
    "            r_earth_helio = r_earth_helio.detach().to(r_helio.device)\n",
    "\n",
    "        # To be absolutely sure, let's clone r_helio to ensure we're not modifying a tensor needed elsewhere\n",
    "        r_helio_clone = r_helio.clone()\n",
    "\n",
    "        # Surface revolution constraint in heliocentric frame\n",
    "        f = att[:, 4].unsqueeze(1).pow(2) - r_helio_clone.norm(dim=1, keepdim=True).pow(2) - \\\n",
    "            (2 * r_earth_helio.norm(dim=1, keepdim=True).pow(5))/(3 * r_helio_clone.norm(dim=1, keepdim=True).pow(3)) \\\n",
    "                + (5/3)*r_earth_helio.norm(dim=1, keepdim=True).pow(2)\n",
    "    \n",
    "        if self.losstype == 'mse':\n",
    "            loss_sr = torch.mean(torch.pow(f, 2))\n",
    "        elif self.losstype == 'logcosh':\n",
    "            loss_sr = torch.mean(self.log_cosh(f))\n",
    "            \n",
    "        out = torch.stack([att[:, 4], att[:, 5]], dim=1) # Already denormalized rho to original scale\n",
    "        penalty = self.negative_rho_penalty(out)\n",
    "        \n",
    "        return loss_sr + penalty\n",
    "\n",
    "\n",
    "    def loss_interstellar(self, r_helio, v_helio): # inputs denormalized\n",
    "        \"\"\"Compute interstellar penalty using cached coordinates.\"\"\"\n",
    "        v2 = (torch.pow(v_helio, 2)).sum(dim=1)\n",
    "        rnorm = r_helio.norm(dim=1)\n",
    "        \n",
    "        E = 0.5 * v2 - mu_dimensionless / (rnorm + 1e-8)\n",
    "        \n",
    "        penalty_interstellar = torch.relu(E)\n",
    "\n",
    "        return penalty_interstellar.mean()*0.1\n",
    "\n",
    "\n",
    "    def losses_epoch(self, use_mixed_precision=False):\n",
    "        \"\"\"\n",
    "        Device-agnostic optimized loss computation with optional mixed precision.\n",
    "        \"\"\"\n",
    "        losses = {'IC': 0.0, 'PDE': 0.0, 'residual': 0.0, \n",
    "                 'interstellar': 0.0, 'surface_revolution': 0.0, 'validation': 0.0}\n",
    "        \n",
    "        # Compute IC loss with optional autocast\n",
    "        for batch in self.ic_loader:\n",
    "            batch = [b.to(device, non_blocking=True) for b in batch]\n",
    "            \n",
    "            if use_mixed_precision:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    losses['IC'] += self.loss_IC(*batch)\n",
    "            else:\n",
    "                losses['IC'] += self.loss_IC(*batch)\n",
    "        \n",
    "        if len(self.ic_loader) > 0:\n",
    "            losses['IC'] /= len(self.ic_loader)\n",
    "        \n",
    "        # Compute training losses with coordinate caching\n",
    "        for batch in self.train_loader:\n",
    "            batch_x, batch_t, batch_y = [b.to(device, non_blocking=True) for b in batch]\n",
    "            batch_t.requires_grad_(True)\n",
    "            \n",
    "            if use_mixed_precision:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    # Compute all losses in one forward pass\n",
    "                    losses_batch = self.losses_batch(batch_x, batch_t, batch_y)\n",
    "            else:\n",
    "                losses_batch = self.losses_batch(batch_x, batch_t, batch_y)\n",
    "            \n",
    "            for key in ['PDE', 'residual', 'interstellar', 'surface_revolution']:\n",
    "                losses[key] += losses_batch[key]\n",
    "        \n",
    "        if len(self.train_loader) > 0:\n",
    "            for key in ['PDE', 'residual', 'interstellar', 'surface_revolution']:\n",
    "                losses[key] /= len(self.train_loader)\n",
    "        \n",
    "        # Compute validation loss\n",
    "        for batch in self.valid_loader:\n",
    "            batch_x, batch_t, batch_y = [b.to(device, non_blocking=True) for b in batch]\n",
    "            batch_t.requires_grad_(True)  # This needs gradients for PDE loss computation\n",
    "                \n",
    "\t\t\t# Use torch.no_grad() only for the final loss computation to save memory but allow gradients for intermediate calculations\n",
    "            if use_mixed_precision:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    val_loss_batch = self.validation_loss_batch(batch_x, batch_t, batch_y)\n",
    "            else:\n",
    "                val_loss_batch = self.validation_loss_batch(batch_x, batch_t, batch_y)\n",
    "                \n",
    "            losses['validation'] += val_loss_batch\n",
    "            \n",
    "        if len(self.valid_loader) > 0:\n",
    "            losses['validation'] /= len(self.valid_loader)\n",
    "        \n",
    "        return losses\n",
    "\n",
    "\n",
    "    def losses_batch(self, batch_x, batch_t, batch_y): # inputs NOT denormalized\n",
    "        \"\"\"Compute all losses for a batch with coordinate caching.\"\"\"\n",
    "        # We need a separate graph for the surface revolution loss to avoid conflicts with the derivatives taken in the PDE loss.\n",
    "        out_pde = self.network_prediction(batch_t, batch_x)\n",
    "        out_sr = self.network_prediction(batch_t, batch_x)\n",
    "        \n",
    "        batch_t = denormalize(batch_t, self.t_border[0], self.t_border[1])\n",
    "        batch_x = denormalize(batch_x, self.x_border[0], self.x_border[1])\n",
    "        out_pde = denormalize(out_pde, self.y_border[0], self.y_border[1])\n",
    "        out_sr = denormalize(out_sr, self.y_border[0], self.y_border[1])\n",
    "\n",
    "        att_pde = torch.cat([batch_x, out_pde], dim=1)\n",
    "        att_sr = torch.cat([batch_x, out_sr], dim=1)\n",
    "\n",
    "        r_geocentric_pde, v_geocentric_pde = self.attributable_to_cartesian(att_pde)\n",
    "        r_helio_pde, v_helio_pde = self.geocentric_to_heliocentric(r_geocentric_pde, v_geocentric_pde, batch_t)\n",
    "\n",
    "        r_geocentric_sr, v_geocentric_sr = self.attributable_to_cartesian(att_sr)\n",
    "        r_helio_sr, v_helio_sr = self.geocentric_to_heliocentric(r_geocentric_sr, v_geocentric_sr, batch_t)\n",
    "\n",
    "        r_geocentric_pde = (r_geocentric_pde / L_c) * AU\n",
    "        v_geocentric_pde = (v_geocentric_pde / V_c) * (AU / DAY) \n",
    "        r_helio_pde = (r_helio_pde / L_c) * AU\n",
    "        v_helio_pde = (v_helio_pde / V_c) * (AU / DAY) \n",
    "        \n",
    "        r_geocentric_sr = (r_geocentric_pde / L_c) * AU\n",
    "        v_geocentric_sr = (v_geocentric_pde / V_c) * (AU / DAY)\n",
    "        r_helio_sr = (r_helio_sr / L_c) * AU\n",
    "        v_helio_sr = (v_helio_sr / V_c) * (AU / DAY) \n",
    "        batch_t = (batch_t / T_c) * DAY\n",
    "\n",
    "        att_sr[:,2] = (att_sr[:,2] / T_c) * DAY\n",
    "        att_sr[:,3] = (att_sr[:,3] / T_c) * DAY\n",
    "\n",
    "        # Compute all losses using cached coordinates allowing all losses to remain connected to the same computation graph\n",
    "        out_sr = out_pde\n",
    "        att_sr = att_pde\n",
    "        r_helio_sr = r_helio_pde\n",
    "\n",
    "        return {\n",
    "            'PDE': self.loss_physics(r_helio_pde, v_helio_pde, batch_t),\n",
    "            'residual': self.loss_residuals(out_sr, batch_y),\n",
    "            'interstellar': self.loss_interstellar(r_helio_pde, v_helio_pde),\n",
    "            'surface_revolution': self.loss_surface_revolution(att_sr, r_helio_sr, batch_t),\n",
    "        }\n",
    "\n",
    "\n",
    "    def validation_loss_batch(self, batch_x, batch_t, batch_y): # inputs NOT denormalized\n",
    "        \"\"\"Compute validation loss for a batch using cached coordinate computation.\"\"\"\n",
    "        # Single forward pass\n",
    "        out = self.network_prediction(batch_t, batch_x)\n",
    "        \n",
    "        batch_t_denorm = denormalize(batch_t, self.t_border[0], self.t_border[1])  # Denormalize time to original scale\n",
    "        batch_x_denorm = denormalize(batch_x, self.x_border[0], self.x_border[1])  # Denormalize x to original scale\n",
    "        out = denormalize(out, self.y_border[0], self.y_border[1])\n",
    "        \n",
    "        att = torch.cat([batch_x_denorm, out], dim=1)\n",
    "    \n",
    "        # Get coordinates once and cache\n",
    "        r_geocentric, v_geocentric = self.attributable_to_cartesian(att)\n",
    "        r_helio, v_helio = self.geocentric_to_heliocentric(r_geocentric, v_geocentric, batch_t_denorm)\n",
    "\n",
    "        r_geocentric = (r_geocentric / L_c) * AU\n",
    "        v_geocentric = (v_geocentric / V_c) * (AU / DAY) \n",
    "        r_helio = (r_helio / L_c) * AU\n",
    "        v_helio = (v_helio / V_c) * (AU / DAY) \n",
    "\n",
    "        att[:,2] = (att[:,2] / T_c) * DAY\n",
    "        att[:,3] = (att[:,3] / T_c) * DAY\n",
    "        \n",
    "        # Compute all losses using cached coordinates\n",
    "        ic_loss = self.loss_weights[0].clone()*self.loss_IC(batch_x, batch_t, batch_y)\n",
    "        pde_loss = self.loss_weights[1].clone()*self.loss_physics(r_helio, v_helio, batch_t_denorm)\n",
    "        residual_loss = self.loss_weights[2].clone()*self.loss_residuals(out, batch_y)\n",
    "        interstellar_loss = self.loss_weights[3].clone()*self.loss_interstellar(r_helio, v_helio)\n",
    "        surface_revolution_loss = self.loss_weights[4].clone()*self.loss_surface_revolution(att, r_helio, batch_t_denorm)\n",
    "\n",
    "        return ic_loss + pde_loss + residual_loss + interstellar_loss + surface_revolution_loss\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------\n",
    "    '''\n",
    "    def forward_temp_weights(self, loss_domain):\n",
    "        loss_domain = torch.tensor(loss_domain, device=self.temporal_weights.device)\n",
    "        \n",
    "        self.temporal_weights = torch.exp(-self.epsilon * torch.cumsum(loss_domain, dim=0))\n",
    "    '''\n",
    "\n",
    "    def forward_loss_weights(self, losses):       \n",
    "        parameters = [p for p in self.new_param if p.requires_grad and p.is_leaf] # pick only the parameters that we actually want to differentiate\n",
    "        \n",
    "        # Create the gradient of each component of the loss respect to the parameters of the model\n",
    "        grad_norms = []\n",
    "        \n",
    "        for i, l in enumerate(losses): \n",
    "            # Create a fresh copy that's detached from the original graph\n",
    "            l_copy = l.detach().clone().requires_grad_(True)\n",
    "        \n",
    "            # Use retain_graph=True for all but the last loss to avoid freeing the graph\n",
    "            retain_graph = (i < len(losses) - 1)\n",
    "        \n",
    "            grads = torch.autograd.grad(\n",
    "                l_copy, parameters, \n",
    "                retain_graph=retain_graph, \n",
    "                create_graph=False,  # Changed to False to avoid creating new graph\n",
    "                allow_unused=True\n",
    "            )\n",
    "\n",
    "            flat = []\n",
    "            for g in grads:\n",
    "                if g is not None:\n",
    "                    flat.append(g.view(-1))\n",
    "            if flat:\n",
    "                grad_norms.append(torch.norm(torch.cat(flat)))\n",
    "            else:\n",
    "                grad_norms.append(torch.tensor(0.0, device=l.device))\n",
    "            \n",
    "        grad_norms = torch.stack(grad_norms)\n",
    "\n",
    "        # Update loss weights\n",
    "        lambda_hat = grad_norms.sum() / (grad_norms + 1e-8) # ensure lambda is not infinite\n",
    "        \n",
    "        self.loss_weights = self.alpha*self.loss_weights.clone() + (1 - self.alpha)*lambda_hat\n",
    "\n",
    "\n",
    "    def get_training_history(self):\n",
    "        loss_his = np.array(self.train_loss_history)\n",
    "        total_loss, loss_IC, loss_PDE, loss_residual, loss_interstellar, loss_surface_revolution, loss_validation = np.split(loss_his, 7, axis=1) \n",
    "        \n",
    "        return total_loss, loss_IC, loss_PDE, loss_residual, loss_interstellar, loss_surface_revolution, loss_validation\n",
    "\n",
    "\n",
    "    # ---------------------------------------------- Training routine ---------------------------------------------------\n",
    "    def train_network(self, epochs: int, learning_rate: float, regularization: float,\n",
    "                     gradient_accumulation_steps: int, early_stopping_patience: int, \n",
    "                     log_interval: int, use_mixed_precision: bool = False):\n",
    "        \"\"\"\n",
    "        Device-agnostic optimized training loop with automatic mixed precision support.\n",
    "        \n",
    "        Args:\n",
    "            epochs: Number of training epochs\n",
    "            learning_rate: Learning rate for optimizer\n",
    "            regularization: L2 regularization weight\n",
    "            gradient_accumulation_steps: Accumulate gradients over multiple batches\n",
    "            early_stopping_patience: Stop training if no improvement for N epochs\n",
    "            log_interval: How often to update progress bar\n",
    "            use_mixed_precision: Enable mixed precision (auto-detected if None)\n",
    "        \"\"\"\n",
    "        # Auto-detect mixed precision capability\n",
    "        if use_mixed_precision is None:\n",
    "            use_mixed_precision = device.type == 'cuda' and torch.cuda.is_available()\n",
    "        \n",
    "        # Mixed precision setup (GPU only)\n",
    "        scaler = torch.cuda.amp.GradScaler() if use_mixed_precision else None\n",
    "        \n",
    "        # Setup optimizer with device-agnostic optimizations\n",
    "        optimizer_kwargs = {\n",
    "            'lr': learning_rate, \n",
    "            'betas': (0.9, 0.999),\n",
    "            'eps': 1e-08, \n",
    "            'weight_decay': regularization, \n",
    "            'amsgrad': True\n",
    "        }\n",
    "        \n",
    "        #Add fused optimizer for GPU\n",
    "        if device.type == 'cuda':\n",
    "            optimizer_kwargs['fused'] = True\n",
    "            \n",
    "        self.optimizer = torch.optim.AdamW(self.new_param, **optimizer_kwargs)\n",
    "        \n",
    "        # Advanced learning rate scheduling\n",
    "        warmup_steps = min(300, epochs // 10)\n",
    "        cosine_lr = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer, T_max=epochs, eta_min=int(learning_rate * 0.01)\n",
    "        )\n",
    "        warmup_lr = torch.optim.lr_scheduler.LambdaLR(\n",
    "            self.optimizer, lr_lambda=lambda step: min(1.0, (step + 1) / warmup_steps)\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.ChainedScheduler([warmup_lr, cosine_lr])\n",
    "        \n",
    "        # Validation tracking with early stopping\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_val_model_state = None\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Performance monitoring\n",
    "        total_training_time = 0\n",
    "        epoch_times = []\n",
    "        \n",
    "        # Training loop with device-agnostic optimizations\n",
    "        progress_bar = tqdm(range(epochs), desc=f\"Training PINN ({device.type.upper()})\", unit=\"epoch\")\n",
    "        \n",
    "        for epoch in progress_bar:\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            # Efficient loss computation with optional mixed precision\n",
    "            losses = self.losses_epoch(use_mixed_precision=use_mixed_precision)\n",
    "            \n",
    "            # Stack losses efficiently \n",
    "            # Apply loss weights and compute total loss using the original (non-detached) losses\n",
    "            weighted_losses = self.loss_weights.clone() * torch.stack([\n",
    "                torch.as_tensor(losses['IC'], device=device, dtype=torch.float32),\n",
    "                torch.as_tensor(losses['PDE'], device=device, dtype=torch.float32),\n",
    "                torch.as_tensor(losses['residual'], device=device, dtype=torch.float32),\n",
    "                torch.as_tensor(losses['interstellar'], device=device, dtype=torch.float32),\n",
    "                torch.as_tensor(losses['surface_revolution'], device=device, dtype=torch.float32)\n",
    "            ])\n",
    "            \n",
    "            total_loss = weighted_losses.sum()\n",
    "\n",
    "            # Gradient accumulation\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                total_loss = total_loss / gradient_accumulation_steps\n",
    "                weighted_losses = weighted_losses / gradient_accumulation_steps\n",
    "                \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Backward pass with optional mixed precision\n",
    "            if use_mixed_precision and scaler is not None:\n",
    "                scaler.scale(total_loss).backward()\n",
    "                if (epoch + 1) % gradient_accumulation_steps == 0:\n",
    "                    scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.new_param, max_norm=1.0)\n",
    "                    scaler.step(self.optimizer)\n",
    "                    scaler.update()\n",
    "                    self.scheduler.step()\n",
    "            else:\n",
    "                # --- MODIFIED BACKWARD PASS: separate the losses that require complex graphs (PDE) from simpler ones\n",
    "                # First: Complex losse that involve derivatives (PDE)\n",
    "                complex_losses = weighted_losses[1] \n",
    "                complex_losses.backward(retain_graph=True)\n",
    "\n",
    "                # Second: Simple losses (IC, residual, interstellar, surface_revolution)\n",
    "                simple_losses = weighted_losses[0] + weighted_losses[2] + weighted_losses[3] + weighted_losses[4] \n",
    "                simple_losses.backward()\n",
    "                \n",
    "                if (epoch + 1) % gradient_accumulation_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.new_param, max_norm=1.0)\n",
    "                    self.optimizer.step()\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Update loss weights periodically\n",
    "            if epoch % (self.f) == 0:\n",
    "                # Compute loss weights without interfering with main training graph\n",
    "                self.forward_loss_weights([losses['IC'], losses['PDE'], losses['residual'], losses['interstellar'], losses['surface_revolution']])\n",
    "\n",
    "            # Validation and early stopping\n",
    "            val_loss = float(losses['validation'])\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.best_val_model_state = {'model': self.state_dict(), 'epoch': epoch}\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"\\nüõë Early stopping at epoch {epoch} - no improvement for {early_stopping_patience} epochs\")\n",
    "                break\n",
    "\n",
    "            # Efficient history logging\n",
    "            if epoch % max(1, epochs // 200) == 0:\n",
    "                history_entry = ([torch.as_tensor(total_loss).cpu().detach()] +\n",
    "                                [torch.as_tensor(wl).cpu().detach() for wl in weighted_losses] +\n",
    "                                [torch.as_tensor(losses['validation']).cpu().detach()])\n",
    "                self.train_loss_history.append(history_entry)\n",
    "            \n",
    "            # Performance tracking\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            epoch_times.append(epoch_time)\n",
    "            total_training_time += epoch_time\n",
    "            \n",
    "            # Update progress bar\n",
    "            if epoch % log_interval == 0 or epoch < 10:\n",
    "                avg_epoch_time = np.mean(epoch_times[-10:]) if epoch_times else epoch_time\n",
    "                \n",
    "                progress_info = {\n",
    "                    'Total': f\"{total_loss:.2e}\",\n",
    "                    'Val': f\"{val_loss:.2e}\",\n",
    "                    'LR': f\"{self.optimizer.param_groups[0]['lr']:.1e}\",\n",
    "                    'Time': f\"{avg_epoch_time:.2f}s\"\n",
    "                }\n",
    "                \n",
    "                # Add memory info if available (for GPU)\n",
    "                if device.type == 'cuda':\n",
    "                    memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "                    progress_info['GPU'] = f\"{memory_used:.1f}GB\"\n",
    "                \n",
    "                progress_bar.set_postfix(progress_info)\n",
    "            \n",
    "            # Memory cleanup (for GPU)\n",
    "            if device.type == 'cuda' and epoch % 100 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Restore best model\n",
    "        if self.best_val_model_state is not None:\n",
    "            print(f\"\\n‚úÖ Restoring best model from epoch {self.best_val_model_state['epoch']} \"\n",
    "                  f\"with validation loss {self.best_val_loss:.4e}\")\n",
    "            self.load_state_dict(self.best_val_model_state['model'])\n",
    "        \n",
    "        # Training summary\n",
    "        avg_epoch_time = np.mean(epoch_times) if epoch_times else 0\n",
    "        print(f\"\\nüìä Training Summary:\")\n",
    "        print(f\"  Total Time: {total_training_time:.2f}s\")\n",
    "        print(f\"  Avg Epoch Time: {avg_epoch_time:.3f}s\")\n",
    "        print(f\"  Device: {device}\")\n",
    "        if use_mixed_precision:\n",
    "            print(f\"  Mixed Precision: Enabled\")\n",
    "        \n",
    "        return total_training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "56e572d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and pre-processing data...\n",
      "üìÑ Successfully loaded DataSet_NEO_VSA_from1to8nights.csv. Shape: (25404, 19)\n",
      "‚úÖ Loaded 25404 valid data points\n",
      "Data pre-processing complete.\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# ===============================================================================\n",
    "\n",
    "print(\"Loading and pre-processing data...\")\n",
    "\n",
    "# Load dataset\n",
    "t_allN, x_allN, y_true_allN, t_0_long, x_0, y_0_true = load_dataset_from_csv('DataSet_NEO_VSA_from1to8nights.csv')\n",
    "\n",
    "# Shuffle (why not!) ---------------\n",
    "idx = torch.randperm(len(t_allN))\n",
    "\n",
    "x_allN = x_allN[idx]\n",
    "t_allN = t_allN[idx]\n",
    "y_true_allN = y_true_allN[idx]\n",
    "\n",
    "x_0 = x_0[idx]\n",
    "t_0_long = t_0_long[idx]\n",
    "y_0_true = y_0_true[idx]\n",
    "# ----------------------------------\n",
    "\n",
    "# Convert to dimensionless units\n",
    "x_allN_processed, t_allN_processed, y_true_allN_processed = non_dimensionalise(x_allN, t_allN, y_true_allN)\n",
    "x0, t0, y0_true = non_dimensionalise(x_0, t_0_long, y_0_true)\n",
    "\n",
    "# Normalize to [-1, 1] range ---------------------------------------------------------------\n",
    "t_allN_processed, t_mins, t_maxs = normalize(t_allN_processed, export_minmax=True)\n",
    "x_allN_processed, x_mins, x_maxs = normalize(x_allN_processed, export_minmax=True)\n",
    "y_true_allN_processed, y_mins, y_maxs = normalize(y_true_allN_processed, export_minmax=True)\n",
    "\n",
    "t0 = normalize(t0)\n",
    "x0 = normalize(x0)\n",
    "y0_true = normalize(y0_true)\n",
    "# ------------------------------------------------------------------------------------------\n",
    "print(\"Data pre-processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1424f514",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Device-Aware Training Configuration (CPU):\n",
      "  loss_type: mse\n",
      "  epochs: 200\n",
      "  learning_rate: 1e-05\n",
      "  regularization: 0.0\n",
      "  fourier_features: 16\n",
      "  fourier_scale: 2.0\n",
      "  batch_size: 16\n",
      "  gradient_accumulation_steps: 2\n",
      "  early_stopping_patience: 66\n",
      "  log_interval: 10\n",
      "  use_mixed_precision: None\n",
      "  Network architecture: [5, 256, 256, 256, 256, 256, 2]\n",
      "  Device: cpu\n",
      "  Mixed Precision: Auto-detect\n",
      "‚úÖ Model initialization complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PINN (CPU):   0%| | 1/200 [01:02<3:27:02, 62.43s/epoch, Total=9.34e+05,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ‚Ä¢ Fused Optimizer\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ‚Ä¢ Non-blocking Data Transfer\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m training_time = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mepochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlearning_rate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mregularization\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mregularization\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgradient_accumulation_steps\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mearly_stopping_patience\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlog_interval\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_mixed_precision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muse_mixed_precision\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müéâ Training completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[92]\u001b[39m\u001b[32m, line 716\u001b[39m, in \u001b[36mPINN_VSA.train_network\u001b[39m\u001b[34m(self, epochs, learning_rate, regularization, gradient_accumulation_steps, early_stopping_patience, log_interval, use_mixed_precision)\u001b[39m\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    713\u001b[39m     \u001b[38;5;66;03m# --- MODIFIED BACKWARD PASS: separate the losses that require complex graphs (PDE) from simpler ones\u001b[39;00m\n\u001b[32m    714\u001b[39m     \u001b[38;5;66;03m# First: Complex losse that involve derivatives (PDE)\u001b[39;00m\n\u001b[32m    715\u001b[39m     complex_losses = weighted_losses[\u001b[32m1\u001b[39m] \n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m     \u001b[43mcomplex_losses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    718\u001b[39m     \u001b[38;5;66;03m# Second: Simple losses (IC, residual, interstellar, surface_revolution)\u001b[39;00m\n\u001b[32m    719\u001b[39m     simple_losses = weighted_losses[\u001b[32m0\u001b[39m] + weighted_losses[\u001b[32m2\u001b[39m] + weighted_losses[\u001b[32m3\u001b[39m] + weighted_losses[\u001b[32m4\u001b[39m] \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PINNs_lab/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    513\u001b[39m         Tensor.backward,\n\u001b[32m    514\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    519\u001b[39m         inputs=inputs,\n\u001b[32m    520\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PINNs_lab/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    284\u001b[39m     retain_graph = create_graph\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PINNs_lab/lib/python3.11/site-packages/torch/autograd/graph.py:769\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    767\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    770\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# DEVICE-AWARE MODEL CONFIGURATION AND TRAINING\n",
    "# ===============================================================================\n",
    "\n",
    "# Network architecture\n",
    "layers = [5, 256, 256, 256, 256, 256, 2]  # [input_features, ...hidden_layers..., output_features]\n",
    "\n",
    "epochs = 200  # Number of training epochs\n",
    "lr = 1e-5  # Learning rate for optimizer\n",
    "\n",
    "# Device-aware training configuration\n",
    "config = {\n",
    "    'loss_type': 'mse',                    # 'mse' or 'logcosh'\n",
    "    'epochs': epochs,                      # Number of training epochs  \n",
    "    'learning_rate': lr,                   # Learning rate for optimizer\n",
    "    'regularization': lr*0,                # L2 regularization for stability\n",
    "    'fourier_features': 16,                 # Fourier embedding size\n",
    "    'fourier_scale': 2.0,                  # Fourier embedding scale\n",
    "    \n",
    "    # Device-aware optimization parameters\n",
    "    'batch_size': 16,\n",
    "    'gradient_accumulation_steps': 1 if device.type == 'cuda' else 2,     # More accumulation for CPU\n",
    "    'early_stopping_patience': epochs//3,                                 # Early stopping patience\n",
    "    'log_interval': epochs//20,                                           # More frequent updates on GPU\n",
    "    'use_mixed_precision': None,                                          # Auto-detect mixed precision\n",
    "}\n",
    "\n",
    "print(f\"üîß Device-Aware Training Configuration ({device.type.upper()}):\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Initialize model with optimized data loading\n",
    "print(f\"  Network architecture: {layers}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Mixed Precision: {'Auto-detect' if config['use_mixed_precision'] is None else config['use_mixed_precision']}\")\n",
    "\n",
    "model = PINN_VSA(\n",
    "    layers,\n",
    "    config['loss_type'],\n",
    "    config['batch_size'],\n",
    "    mu,\n",
    "    T_0_MJD,\n",
    "    t0, x0, y0_true,           # Initial conditions\n",
    "    t_allN_processed, x_allN_processed, y_true_allN_processed, # Training and validation data\n",
    "    fourier_m = config['fourier_features'],\n",
    "    fourier_scale = config['fourier_scale'],\n",
    "    corners = [t_mins, t_maxs, x_mins, x_maxs, y_mins, y_maxs],\n",
    ").to(device)\n",
    "\n",
    "print(\"‚úÖ Model initialization complete.\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"  ‚Ä¢ CUDNN Benchmarking\")\n",
    "    print(f\"  ‚Ä¢ Fused Optimizer\")\n",
    "    print(f\"  ‚Ä¢ Non-blocking Data Transfer\")\n",
    "\n",
    "training_time = model.train_network(\n",
    "    epochs=config['epochs'],\n",
    "    learning_rate=config['learning_rate'],\n",
    "    regularization=config['regularization'],\n",
    "    gradient_accumulation_steps=config['gradient_accumulation_steps'],\n",
    "    early_stopping_patience=config['early_stopping_patience'],\n",
    "    log_interval=config['log_interval'],\n",
    "    use_mixed_precision=config['use_mixed_precision']\n",
    ")\n",
    "\n",
    "print(f\"\\nüéâ Training completed in {training_time:.2f}s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35676103",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# TRAINING RESULTS VISUALIZATION\n",
    "# ===============================================================================\n",
    "\n",
    "# Get training history\n",
    "(total_loss, loss_IC, loss_PDE, loss_residual, \n",
    " loss_interstellar, loss_surface_revolution, loss_validation) = model.get_training_history()\n",
    "\n",
    "# Create enhanced loss plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Main loss plot\n",
    "loss_components = [\n",
    "    (total_loss, 'Total Loss', 'black', 2.0),\n",
    "    (loss_residual, 'Data Residual', 'blue', 1.5),\n",
    "    (loss_IC, 'Initial Conditions', 'green', 1.5),\n",
    "    (loss_PDE, 'Physics (PDE)', 'red', 1.5),\n",
    "    (loss_surface_revolution, 'Surface Revolution', 'orange', 1.5),\n",
    "    (loss_interstellar, 'Interstellar Penalty', 'purple', 1.5),\n",
    "    (loss_validation, 'Validation', 'gray', 2.0)\n",
    "]\n",
    "\n",
    "for loss_data, label, color, width in loss_components:\n",
    "    if len(loss_data) > 0:\n",
    "        ax1.plot(loss_data.flatten(), label=label, color=color, linewidth=width, alpha=0.8)\n",
    "\n",
    "# Formatting for main plot\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title('PINN Training Progress', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=10, loc='upper right')\n",
    "\n",
    "# Set x-axis ticks\n",
    "max_epochs = len(total_loss) if len(total_loss) > 0 else config['epochs']\n",
    "tick_spacing = max(1, max_epochs // 10)\n",
    "ax1.set_xticks(np.arange(0, max_epochs + 1, tick_spacing))\n",
    "\n",
    "# Learning rate plot (if available)\n",
    "if hasattr(model, 'scheduler'):\n",
    "    # Simulate LR schedule for visualization\n",
    "    lrs = []\n",
    "    temp_optimizer = torch.optim.AdamW([torch.tensor(1.0)], lr=config['learning_rate'])\n",
    "    temp_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        temp_optimizer, T_max=max_epochs, eta_min=config['learning_rate'] * 0.01\n",
    "    )\n",
    "    \n",
    "    for _ in range(max_epochs):\n",
    "        lrs.append(temp_optimizer.param_groups[0]['lr'])\n",
    "        temp_optimizer.step()\n",
    "        temp_scheduler.step()\n",
    "    \n",
    "    ax2.plot(lrs, color='orange', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Learning Rate', fontsize=12)\n",
    "    ax2.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('Training_Progress.png')\n",
    "\n",
    "# Print final results\n",
    "if len(total_loss) > 0:\n",
    "    print(f\"üéØ Final Training Results:\")\n",
    "    print(f\"  Total Loss: {total_loss[-1].item():.4e}\")\n",
    "    if len(loss_validation) > 0:\n",
    "        print(f\"  Validation Loss: {loss_validation[-1].item():.4e}\")\n",
    "    if len(loss_residual) > 0:\n",
    "        print(f\"  Data Residual: {loss_residual[-1].item():.4e}\")\n",
    "    if len(loss_PDE) > 0:\n",
    "        print(f\"  Physics Loss: {loss_PDE[-1].item():.4e}\")\n",
    "    \n",
    "    # Convergence analysis\n",
    "    if len(total_loss) > 100:\n",
    "        recent_avg = np.mean([x.item() for x in total_loss[-50:]])\n",
    "        early_avg = np.mean([x.item() for x in total_loss[50:100]])\n",
    "        improvement_rate = (early_avg - recent_avg) / early_avg * 100\n",
    "        print(f\"  Recent Improvement Rate: {improvement_rate:.1f}%\")\n",
    "        \n",
    "        if improvement_rate < 1.0:\n",
    "            print(\"  üìà Training appears to have converged!\")\n",
    "        else:\n",
    "            print(\"  üîÑ Training still improving - consider more epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfbfba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model predictions on validation set\n",
    "with torch.no_grad():\n",
    "    y_pred = model.network_prediction(model.t_valid, model.x_valid).detach().cpu()\n",
    "    \n",
    "print(f\"Validation predictions shape: {y_pred.shape}\")\n",
    "print(f\"Validation targets shape: {model.y_valid.shape}\")\n",
    "\n",
    "# ===============================================================================\n",
    "# HISTOGRAM VISUALIZATION OF VALIDATION RESULTS\n",
    "# ===============================================================================\n",
    "\n",
    "\n",
    "# Denormalize predictions and validation data for plotting\n",
    "t_valid_np = denormalize(model.t_valid, t_mins, t_maxs).detach().cpu().numpy()\n",
    "y_valid_np = denormalize(model.y_valid, y_mins, y_maxs).detach().cpu().numpy()\n",
    "y_pred_np = denormalize(y_pred, y_mins, y_maxs).numpy()\n",
    "\n",
    "n_bins = 100\n",
    "\n",
    "# Calculate errors for histogram analysis\n",
    "error_rho = y_valid_np[:, 0] - y_pred_np[:, 0]\n",
    "error_rho_dot = y_valid_np[:, 1] - y_pred_np[:, 1]\n",
    "\n",
    "# Create comprehensive histogram plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Row 1: Range (œÅ) analysis\n",
    "# Distribution comparison\n",
    "axes[0, 0].hist(y_valid_np[:, 0], bins=n_bins, alpha=0.7, label='True', color='blue', density=True)\n",
    "axes[0, 0].hist(y_pred_np[:, 0], bins=n_bins, alpha=0.7, label='Predicted', color='red', density=True)\n",
    "axes[0, 0].set_xlabel('œÅ [dimensionless]', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Density', fontsize=12)\n",
    "axes[0, 0].set_title('Range Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10, loc='upper right')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error histogram\n",
    "axes[0, 1].hist(error_rho, bins=n_bins, alpha=0.7, color='green', density=True)\n",
    "axes[0, 1].axvline(np.mean(error_rho), color='red', linestyle='-', linewidth=2, label=f'Mean Error: {np.mean(error_rho):.3e}')\n",
    "axes[0, 1].set_xlabel('Prediction Error', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Density', fontsize=12)\n",
    "axes[0, 1].set_title('Range Error Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Absolute error histogram\n",
    "axes[0, 2].hist(np.abs(error_rho), bins=n_bins, alpha=0.7, color='orange', density=True)\n",
    "axes[0, 2].axvline(np.mean(np.abs(error_rho)), color='red', linestyle='-', linewidth=2, \n",
    "                   label=f'MAE: {np.mean(np.abs(error_rho)):.3e}')\n",
    "axes[0, 2].set_xlabel('Absolute Prediction Error', fontsize=12)\n",
    "axes[0, 2].set_ylabel('Density', fontsize=12)\n",
    "axes[0, 2].set_title('Range Absolute Error Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].legend(fontsize=10)\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: Range Rate (œÅÃá) analysis\n",
    "# Distribution comparison\n",
    "axes[1, 0].hist(y_valid_np[:, 1], bins=n_bins, alpha=0.7, label='True', color='blue', density=True)\n",
    "axes[1, 0].hist(y_pred_np[:, 1], bins=n_bins, alpha=0.7, label='Predicted', color='red', density=True)\n",
    "axes[1, 0].set_xlabel('œÅÃá [dimensionless]', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Density', fontsize=12)\n",
    "axes[1, 0].set_title('Range Rate Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=10, loc='upper right')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error histogram\n",
    "axes[1, 1].hist(error_rho_dot, bins=n_bins, alpha=0.7, color='green', density=True)\n",
    "axes[1, 1].axvline(np.mean(error_rho_dot), color='red', linestyle='-', linewidth=2, \n",
    "                   label=f'Mean Error: {np.mean(error_rho_dot):.3e}')\n",
    "axes[1, 1].set_xlabel('Prediction Error', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Density', fontsize=12)\n",
    "axes[1, 1].set_title('Range Rate Error Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Absolute error histogram\n",
    "axes[1, 2].hist(np.abs(error_rho_dot), bins=n_bins, alpha=0.7, color='orange', density=True)\n",
    "axes[1, 2].axvline(np.mean(np.abs(error_rho_dot)), color='red', linestyle='-', linewidth=2, \n",
    "                   label=f'MAE: {np.mean(np.abs(error_rho_dot)):.3e}')\n",
    "axes[1, 2].set_xlabel('Absolute Prediction Error', fontsize=12)\n",
    "axes[1, 2].set_ylabel('Density', fontsize=12)\n",
    "axes[1, 2].set_title('Range Rate Absolute Error Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 2].legend(fontsize=10)\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('Errors_Distribution.png')\n",
    "\n",
    "\n",
    "# Statistical summary of errors\n",
    "print(f\"\\nüìä Detailed Error Analysis:\")\n",
    "print(f\"  Range (œÅ) Errors:\")\n",
    "print(f\"    Mean Error: {np.mean(error_rho):.6e}\")\n",
    "print(f\"    Std Error: {np.std(error_rho):.6e}\")\n",
    "print(f\"    Min Error: {np.min(error_rho):.6e}\")\n",
    "print(f\"    Max Error: {np.max(error_rho):.6e}\")\n",
    "print(f\"    Error Range: {np.max(error_rho) - np.min(error_rho):.6e}\")\n",
    "\n",
    "print(f\"\\n  Range Rate (œÅÃá) Errors:\")\n",
    "print(f\"    Mean Error: {np.mean(error_rho_dot):.6e}\")\n",
    "print(f\"    Std Error: {np.std(error_rho_dot):.6e}\")\n",
    "print(f\"    Min Error: {np.min(error_rho_dot):.6e}\")\n",
    "print(f\"    Max Error: {np.max(error_rho_dot):.6e}\")\n",
    "print(f\"    Error Range: {np.max(error_rho_dot) - np.min(error_rho_dot):.6e}\")\n",
    "\n",
    "# Distribution tests\n",
    "from scipy import stats\n",
    "\n",
    "# Normality tests for errors\n",
    "_, p_rho_normal = stats.shapiro(error_rho)\n",
    "_, p_rho_dot_normal = stats.shapiro(error_rho_dot)\n",
    "\n",
    "print(f\"\\nüìà Error Distribution Analysis:\")\n",
    "print(f\"  Range Error Normality Test (Shapiro-Wilk):\")\n",
    "print(f\"    p-value: {p_rho_normal:.6f} ({'Normal' if p_rho_normal > 0.05 else 'Non-normal'} distribution)\")\n",
    "print(f\"  Range Rate Error Normality Test (Shapiro-Wilk):\")\n",
    "print(f\"    p-value: {p_rho_dot_normal:.6f} ({'Normal' if p_rho_dot_normal > 0.05 else 'Non-normal'} distribution)\")\n",
    "\n",
    "# Percentile analysis\n",
    "print(f\"\\nüìä Error Percentiles:\")\n",
    "print(f\"  Range (œÅ) Error Percentiles:\")\n",
    "print(f\"    5th: {np.percentile(np.abs(error_rho), 5):.6e}\")\n",
    "print(f\"    25th: {np.percentile(np.abs(error_rho), 25):.6e}\")\n",
    "print(f\"    50th (Median): {np.percentile(np.abs(error_rho), 50):.6e}\")\n",
    "print(f\"    75th: {np.percentile(np.abs(error_rho), 75):.6e}\")\n",
    "print(f\"    95th: {np.percentile(np.abs(error_rho), 95):.6e}\")\n",
    "\n",
    "print(f\"\\n  Range Rate (œÅÃá) Error Percentiles:\")\n",
    "print(f\"    5th: {np.percentile(np.abs(error_rho_dot), 5):.6e}\")\n",
    "print(f\"    25th: {np.percentile(np.abs(error_rho_dot), 25):.6e}\")\n",
    "print(f\"    50th (Median): {np.percentile(np.abs(error_rho_dot), 50):.6e}\")\n",
    "print(f\"    75th: {np.percentile(np.abs(error_rho_dot), 75):.6e}\")\n",
    "print(f\"    95th: {np.percentile(np.abs(error_rho_dot), 95):.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79658812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# PLOT WITH CONVERTED PREDICTIONS TO PHYSICAL UNITS\n",
    "# ===============================================================================\n",
    "\n",
    "print(\"üîß CONVERTING PREDICTIONS TO PHYSICAL UNITS FOR PLOTTING:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get the validation indices to extract corresponding original time values\n",
    "val_size = len(y_valid_np)\n",
    "val_indices = torch.arange(len(t_allN) - val_size, len(t_allN))  # Last val_size points\n",
    "t_valid_original = t_allN[val_indices].cpu().numpy().flatten()  # Original MJD values\n",
    "\n",
    "print(\"Min MJD:\", t_valid_original.min())\n",
    "print(\"Max MJD:\", t_valid_original.max())\n",
    "\n",
    "y_pred_physical = np.zeros_like(y_pred_np)\n",
    "y_pred_physical[:, 0] = (y_pred_np[:, 0] * L_c) / AU  # Convert rho back to AU\n",
    "y_pred_physical[:, 1] = (y_pred_np[:, 1] * V_c * DAY) / AU  # Convert rho_dot back to AU/day\n",
    "\n",
    "y_valid_physical = np.zeros_like(y_valid_np)\n",
    "y_valid_physical[:, 0] = (y_valid_np[:, 0] * L_c) / AU\n",
    "y_valid_physical[:, 1] = (y_valid_np[:, 1] * V_c * DAY) / AU\n",
    "\n",
    "# Now create the plot with both in physical units\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Top-left: Range Distribution Comparison (both in AU)\n",
    "axes[0, 0].hist(y_valid_physical[:, 0], bins=n_bins, alpha=0.7, label='True', color='blue')\n",
    "axes[0, 0].hist(y_pred_physical[:, 0], bins=n_bins, alpha=0.7, label='Predicted', color='red')\n",
    "axes[0, 0].set_xlabel('œÅ (AU)')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].set_title('Range Distribution (Physical Units)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Top-right: Range Rate Distribution Comparison (both in AU/day)\n",
    "axes[0, 1].hist(y_valid_physical[:, 1], bins=n_bins, alpha=0.7, label='True', color='blue')\n",
    "axes[0, 1].hist(y_pred_physical[:, 1], bins=n_bins, alpha=0.7, label='Predicted', color='red')\n",
    "axes[0, 1].set_xlabel('œÅÃá (AU/day)')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_title('Range Rate Distribution (Physical Units)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom-left: Range vs Time (both in physical units)\n",
    "axes[1, 0].scatter(t_valid_original, y_valid_physical[:, 0], alpha=0.6, s=10, color='blue', label='True')\n",
    "axes[1, 0].scatter(t_valid_original, y_pred_physical[:, 0], alpha=0.6, s=10, color='red', label='Predicted')\n",
    "axes[1, 0].set_xlabel('Time (MJD)')\n",
    "axes[1, 0].set_ylabel('œÅ (AU)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom-right: Range Rate vs Time (both in physical units)\n",
    "axes[1, 1].scatter(t_valid_original, y_valid_physical[:, 1], alpha=0.6, s=10, color='blue', label='True')\n",
    "axes[1, 1].scatter(t_valid_original, y_pred_physical[:, 1], alpha=0.6, s=10, color='red', label='Predicted')\n",
    "axes[1, 1].set_xlabel('Time (MJD)')\n",
    "axes[1, 1].set_ylabel('œÅÃá (AU/day)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('Estimation_VS_prediction.png')\n",
    "\n",
    "# Calculate meaningful error metrics in physical units\n",
    "rho_error = np.abs(y_pred_physical[:, 0] - y_valid_physical[:, 0])\n",
    "rho_dot_error = np.abs(y_pred_physical[:, 1] - y_valid_physical[:, 1])\n",
    "\n",
    "print(f\"\\nüìä ERROR ANALYSIS (Physical Units):\")\n",
    "print(f\"Range (œÅ) errors:\")\n",
    "print(f\"  Mean absolute error: {rho_error.mean():.6f} AU\")\n",
    "print(f\"  Max error: {rho_error.max():.6f} AU\")\n",
    "print(f\"  Relative error: {(rho_error.mean() / y_valid_physical[:, 0].mean()) * 100:.2f}%\")\n",
    "\n",
    "print(f\"\\nRange rate (œÅÃá) errors:\")\n",
    "print(f\"  Mean absolute error: {rho_dot_error.mean():.6f} AU/day\")\n",
    "print(f\"  Max error: {rho_dot_error.max():.6f} AU/day\")\n",
    "print(f\"  Relative error: {(rho_dot_error.mean() / np.abs(y_valid_physical[:, 1]).mean()) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30169a70-6ea9-4fe4-9357-2d3075fe4357",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "PINNs_lab",
   "language": "python",
   "name": "pinns_lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": -2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
